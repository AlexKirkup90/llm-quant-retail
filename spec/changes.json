{
  "app.py": "# app.py\nimport json\nimport sys\nfrom pathlib import Path\n\n# --- Make project root importable (works under streamlit/pytest/CI) ---\nROOT = Path(__file__).resolve().parent\nif str(ROOT) not in sys.path:\n    sys.path.insert(0, str(ROOT))\n\ndef main():\n    import streamlit as st\n    import pandas as pd\n    import numpy as np\n    from datetime import date\n\n    # Local imports\n    from src import (\n        dataops,\n        features,\n        signals,\n        portfolio,\n        metrics,\n        report,\n        memory,\n        universe,\n        universe_registry,\n        universe_selector,\n    )\n\n    st.title(\"LLM-Codex Quant (S&P 500) \u2014 Weekly\")\n\n    as_of = st.date_input(\"As-of date\", value=date.today())\n    universe_choices = [\"SP500_MINI\", \"SP500_FULL\", \"R1000\", \"NASDAQ_100\", \"FTSE_350\"]\n    default_index = (\n        universe_choices.index(\"SP500_FULL\") if \"SP500_FULL\" in universe_choices else 0\n    )\n\n    universe_mode = st.selectbox(\"Universe Mode\", [\"auto\", \"manual\"], index=0)\n    manual_universe = None\n    if universe_mode == \"manual\":\n        manual_universe = st.selectbox(\"Universe\", universe_choices, index=default_index)\n\n    apply_filters = st.checkbox(\"Apply liquidity/price filters\", value=True)\n\n    if st.button(\"Refresh universe lists now\"):\n        results = universe_registry.refresh_all(force=True)\n        st.success(\"Universe registry refresh complete.\")\n        st.json(results)\n\n        row_counts = {}\n        symbol_samples = {}\n        universes = getattr(universe_registry, \"_UNIVERSES\", {})\n        for name, status in results.items():\n            if isinstance(status, str) and status.lower().startswith(\"error:\"):\n                st.markdown(f\"**{name}**\")\n                st.error(status)\n                continue\n            definition = universes.get(name)\n            if not definition:\n                continue\n            try:\n                df = universe_registry.load_universe(name)\n                row_counts[name] = len(df)\n                symbol_samples[name] = df[\"symbol\"].head(5).tolist()\n            except Exception as exc:\n                st.markdown(f\"**{name}**\")\n                st.error(f\"error: {exc}\")\n\n        if row_counts:\n            st.write(\"Latest universe row counts:\")\n            st.json(row_counts)\n        if symbol_samples:\n            st.write(\"Sample symbols (first 5):\")\n            st.json(symbol_samples)\n\n    st.markdown(\n        \"This demo builds a simplified, AI-driven S&P 500 portfolio using \"\n        \"auto-generated features, fitted weights, and quick performance metrics. \"\n        \"Click **Run Weekly Cycle** to execute a full pass.\"\n    )\n\n    if st.button(\"Run Weekly Cycle\"):\n        try:\n            spec_path = Path(\"spec/current_spec.json\")\n            spec_data = json.loads(spec_path.read_text()) if spec_path.exists() else {}\n\n            decision_info = None\n            selection_cfg = spec_data.get(\"universe_selection\", {})\n            if universe_mode == \"auto\":\n                candidates = selection_cfg.get(\"candidates\") or spec_data.get(\"universe\", {}).get(\n                    \"modes\", []\n                )\n                if not candidates:\n                    candidates = universe_choices\n                constraints = selection_cfg.get(\"constraints\", {}) or {}\n                decision_info = universe_selector.choose_universe(\n                    list(candidates),\n                    constraints,\n                    universe_registry.load_universe,\n                    Path(\"metrics_history.json\"),\n                    spec_data,\n                    as_of,\n                )\n                selected_universe_name = decision_info.get(\"winner\") or candidates[0]\n            else:\n                selected_universe_name = manual_universe or universe_choices[default_index]\n\n            # === 1. Universe ===\n            try:\n                uni = universe.load_universe(\n                    selected_universe_name, apply_filters=apply_filters\n                )\n            except universe_registry.UniverseRegistryError as exc:\n                st.error(str(exc))\n                st.stop()\n            except Exception as exc:\n                st.error(f\"Failed to load {selected_universe_name}: {exc}\")\n                st.stop()\n\n            filter_meta = uni.attrs.get(\"universe_filter_meta\", {})\n            raw_count = int(filter_meta.get(\"raw_count\", len(uni)))\n            filtered_count = int(filter_meta.get(\"filtered_count\", len(uni)))\n            filters_applied = bool(filter_meta.get(\"filters_applied\", False))\n            reason = str(filter_meta.get(\"reason\", \"\")) if filter_meta else \"\"\n\n            st.write(f\"Universe size (raw): {raw_count}\")\n            st.write(\"Universe size (post-filter): {}\".format(filtered_count))\n            if not filters_applied and reason:\n                st.info(reason)\n\n            if decision_info:\n                metrics_table = decision_info[\"metrics\"].copy()\n                metrics_table = metrics_table.reindex(decision_info.get(\"candidates\", metrics_table.index))\n                display_cols = [\n                    \"alpha\",\n                    \"sortino\",\n                    \"mdd\",\n                    \"coverage\",\n                    \"turnover_cost\",\n                    \"n_weeks\",\n                ]\n                for col in display_cols:\n                    if col not in metrics_table.columns:\n                        metrics_table[col] = np.nan\n                summary_df = metrics_table[display_cols].copy()\n                score_series = pd.Series(decision_info[\"scores\"])\n                prob_series = pd.Series(decision_info[\"probabilities\"])\n                summary_df[\"score\"] = score_series.reindex(summary_df.index)\n                summary_df[\"probability\"] = prob_series.reindex(summary_df.index)\n                summary_df = summary_df.fillna(0.0)\n\n                def _highlight(row):\n                    if row.name == selected_universe_name:\n                        return [\"background-color: #0b5394; color: white\"] * len(row)\n                    return [\"\"] * len(row)\n\n                st.subheader(\"Universe decision (auto)\")\n                st.dataframe(\n                    summary_df.style.format(\n                        {\n                            \"alpha\": \"{:.4f}\",\n                            \"sortino\": \"{:.2f}\",\n                            \"mdd\": \"{:.2%}\",\n                            \"coverage\": \"{:.1%}\",\n                            \"turnover_cost\": \"{:.4%}\",\n                            \"probability\": \"{:.1%}\",\n                            \"score\": \"{:.4f}\",\n                        }\n                    ).apply(_highlight, axis=1)\n                )\n                st.caption(decision_info.get(\"rationale\", \"\"))\n\n            symbols = [s for s in uni[\"symbol\"].tolist() if isinstance(s, str)]\n            coverage_current = 0.0\n            if len(uni) > 0:\n                min_expected = universe_registry.expected_min_constituents(selected_universe_name)\n                coverage_current = (\n                    float(len(uni)) / float(min_expected) if min_expected else 0.0\n                )\n                coverage_current = float(min(1.0, coverage_current))\n            if \"SPY\" not in symbols:\n                symbols.append(\"SPY\")\n            max_symbols = 150\n            if len(symbols) > max_symbols:\n                st.info(f\"Capping universe to first {max_symbols} symbols for runtime safety.\")\n                symbols = symbols[:max_symbols]\n            st.write(f\"Universe size: {len(symbols)}\")\n\n            # === 2. Data ===\n            try:\n                prices = dataops.fetch_prices(symbols, years=5)\n                dataops.cache_parquet(prices, f\"prices_{selected_universe_name.lower()}\")\n            except Exception:\n                # fabricate dummy data if network or API fails\n                idx = pd.date_range(end=date.today(), periods=252 * 5, freq=\"B\")\n                prices = pd.DataFrame(\n                    np.cumprod(1 + np.random.randn(len(idx), len(symbols)) * 0.001, axis=0),\n                    index=idx,\n                    columns=symbols,\n                )\n            st.write(\"Downloaded prices:\", prices.shape)\n\n            # === 3. Feature engineering ===\n            try:\n                feats = features.combine_features(prices)\n                if feats.isna().all().all():\n                    raise ValueError(\"Empty features\")\n            except Exception:\n                # fallback: fabricate random standardized features\n                feats = pd.DataFrame(\n                    np.random.randn(len(prices.columns), 6),\n                    index=prices.columns,\n                    columns=[\n                        \"mom_6m\", \"value_ey\", \"quality_roic\",\n                        \"risk_beta\", \"eps_rev_3m\", \"news_sent\",\n                    ],\n                )\n\n            feats = feats.fillna(0.0)\n\n            # === 4. Forward-return target (robust) ===\n            rets = prices.pct_change().dropna(how=\"all\")\n            try:\n                fwd5 = (1 + rets).rolling(5, min_periods=5).apply(lambda x: x.prod() - 1).shift(-5)\n                fwd5 = fwd5.iloc[:-5] if len(fwd5) >= 5 else fwd5.iloc[0:0]\n                last_valid = fwd5.dropna(how=\"all\").iloc[-1] if not fwd5.dropna(how=\"all\").empty else None\n                fwd_target = last_valid if last_valid is not None else pd.Series(0.0, index=feats.columns)\n            except Exception:\n                fwd_target = pd.Series(0.0, index=feats.columns)\n            fwd_target.name = \"fwd_5d\"\n\n            # === 5. Signals ===\n            feature_history = {}\n            hist_returns = fwd5.dropna(how=\"all\")\n            history_dates = hist_returns.tail(signals.ROLLING_WEEKS).index\n            for ts in history_dates:\n                price_slice = prices.loc[:ts]\n                if price_slice.empty:\n                    continue\n                try:\n                    feature_snapshot = features.combine_features(price_slice)\n                    feature_history[ts] = feature_snapshot\n                except Exception:\n                    continue\n\n            try:\n                rolling_returns = hist_returns.loc[history_dates]\n                w_ridge = signals.fit_rolling_ridge(rolling_returns, feature_history)\n                if w_ridge.empty:\n                    w_ridge = signals.fit_ridge(feats, fwd_target)\n                scores = signals.score_current(feats, w_ridge)\n            except Exception:\n                # fallback: random scores\n                scores = pd.Series(np.random.randn(len(feats.index)), index=feats.index)\n\n            st.subheader(\"Top candidates\")\n            st.dataframe(scores.head(20).to_frame())\n\n            weights_path = Path(signals.RUNS_DIR) / \"feature_weights.json\"\n            if weights_path.exists():\n                try:\n                    weights_json = json.loads(weights_path.read_text())\n                    st.subheader(\"Feature weights (smoothed)\")\n                    st.dataframe(pd.Series(weights_json.get(\"weights\", {}), name=\"weight\"))\n                except Exception:\n                    st.warning(\"Unable to load feature weights cache.\")\n\n            # === 6. Portfolio ===\n            returns_252 = prices.pct_change().iloc[-252:]\n            topN = scores.head(20).index.tolist()\n            if len(topN) == 0:\n                st.warning(\"No scored candidates, fabricating dummy weights.\")\n                topN = list(prices.columns[:15])\n            try:\n                w0 = portfolio.inverse_vol_weights(\n                    returns_252, topN, cap_single=0.10, k=min(15, len(topN))\n                )\n            except Exception:\n                w0 = pd.Series(1 / len(topN), index=topN)\n\n            # Sector cap + turnover handling\n            try:\n                sector_map = uni.set_index(\"symbol\")[\"sector\"]\n                w1 = portfolio.apply_sector_caps(w0, sector_map, cap=0.35)\n            except Exception:\n                w1 = w0\n\n            last = memory.load_last_portfolio()\n            last_w = pd.Series(\n                {h[\"ticker\"]: h[\"weight\"] for h in last.get(\"holdings\", [])}\n            ) if last else None\n            try:\n                w_final = portfolio.enforce_turnover(last_w, w1, t_cap=0.30)\n            except Exception:\n                w_final = w1\n\n            w_final = (w_final / w_final.sum()).sort_values(ascending=False)\n            try:\n                turnover_fraction = float(portfolio.turnover(last_w, w_final))\n            except Exception:\n                turnover_fraction = float(w_final.abs().sum()) if w_final is not None else 0.0\n            port = {\n                \"as_of\": str(as_of),\n                \"holdings\": [{\"ticker\": t, \"weight\": float(w_final[t])} for t in w_final.index],\n                \"cash_weight\": float(max(0.0, 1.0 - w_final.sum())),\n            }\n            memory.save_portfolio(port)\n            st.success(\"Weekly portfolio created.\")\n            st.json(port)\n\n            # === 7. Evaluation ===\n            port_rets = (\n                (returns_252[w_final.index] * w_final.reindex(returns_252.columns, fill_value=0.0))\n                .sum(axis=1)\n                .fillna(0.0)\n            )\n            curve = (1 + port_rets).cumprod()\n            mdd = metrics.max_drawdown(curve) if len(curve) > 0 else 0.0\n            sor = metrics.sortino(port_rets) if len(port_rets) > 0 else 0.0\n            bench = returns_252.get(\"SPY\", pd.Series(0.0, index=returns_252.index))\n            alpha = metrics.alpha_vs_bench(port_rets, bench) if not bench.empty else 0.0\n\n            st.subheader(\"Weekly metrics\")\n            st.write(\n                f\"- Sortino: **{sor:.2f}**  \\n\"\n                f\"- Max Drawdown: **{mdd:.2%}**  \\n\"\n                f\"- Alpha vs SPY (weekly mean): **{alpha:.4%}**\"\n            )\n\n            # === 8. Report ===\n            note = (\n                f\"# Weekly AI Portfolio \u2014 {as_of}\\n\\n\"\n                f\"- Sortino: {sor:.2f}\\n\"\n                f\"- Max Drawdown: {mdd:.2%}\\n\"\n                f\"- Alpha (vs SPY, weekly mean): {alpha:.4%}\\n\"\n            )\n            out = report.write_markdown(note)\n            st.download_button(\n                \"Download weekly report\",\n                data=open(out, \"rb\"),\n                file_name=out.split(\"/\")[-1],\n            )\n            # === 9. Log Metrics for Evaluator ===\n            val = metrics.val_metrics(port_rets, bench)\n            spec_version = str(spec_data.get(\"version\", \"0.4\"))\n            if not spec_version.startswith(\"v\"):\n                spec_version = f\"v{spec_version}\"\n\n            metrics_record = {\n                \"spec\": spec_version,\n                \"date\": str(as_of),\n                \"alpha\": float(alpha),\n                \"sortino\": float(sor),\n                \"mdd\": float(mdd),\n                \"hit_rate\": float((port_rets > bench).mean()),\n                \"val_sortino\": float(val.get(\"val_sortino\", float(\"nan\"))),\n                \"val_alpha\": float(val.get(\"val_alpha\", float(\"nan\"))),\n                \"universe\": selected_universe_name,\n                \"coverage\": float(coverage_current),\n                \"turnover_cost\": float(0.0005 * turnover_fraction if turnover_fraction else 0.0),\n            }\n\n            metrics_file = Path(\"metrics_history.json\")\n            if metrics_file.exists():\n                try:\n                    history = json.loads(metrics_file.read_text())\n                    if not isinstance(history, list):\n                        history = []\n                except json.JSONDecodeError:\n                    history = []\n            else:\n                history = []\n\n            history.append(metrics_record)\n            with open(metrics_file, \"w\") as f:\n                json.dump(history, f, indent=2)\n\n            st.success(\"Metrics logged to metrics_history.json\")\n        \n        except Exception as e:\n            st.error(f\"Run failed: {e}\")\n\nif __name__ == \"__main__\":\n    try:\n        main()\n    except KeyboardInterrupt:\n        pass\n",
  "data/reference/snapshots/ftse100_wikipedia.html": "<html>\n  <body>\n    <h2>FTSE 100 companies</h2>\n    <table class=\"wikitable\">\n      <thead>\n        <tr>\n          <th>EPIC</th>\n          <th>Company</th>\n          <th>Sector</th>\n        </tr>\n      </thead>\n      <tbody>\n        <tr><td>AZN</td><td>AstraZeneca plc</td><td>Health Care</td></tr>\n        <tr><td>BP.</td><td>BP plc</td><td>Energy</td></tr>\n      </tbody>\n    </table>\n  </body>\n</html>\n",
  "data/reference/snapshots/ftse250_wikipedia.html": "<html>\n  <body>\n    <h2>FTSE 250 companies</h2>\n    <table class=\"wikitable\">\n      <thead>\n        <tr>\n          <th>TIDM</th>\n          <th>Company</th>\n          <th>Sector</th>\n        </tr>\n      </thead>\n      <tbody>\n        <tr><td>SMIN</td><td>Smiths Group plc</td><td>Industrials</td></tr>\n        <tr><td>WDH</td><td>Warehouse REIT plc</td><td>Real Estate</td></tr>\n      </tbody>\n    </table>\n  </body>\n</html>\n",
  "src/codegen/file_contract.py": "SPEC_REQUIRED_KEYS = {\n  \"as_of\",\"universe\",\"data\",\"features\",\"weights\",\"risk\",\"evaluation\",\"change_proposal\"\n}\nALLOWED_DIRS = {\"src\", \"tests\", \"app.py\", \"prompts\", \"data/reference\"}\n",
  "src/dataops.py": "import hashlib\nfrom datetime import datetime, timedelta\nfrom typing import Sequence\n\nimport pandas as pd\nimport yfinance as yf\n\nfrom .config import CACHE_DIR\nfrom .universe import load_universe as _load_universe\n\n\ndef fetch_prices(symbols, years=5, interval=\"1d\") -> pd.DataFrame:\n    start = (datetime.utcnow() - timedelta(days=365 * years)).strftime(\"%Y-%m-%d\")\n    tickers = \" \".join(symbols)\n    df = yf.download(tickers=tickers, start=start, interval=interval, auto_adjust=True, threads=True)[\"Close\"]\n    if isinstance(df, pd.Series):\n        df = df.to_frame()\n    df = df.dropna(how=\"all\").sort_index()\n    df.columns = [c.replace(\" \", \"\") for c in df.columns]\n    return df\n\ndef load_universe(mode: str = \"SP500\") -> pd.DataFrame:\n    return _load_universe(mode)\n\n\ndef cache_parquet(df: pd.DataFrame, name: str) -> str:\n    path = CACHE_DIR / f\"{name}.parquet\"\n    df.to_parquet(path)\n    return str(path)\n\n\ndef compute_adv_from_prices_approx(prices: pd.DataFrame) -> pd.Series:\n    \"\"\"Return an approximate ADV series when dollar volume metadata is available.\"\"\"\n\n    meta = getattr(prices, \"attrs\", {}) or {}\n    dollar_volume = meta.get(\"daily_dollar_volume\")\n    if isinstance(dollar_volume, pd.Series):\n        return dollar_volume\n    if isinstance(dollar_volume, dict):\n        return pd.Series(dollar_volume)\n    return pd.Series(index=getattr(prices, \"columns\", None), dtype=\"float64\")\n\n\ndef _stable_int(symbol: str) -> int:\n    \"\"\"Create a deterministic integer from a ticker symbol.\"\"\"\n    digest = hashlib.sha256(symbol.encode(\"utf-8\")).hexdigest()\n    return int(digest[:12], 16)\n\n\ndef fetch_fundamentals(symbols: Sequence[str]) -> pd.DataFrame:\n    \"\"\"Return deterministic fundamental metrics for each symbol.\"\"\"\n    rows = []\n    for sym in symbols:\n        token = _stable_int(sym)\n        pe_ratio = 8.0 + (token % 2500) / 150.0\n        dividend_yield = ((token // 17) % 600) / 10000.0\n        roe = 0.04 + ((token // 131) % 400) / 1000.0\n        debt_to_equity = 0.15 + ((token // 19) % 220) / 120.0\n        rows.append({\n            \"symbol\": sym,\n            \"pe_ratio\": round(pe_ratio, 4),\n            \"dividend_yield\": round(dividend_yield, 4),\n            \"roe\": round(roe, 4),\n            \"debt_to_equity\": round(debt_to_equity, 4),\n        })\n    df = pd.DataFrame(rows).set_index(\"symbol\")\n    return df\n\n\ndef fetch_news_sentiment(symbols: Sequence[str], window: int = 7) -> pd.Series:\n    \"\"\"Return a smoothed sentiment score in [-1, 1] for each symbol.\"\"\"\n    scores = {}\n    damp = max(0.2, 1.0 - min(window, 30) / 40.0)\n    for sym in symbols:\n        token = _stable_int(f\"{sym}:{window}\")\n        raw = ((token % 2001) / 1000.0) - 1.0\n        score = max(-1.0, min(1.0, round(raw * damp, 4)))\n        scores[sym] = score\n    return pd.Series(scores, name=\"news_sentiment\")\n",
  "src/universe.py": "from __future__ import annotations\n\nimport json\nimport logging\nfrom dataclasses import dataclass\nfrom typing import Dict, Iterable, Tuple\n\nimport pandas as pd\n\nfrom . import universe_registry\nfrom .config import REF_DIR, RUNS_DIR\nfrom .utils import load_sp500_symbols\n\nLOGGER = logging.getLogger(__name__)\n\nOHLCV_FILE = REF_DIR / \"ohlcv_latest.csv\"\n\nMIN_PRICE = 3.0\nMIN_ADV = 5_000_000\n_MIN_COVERAGE = 0.7\n\n\n@dataclass\nclass FilterMetadata:\n    raw_count: int\n    filtered_count: int\n    reason: str\n    filters_applied: bool\n\n    def to_dict(self) -> Dict[str, object]:\n        return {\n            \"raw_count\": self.raw_count,\n            \"filtered_count\": self.filtered_count,\n            \"reason\": self.reason,\n            \"filters_applied\": self.filters_applied,\n        }\n\n\ndef _load_base(mode: str) -> pd.DataFrame:\n    mode = mode.upper()\n    if mode == \"SP500\":\n        df = load_sp500_symbols()\n        df[\"symbol\"] = df[\"symbol\"].str.upper().str.strip()\n        return df\n    if mode in universe_registry.registry_list():\n        try:\n            df = universe_registry.load_universe(mode)\n        except universe_registry.UniverseRegistryError:\n            if mode == \"SP500_MINI\":\n                df = load_sp500_symbols().head(5).copy()\n                df[\"symbol\"] = df[\"symbol\"].str.upper().str.strip()\n                mini_path = REF_DIR / \"sp500_mini.csv\"\n                mini_path.parent.mkdir(parents=True, exist_ok=True)\n                df.to_csv(mini_path, index=False)\n                return df\n            raise\n        df[\"symbol\"] = df[\"symbol\"].str.upper().str.strip()\n        return df\n    raise ValueError(f\"Unknown universe mode: {mode}\")\n\n\ndef _ensure_mini_cache(df: pd.DataFrame) -> None:\n    mini_path = REF_DIR / \"sp500_mini.csv\"\n    try:\n        mini_path.parent.mkdir(parents=True, exist_ok=True)\n        if not mini_path.exists():\n            df.head(5).to_csv(mini_path, index=False)\n    except Exception as exc:\n        LOGGER.debug(\"Unable to cache mini universe: %s\", exc)\n\n\ndef _load_ohlcv_snapshot(min_rows: int) -> Tuple[pd.DataFrame, str]:\n    if not OHLCV_FILE.exists():\n        msg = f\"Liquidity filters skipped: missing OHLCV snapshot at {OHLCV_FILE}.\"\n        LOGGER.info(msg)\n        return pd.DataFrame(), msg\n    try:\n        df = pd.read_csv(OHLCV_FILE)\n    except Exception as exc:\n        msg = f\"Liquidity filters skipped: failed to load OHLCV snapshot ({exc}).\"\n        LOGGER.warning(msg)\n        return pd.DataFrame(), msg\n    df[\"symbol\"] = df[\"symbol\"].str.upper().str.strip()\n    if len(df) < max(1, min_rows):\n        msg = (\n            \"Liquidity filters skipped: OHLCV snapshot too small \"\n            f\"({len(df)} rows < {min_rows}).\"\n        )\n        LOGGER.info(msg)\n        return pd.DataFrame(), msg\n    return df, \"\"\n\n\ndef _extract_adv(snapshot: pd.DataFrame) -> Tuple[pd.Series, str]:\n    if \"adv_usd\" in snapshot.columns:\n        return snapshot[\"adv_usd\"], \"\"\n    volume_candidates = [\n        col\n        for col in snapshot.columns\n        if col.lower() in {\"volume\", \"volume_30d\", \"avg_volume_30d\", \"average_volume_30d\"}\n    ]\n    if volume_candidates:\n        volume_col = volume_candidates[0]\n        try:\n            adv_series = snapshot[\"close\"] * snapshot[volume_col]\n            return adv_series, \"\"\n        except Exception as exc:  # pragma: no cover - defensive\n            return pd.Series(dtype=\"float64\"), (\n                f\"Liquidity filters skipped: failed to compute ADV ({exc}).\"\n            )\n    return pd.Series(dtype=\"float64\"), \"Liquidity filters skipped: ADV data missing in snapshot.\"\n\n\ndef _apply_filters(base: pd.DataFrame, mode: str) -> Tuple[pd.DataFrame, FilterMetadata]:\n    meta = FilterMetadata(\n        raw_count=len(base),\n        filtered_count=len(base),\n        reason=\"\",\n        filters_applied=False,\n    )\n    if base.empty:\n        meta.reason = \"Base universe empty; skipping liquidity filters.\"\n        return base, meta\n\n    min_rows = universe_registry.expected_min_constituents(mode)\n    snapshot, snapshot_reason = _load_ohlcv_snapshot(min_rows)\n    if snapshot.empty:\n        meta.reason = snapshot_reason\n        return base, meta\n\n    required_columns = {\"symbol\", \"close\"}\n    if not required_columns.issubset(snapshot.columns):\n        missing = sorted(required_columns - set(snapshot.columns))\n        meta.reason = (\n            \"Liquidity filters skipped: OHLCV snapshot missing columns \"\n            f\"{', '.join(missing)}.\"\n        )\n        LOGGER.info(meta.reason)\n        return base, meta\n\n    adv_series, adv_reason = _extract_adv(snapshot)\n    if adv_reason:\n        meta.reason = adv_reason\n        LOGGER.info(meta.reason)\n        return base, meta\n\n    working = base.copy()\n    working[\"symbol\"] = working[\"symbol\"].str.upper().str.strip()\n    snapshot = snapshot.copy()\n    snapshot = snapshot[[\"symbol\", \"close\"]].copy().rename(columns={\"close\": \"last_price\"})\n    snapshot[\"adv_usd\"] = adv_series\n\n    merged = working.merge(snapshot, on=\"symbol\", how=\"left\")\n    coverage_mask = merged[\"last_price\"].notna() & merged[\"adv_usd\"].notna()\n    coverage = float(coverage_mask.mean()) if len(merged) else 0.0\n    if coverage < _MIN_COVERAGE:\n        meta.reason = (\n            \"Liquidity filters skipped: insufficient OHLCV coverage \"\n            f\"({coverage:.0%} < {_MIN_COVERAGE:.0%}).\"\n        )\n        LOGGER.info(meta.reason)\n        return base, meta\n\n    mask = coverage_mask & (\n        (merged[\"last_price\"] >= MIN_PRICE) & (merged[\"adv_usd\"] >= MIN_ADV)\n    )\n    dropped = merged.loc[~mask & coverage_mask, \"symbol\"].dropna().tolist()\n    if dropped:\n        LOGGER.info(\n            \"Filtered %d symbols below thresholds (price >= %.2f, ADV >= %.0f): %s\",\n            len(dropped),\n            MIN_PRICE,\n            MIN_ADV,\n            \", \".join(sorted(dropped)),\n        )\n\n    filtered = merged.loc[mask].copy()\n    if filtered.empty:\n        meta.reason = (\n            \"Liquidity filters skipped: all symbols removed by thresholds; using raw universe.\"\n        )\n        LOGGER.warning(meta.reason)\n        return base, meta\n\n    meta.filters_applied = True\n    meta.filtered_count = len(filtered)\n    selected_columns = list(base.columns)\n    for extra in (\"last_price\", \"adv_usd\"):\n        if extra not in selected_columns:\n            selected_columns.append(extra)\n    filtered = filtered[selected_columns]\n    return filtered.reset_index(drop=True), meta\n\n\ndef _log_universe(mode: str, symbols: Iterable[str]) -> None:\n    RUNS_DIR.mkdir(parents=True, exist_ok=True)\n    symbols_list = list(symbols)\n    payload = {\n        \"mode\": mode,\n        \"count\": len(symbols_list),\n    }\n    try:\n        log_path = RUNS_DIR / \"last_universe.json\"\n        log_path.write_text(json.dumps(payload, indent=2))\n    except Exception as exc:\n        LOGGER.debug(\"Failed to write last_universe.json: %s\", exc)\n\n\ndef _load_and_filter_universe(\n    mode: str, apply_filters: bool\n) -> Tuple[pd.DataFrame, FilterMetadata, pd.DataFrame]:\n    base = _load_base(mode)\n    if base.empty:\n        if mode != \"SP500_MINI\":\n            raise universe_registry.UniverseRegistryError(\n                f\"Universe {mode} is empty after loading\"\n            )\n        base = base.head(5)\n\n    if not apply_filters:\n        reason = \"Liquidity filters bypassed via apply_filters=False.\"\n        return base, FilterMetadata(\n            raw_count=len(base),\n            filtered_count=len(base),\n            reason=reason,\n            filters_applied=False,\n        ), base\n\n    filtered, meta = _apply_filters(base, mode)\n    return filtered, meta, base\n\n\ndef load_universe_with_meta(\n    mode: str, apply_filters: bool = True\n) -> Tuple[pd.DataFrame, Dict[str, object]]:\n    \"\"\"Return the requested universe along with filter metadata.\"\"\"\n\n    mode = (mode or \"SP500\").upper()\n    filtered, meta, base = _load_and_filter_universe(mode, apply_filters)\n    _ensure_mini_cache(base)\n    if not meta.filters_applied and filtered is not None and filtered.empty:\n        filtered = base\n\n    result = filtered.copy()\n    if meta.filters_applied and result.empty:\n        LOGGER.warning(\n            \"Liquidity filters produced an empty universe for mode=%s; using raw symbols.\",\n            mode,\n        )\n        result = base\n        meta = FilterMetadata(\n            raw_count=len(base),\n            filtered_count=len(base),\n            reason=\"Liquidity filters skipped: empty result; using raw universe.\",\n            filters_applied=False,\n        )\n\n    return result, meta.to_dict()\n\n\ndef load_universe(mode: str, apply_filters: bool = True) -> pd.DataFrame:\n    \"\"\"Load the desired universe and attach filter metadata in ``.attrs``.\"\"\"\n\n    normalized_mode = (mode or \"SP500\").upper()\n    result, meta = load_universe_with_meta(normalized_mode, apply_filters=apply_filters)\n    result.attrs[\"universe_filter_meta\"] = meta\n    symbols = result.get(\"symbol\", [])\n    _log_universe(normalized_mode, symbols)\n    return result\n",
  "src/universe_registry.py": "from __future__ import annotations\n\nimport json\nimport logging\nfrom dataclasses import dataclass\nfrom datetime import datetime, timezone, timedelta\nfrom html.parser import HTMLParser\nfrom io import StringIO\nfrom pathlib import Path\nfrom typing import Callable, Dict, List, Optional, Tuple\n\nimport pandas as pd\nimport requests\nfrom pandas import DataFrame\nfrom requests import Response\nfrom requests.adapters import HTTPAdapter\nfrom urllib3.exceptions import InsecureRequestWarning\nfrom urllib3.util.retry import Retry\n\nfrom .config import REF_DIR\n\nLOGGER = logging.getLogger(__name__)\n\n# Silence only the insecure request warnings that can pop up on some Wikipedia mirrors\nrequests.packages.urllib3.disable_warnings(category=InsecureRequestWarning)  # type: ignore[attr-defined]\n\nProviderFn = Callable[[Optional[Path]], DataFrame]\n\n\ndef _build_session() -> requests.Session:\n    session = requests.Session()\n    session.headers.update(\n        {\n            \"User-Agent\": (\n                \"Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) \"\n                \"AppleWebKit/537.36 (KHTML, like Gecko) \"\n                \"Chrome/123.0.0.0 Safari/537.36\"\n            )\n        }\n    )\n    retries = Retry(total=3, backoff_factor=0.5, status_forcelist=[429, 500, 502, 503, 504])\n    adapter = HTTPAdapter(max_retries=retries)\n    session.mount(\"https://\", adapter)\n    session.mount(\"http://\", adapter)\n    return session\n\n\n_SESSION = _build_session()\n\n\n@dataclass\nclass UniverseDefinition:\n    name: str\n    url: str\n    csv_filename: str\n    provider: Optional[ProviderFn]\n    refresh_days: int = 90\n\n    @property\n    def csv_path(self) -> Path:\n        return REF_DIR / self.csv_filename\n\n\n_COLUMN_NORMALISATION = {\n    \"symbol\": [\"symbol\", \"ticker\", \"code\", \"tickers\", \"epic\", \"tidm\", \"ric\"],\n    \"name\": [\"security\", \"name\", \"company\", \"constituent\", \"issuer\"],\n    \"sector\": [\n        \"gics sector\",\n        \"sector\",\n        \"industry\",\n        \"icb sector\",\n        \"icb industry\",\n        \"gics sub-industry\",\n    ],\n}\n\n_MIN_ROWS = {\n    \"SP500_FULL\": 450,\n    \"R1000\": 800,\n    \"NASDAQ_100\": 90,\n    \"FTSE_350\": 300,\n    \"SP500_MINI\": 5,\n}\n\n_SPEC_MIN_CACHE: Dict[Path, Dict[str, int]] = {}\n\n\ndef _load_spec_min_constraints(spec_path: Path) -> Dict[str, int]:\n    if spec_path in _SPEC_MIN_CACHE:\n        return _SPEC_MIN_CACHE[spec_path]\n\n    try:\n        data = json.loads(spec_path.read_text())\n        selection = data.get(\"universe_selection\", {})\n        constraints = selection.get(\"constraints\", {})\n        min_map = constraints.get(\"min_constituents\", {})\n        if isinstance(min_map, dict):\n            result = {\n                key: int(value)\n                for key, value in min_map.items()\n                if value is not None\n            }\n        else:\n            result = {}\n    except (FileNotFoundError, json.JSONDecodeError, TypeError, ValueError):\n        result = {}\n\n    _SPEC_MIN_CACHE[spec_path] = result\n    return result\n\n\ndef expected_min_constituents(\n    name: str, spec_path: Path = Path(\"spec/current_spec.json\")\n) -> int:\n    \"\"\"Return expected minimum count for a universe.\n\n    Preference order:\n        1. Spec-defined constraints (spec/current_spec.json).\n        2. Local fallback map based on historical expectations.\n        3. Default of 1 to avoid division by zero.\n    \"\"\"\n\n    min_constraints = _load_spec_min_constraints(spec_path)\n    if name in min_constraints:\n        return max(1, int(min_constraints[name]))\n    if \"default\" in min_constraints:\n        return max(1, int(min_constraints[\"default\"]))\n    if name in _MIN_ROWS:\n        return max(1, int(_MIN_ROWS[name]))\n    return 1\n\n\nclass UniverseRegistryError(RuntimeError):\n    \"\"\"Raised when a universe cannot be loaded from either live or cached data.\"\"\"\n\n\ndef registry_list() -> List[str]:\n    return list(_UNIVERSES.keys())\n\n\ndef _read_html(url: str, html_path: Optional[Path]) -> List[DataFrame]:\n    def _read_from_text(text: str) -> List[DataFrame]:\n        for flavor in (\"lxml\", \"bs4\"):\n            try:\n                return pd.read_html(StringIO(text), flavor=flavor)\n            except ImportError:\n                continue\n        return _parse_html_tables_basic(text)\n\n    if html_path is not None:\n        content = Path(html_path).read_text(encoding=\"utf-8\")\n        return _read_from_text(content)\n    try:\n        response: Response = _SESSION.get(url, timeout=10)\n        response.raise_for_status()\n    except requests.RequestException as exc:  # pragma: no cover - exercised via ValueError path\n        raise ValueError(f\"Failed to download universe page from {url}: {exc}\") from exc\n    return _read_from_text(response.text)\n\n\nclass _TableParser(HTMLParser):\n    def __init__(self) -> None:\n        super().__init__()\n        self.tables: List[List[Tuple[List[str], List[str]]]] = []\n        self._in_table = False\n        self._in_row = False\n        self._capture = False\n        self._current_table: List[Tuple[List[str], List[str]]] = []\n        self._current_row_cells: List[str] = []\n        self._current_row_types: List[str] = []\n        self._buffer: List[str] = []\n\n    def handle_starttag(self, tag: str, attrs) -> None:  # type: ignore[override]\n        tag = tag.lower()\n        if tag == \"table\":\n            self._in_table = True\n            self._current_table = []\n        elif self._in_table and tag == \"tr\":\n            self._in_row = True\n            self._current_row_cells = []\n            self._current_row_types = []\n        elif self._in_table and self._in_row and tag in {\"td\", \"th\"}:\n            self._capture = True\n            self._buffer = []\n            self._current_row_types.append(tag)\n\n    def handle_data(self, data: str) -> None:  # type: ignore[override]\n        if self._capture:\n            self._buffer.append(data)\n\n    def handle_endtag(self, tag: str) -> None:  # type: ignore[override]\n        tag = tag.lower()\n        if self._in_table and self._in_row and tag in {\"td\", \"th\"} and self._capture:\n            text = \"\".join(self._buffer).strip()\n            self._current_row_cells.append(text)\n            self._buffer = []\n            self._capture = False\n        elif self._in_table and tag == \"tr\" and self._in_row:\n            if self._current_row_cells:\n                self._current_table.append((self._current_row_types, self._current_row_cells))\n            self._in_row = False\n        elif tag == \"table\" and self._in_table:\n            if self._current_table:\n                self.tables.append(self._current_table)\n            self._in_table = False\n            self._current_table = []\n\n\ndef _parse_html_tables_basic(text: str) -> List[DataFrame]:\n    parser = _TableParser()\n    parser.feed(text)\n    frames: List[DataFrame] = []\n    for table in parser.tables:\n        if not table:\n            continue\n        header: Optional[List[str]] = None\n        rows: List[List[str]] = []\n        max_len = 0\n        for cell_types, cells in table:\n            max_len = max(max_len, len(cells))\n            if header is None and any(t == \"th\" for t in cell_types):\n                header = cells\n            else:\n                rows.append(cells)\n        if not rows:\n            continue\n        if header is None:\n            header = [f\"col_{idx}\" for idx in range(max_len)]\n        header = [col or f\"col_{idx}\" for idx, col in enumerate(header)]\n        normalised_rows = [row + [\"\"] * (max_len - len(row)) for row in rows]\n        frame = pd.DataFrame(normalised_rows, columns=header[:max_len])\n        frames.append(frame)\n    if not frames:\n        raise ValueError(\"No HTML tables could be parsed without lxml/bs4\")\n    return frames\n\n\ndef _flatten_columns(df: DataFrame) -> DataFrame:\n    if isinstance(df.columns, pd.MultiIndex):\n        df = df.copy()\n        df.columns = [\n            \" \".join(str(level).strip() for level in col if str(level) != \"nan\").strip()\n            for col in df.columns\n        ]\n    return df\n\n\ndef _normalize_headers(df: DataFrame) -> DataFrame:\n    df = _flatten_columns(df)\n    df = df.copy()\n    df.columns = [str(col).strip().lower() for col in df.columns]\n    return df\n\n\ndef _maybe_promote_first_row(df: DataFrame) -> DataFrame:\n    if df.empty:\n        return df\n    first_row = _to_str_series(df.iloc[0])\n    lowered_values = [value.lower() for value in first_row.tolist()]\n    candidate_values = {\n        value for values in _COLUMN_NORMALISATION.values() for value in values\n    }\n    if not any(value in candidate_values for value in lowered_values):\n        return df\n    df = df.iloc[1:].copy()\n    df.columns = [value or f\"col_{idx}\" for idx, value in enumerate(lowered_values)]\n    df.columns = [str(col).strip().lower() for col in df.columns]\n    return df\n\n\ndef _find_column(df: DataFrame, candidates: List[str]) -> Optional[str]:\n    lowered = {str(col).strip().lower(): col for col in df.columns}\n    for candidate in candidates:\n        if candidate in lowered:\n            return lowered[candidate]\n    for col in df.columns:\n        col_norm = str(col).strip().lower()\n        for candidate in candidates:\n            if candidate in col_norm:\n                return col\n    return None\n\n\ndef _to_str_series(s: pd.Series) -> pd.Series:\n    if not isinstance(s, pd.Series):\n        s = pd.Series(s)\n    s = s.astype(\"string\")\n    s = s.fillna(\"\").str.replace(r\"\\s+\", \" \", regex=True).str.strip()\n    s = s.str.replace(r\"\\[\\d+\\]\", \"\", regex=True).str.strip()\n    return s\n\n\ndef _ensure_l_suffix(sym: str) -> str:\n    sym = sym or \"\"\n    if not sym:\n        return sym\n    if sym.endswith(\".L\"):\n        return sym\n    if sym.endswith(\".\"):\n        return f\"{sym[:-1]}.L\"\n    return f\"{sym}.L\"\n\n\ndef _normalize_universe_df(df: pd.DataFrame, universe: str) -> pd.DataFrame:\n    df = _normalize_headers(df)\n    df = _maybe_promote_first_row(df)\n    df = df.replace({pd.NA: \"\", None: \"\"})\n\n    symbol_col = _find_column(df, _COLUMN_NORMALISATION[\"symbol\"])\n    if symbol_col is None:\n        raise ValueError(f\"No symbol-like column found while parsing {universe}\")\n    name_col = _find_column(df, _COLUMN_NORMALISATION[\"name\"])\n    sector_col = _find_column(df, _COLUMN_NORMALISATION[\"sector\"])\n\n    symbol = _to_str_series(df[symbol_col]).str.upper()\n    name = _to_str_series(df[name_col]) if name_col else pd.Series(\"\", index=df.index, dtype=\"string\")\n    sector = (\n        _to_str_series(df[sector_col])\n        if sector_col\n        else pd.Series(\"\", index=df.index, dtype=\"string\")\n    )\n\n    out = pd.DataFrame({\"symbol\": symbol, \"name\": name, \"sector\": sector})\n    out = out.loc[out[\"symbol\"] != \"\"].copy()\n\n    if universe.upper() == \"FTSE_350\":\n        out[\"symbol\"] = out[\"symbol\"].map(_ensure_l_suffix)\n\n    out = out.drop_duplicates(subset=\"symbol\", keep=\"first\")\n\n    return out[[\"symbol\", \"name\", \"sector\"]].reset_index(drop=True)\n\n\ndef _score_table(df: DataFrame) -> Tuple[int, int]:\n    score = 0\n    if _find_column(df, _COLUMN_NORMALISATION[\"name\"]):\n        score += 1\n    if _find_column(df, _COLUMN_NORMALISATION[\"sector\"]):\n        score += 1\n    return score, len(df)\n\ndef _prepare_candidate_table(table: DataFrame) -> DataFrame:\n    table = _normalize_headers(table)\n    table = _maybe_promote_first_row(table)\n    return table\n\n\ndef _extract_symbol_tables(url: str, html_path: Optional[Path]) -> List[DataFrame]:\n    tables = _read_html(url, html_path)\n    candidates: List[DataFrame] = []\n    for table in tables:\n        candidate = _prepare_candidate_table(table)\n        if _find_column(candidate, _COLUMN_NORMALISATION[\"symbol\"]) is None:\n            continue\n        candidates.append(candidate)\n    if not candidates:\n        raise ValueError(f\"No table with symbol-like column found for {url}\")\n    return candidates\n\n\ndef _extract_symbol_table(url: str, html_path: Optional[Path]) -> DataFrame:\n    candidates = _extract_symbol_tables(url, html_path)\n    best = max(candidates, key=_score_table)\n    return best\n\n\ndef fetch_sp500_full(html_path: Optional[Path] = None) -> DataFrame:\n    url = \"https://en.wikipedia.org/wiki/List_of_S%26P_500_companies\"\n    try:\n        table = _extract_symbol_table(url, html_path)\n        return _normalize_universe_df(table, \"SP500_FULL\")\n    except ValueError as exc:\n        raise ValueError(f\"SP500_FULL provider failed: {exc}\") from exc\n\n\ndef fetch_nasdaq_100(html_path: Optional[Path] = None) -> DataFrame:\n    url = \"https://en.wikipedia.org/wiki/NASDAQ-100\"\n    try:\n        table = _extract_symbol_table(url, html_path)\n        return _normalize_universe_df(table, \"NASDAQ_100\")\n    except ValueError as exc:\n        raise ValueError(f\"NASDAQ_100 provider failed: {exc}\") from exc\n\n\ndef fetch_r1000(html_path: Optional[Path] = None) -> DataFrame:\n    url = \"https://en.wikipedia.org/wiki/Russell_1000_Index\"\n    try:\n        table = _extract_symbol_table(url, html_path)\n        return _normalize_universe_df(table, \"R1000\")\n    except ValueError as exc:\n        raise ValueError(f\"R1000 provider failed: {exc}\") from exc\n\n\ndef _extract_largest_symbol_table(url: str, html_path: Optional[Path]) -> DataFrame:\n    candidates = _extract_symbol_tables(url, html_path)\n    largest = max(candidates, key=lambda frame: len(frame))\n    return largest\n\n\ndef _fetch_ftse_component(\n    url: str,\n    html_path: Optional[Path],\n    label: str,\n) -> DataFrame:\n    try:\n        table = _extract_largest_symbol_table(url, html_path)\n    except ValueError as exc:\n        raise ValueError(f\"{label} lookup failed: {exc}\") from exc\n    return _normalize_universe_df(table, \"FTSE_350\")\n\n\ndef fetch_ftse_350(\n    html_path: Optional[Path] = None,\n    *,\n    html_path_100: Optional[Path] = None,\n    html_path_250: Optional[Path] = None,\n) -> DataFrame:\n    url_100 = \"https://en.wikipedia.org/wiki/FTSE_100_Index\"\n    url_250 = \"https://en.wikipedia.org/wiki/FTSE_250_Index\"\n\n    if html_path is not None and html_path_100 is None and html_path_250 is None:\n        # Backwards compatibility with combined snapshot tests.\n        try:\n            tables = _extract_symbol_tables(\n                \"https://en.wikipedia.org/wiki/FTSE_350_Index\", html_path\n            )\n            normalized = [_normalize_universe_df(table, \"FTSE_350\") for table in tables]\n            combined = pd.concat(normalized, ignore_index=True)\n            combined = combined.drop_duplicates(subset=\"symbol\", keep=\"first\")\n            return combined.reset_index(drop=True)\n        except ValueError as exc:\n            raise ValueError(f\"FTSE_350 provider failed: {exc}\") from exc\n\n    errors: List[str] = []\n    frames: List[DataFrame] = []\n    components = [\n        (\"FTSE 100\", url_100, html_path_100),\n        (\"FTSE 250\", url_250, html_path_250),\n    ]\n\n    for label, url, path in components:\n        try:\n            frame = _fetch_ftse_component(url, path, label)\n        except ValueError as exc:\n            errors.append(str(exc))\n        else:\n            frames.append(frame)\n\n    if errors:\n        joined = \"; \".join(errors)\n        raise ValueError(f\"FTSE_350 provider failed: {joined}\")\n\n    combined = pd.concat(frames, ignore_index=True)\n    combined = combined.drop_duplicates(subset=\"symbol\", keep=\"first\")\n    return combined.reset_index(drop=True)[[\"symbol\", \"name\", \"sector\"]]\n\n\n_UNIVERSES: Dict[str, UniverseDefinition] = {\n    \"SP500_FULL\": UniverseDefinition(\n        name=\"SP500_FULL\",\n        url=\"https://en.wikipedia.org/wiki/List_of_S%26P_500_companies\",\n        csv_filename=\"sp500_full.csv\",\n        provider=fetch_sp500_full,\n        refresh_days=90,\n    ),\n    \"R1000\": UniverseDefinition(\n        name=\"R1000\",\n        url=\"https://en.wikipedia.org/wiki/Russell_1000_Index\",\n        csv_filename=\"r1000.csv\",\n        provider=fetch_r1000,\n        refresh_days=90,\n    ),\n    \"NASDAQ_100\": UniverseDefinition(\n        name=\"NASDAQ_100\",\n        url=\"https://en.wikipedia.org/wiki/NASDAQ-100\",\n        csv_filename=\"nasdaq_100.csv\",\n        provider=fetch_nasdaq_100,\n        refresh_days=60,\n    ),\n    \"FTSE_350\": UniverseDefinition(\n        name=\"FTSE_350\",\n        url=\"https://en.wikipedia.org/wiki/FTSE_350_Index\",\n        csv_filename=\"ftse_350.csv\",\n        provider=fetch_ftse_350,\n        refresh_days=60,\n    ),\n    \"SP500_MINI\": UniverseDefinition(\n        name=\"SP500_MINI\",\n        url=\"\",\n        csv_filename=\"sp500_mini.csv\",\n        provider=None,\n        refresh_days=0,\n    ),\n}\n\n\ndef _should_refresh(definition: UniverseDefinition, force: bool) -> bool:\n    if force:\n        return True\n    path = definition.csv_path\n    if not path.exists():\n        return True\n    if definition.refresh_days <= 0:\n        return False\n    modified = datetime.fromtimestamp(path.stat().st_mtime, tz=timezone.utc)\n    age = datetime.now(tz=timezone.utc) - modified\n    return age > timedelta(days=definition.refresh_days)\n\n\ndef _ensure_directory(path: Path) -> None:\n    path.parent.mkdir(parents=True, exist_ok=True)\n\n\ndef _write_csv(definition: UniverseDefinition, df: DataFrame) -> None:\n    _ensure_directory(definition.csv_path)\n    df.to_csv(definition.csv_path, index=False)\n\n\ndef _load_csv(definition: UniverseDefinition) -> DataFrame:\n    if not definition.csv_path.exists():\n        raise UniverseRegistryError(\n            f\"No cached CSV found for {definition.name} at {definition.csv_path}\"\n        )\n    df = pd.read_csv(definition.csv_path)\n    try:\n        normalized = _normalize_universe_df(df, definition.name)\n    except ValueError as exc:\n        raise UniverseRegistryError(\n            f\"Cached CSV for {definition.name} could not be parsed: {exc}\"\n        ) from exc\n    expected_columns = {\"symbol\", \"name\", \"sector\"}\n    missing = expected_columns.difference(normalized.columns)\n    if missing:\n        raise UniverseRegistryError(\n            f\"Cached CSV for {definition.name} is missing columns: {sorted(missing)}\"\n        )\n    return normalized\n\n\ndef refresh_universe(name: str, force: bool = False) -> Tuple[DataFrame, str]:\n    name = (name or \"\").upper()\n    if name not in _UNIVERSES:\n        raise ValueError(f\"Unknown universe: {name}\")\n    definition = _UNIVERSES[name]\n\n    if definition.provider is None:\n        df = _load_csv(definition)\n        return df, \"cache\"\n\n    needs_refresh = _should_refresh(definition, force)\n    if needs_refresh:\n        try:\n            df = definition.provider(None)\n            _write_csv(definition, df)\n            return df, \"live\"\n        except ValueError as exc:\n            LOGGER.warning(\"Live refresh failed for %s: %s\", name, exc)\n            try:\n                cached = _load_csv(definition)\n            except UniverseRegistryError:\n                raise UniverseRegistryError(\n                    f\"Unable to refresh {name}: {exc}. No cached data available.\"\n                ) from exc\n            return cached, \"cache\"\n    cached = _load_csv(definition)\n    return cached, \"cache\"\n\n\ndef load_universe(name: str, force_refresh: bool = False) -> DataFrame:\n    df, _ = refresh_universe(name, force=force_refresh)\n    return df\n\n\ndef refresh_all(force: bool = False) -> Dict[str, str]:\n    results: Dict[str, str] = {}\n    for name in registry_list():\n        try:\n            _, source = refresh_universe(name, force=force)\n            results[name] = source\n        except Exception as exc:  # pragma: no cover - used for runtime reporting\n            results[name] = f\"error: {exc}\"\n    return results\n\n\n__all__ = [\n    \"UniverseRegistryError\",\n    \"fetch_sp500_full\",\n    \"fetch_nasdaq_100\",\n    \"fetch_r1000\",\n    \"fetch_ftse_350\",\n    \"load_universe\",\n    \"refresh_all\",\n    \"refresh_universe\",\n    \"registry_list\",\n]\n",
  "src/universe_selector.py": "from __future__ import annotations\n\nimport json\nfrom dataclasses import dataclass\nfrom pathlib import Path\nfrom typing import Callable, Dict, Iterable, List, Mapping, MutableMapping, Tuple\n\nimport numpy as np\nimport pandas as pd\n\ntry:  # pragma: no cover - defensive import for optional dependency graph\n    from .universe_registry import expected_min_constituents as _expected_min_constituents\nexcept Exception:  # pragma: no cover - fallback if registry import fails\n\n    def _expected_min_constituents(name: str) -> int:\n        return 1\n\n\n_METRIC_COLUMNS = [\n    \"alpha\",\n    \"sortino\",\n    \"mdd\",\n    \"hit_rate\",\n    \"val_alpha\",\n    \"val_sortino\",\n    \"coverage\",\n    \"turnover_cost\",\n]\n\n\n@dataclass\nclass UniverseDecision:\n    \"\"\"Structured container for a selection outcome.\"\"\"\n\n    winner: str\n    scores: Dict[str, float]\n    probabilities: Dict[str, float]\n    rationale: str\n    parameters: Dict[str, float]\n    metrics: pd.DataFrame\n    as_of: str\n    spec_version: str\n    lookback_weeks: int\n    min_weeks: int\n    candidates: List[str]\n    coverage_now: Dict[str, float]\n\n    def to_log_record(self, fields: Iterable[str] | None = None) -> Dict[str, object]:\n        base: Dict[str, object] = {\n            \"as_of\": self.as_of,\n            \"spec\": self.spec_version,\n            \"winner\": self.winner,\n            \"candidates\": self.candidates,\n            \"scores\": self.scores,\n            \"probabilities\": self.probabilities,\n            \"rationale\": self.rationale,\n            \"lookback_weeks\": self.lookback_weeks,\n            \"min_weeks\": self.min_weeks,\n            \"parameters\": self.parameters,\n            \"coverage_now\": self.coverage_now,\n        }\n        if fields is None:\n            return base\n        selected: Dict[str, object] = {}\n        for field in fields:\n            if field in base:\n                selected[field] = base[field]\n        return selected\n\n\ndef _ensure_columns(df: pd.DataFrame, columns: Iterable[str]) -> pd.DataFrame:\n    \"\"\"Ensure all requested columns exist, padding with NaNs where required.\"\"\"\n\n    for col in columns:\n        if col not in df.columns:\n            df[col] = np.nan\n    return df\n\n\ndef compute_universe_metrics(\n    history_df: pd.DataFrame, lookback_weeks: int, min_weeks: int\n) -> pd.DataFrame:\n    \"\"\"Aggregate trailing realised metrics for each universe.\n\n    Parameters\n    ----------\n    history_df:\n        Raw metrics history with at least ``date`` and ``universe`` columns.\n    lookback_weeks:\n        Number of most recent rows per-universe to aggregate.\n    min_weeks:\n        Minimum required rows for informational purposes; sub-minimum universes\n        are still returned but labelled via ``n_weeks``.\n    \"\"\"\n\n    if history_df is None or history_df.empty:\n        empty = pd.DataFrame(columns=_METRIC_COLUMNS + [\"n_weeks\"])\n        empty.index.name = \"universe\"\n        return empty\n\n    df = history_df.copy()\n    if \"date\" in df.columns:\n        df[\"date\"] = pd.to_datetime(df[\"date\"], errors=\"coerce\")\n        df = df.sort_values([\"universe\", \"date\"])\n    else:\n        df = df.sort_values(\"universe\")\n\n    window = max(int(lookback_weeks or 0), 0)\n    grouped = []\n    for name, group in df.groupby(\"universe\"):\n        if window > 0:\n            group = group.tail(window)\n        grouped.append((name, group))\n\n    records: List[pd.Series] = []\n    for universe_name, group in grouped:\n        stats: Dict[str, float] = {}\n        for col in _METRIC_COLUMNS:\n            series = group[col] if col in group.columns else pd.Series(dtype=float)\n            series = series.dropna()\n            stats[col] = float(series.mean()) if not series.empty else float(\"nan\")\n        stats[\"n_weeks\"] = float(len(group))\n        records.append(pd.Series(stats, name=universe_name))\n\n    metrics_df = pd.DataFrame(records)\n    metrics_df.index.name = \"universe\"\n    metrics_df = metrics_df.astype({\"n_weeks\": float})\n    return metrics_df\n\n\ndef score_universes(\n    metrics_df: pd.DataFrame, weights: Mapping[str, float], temperature: float\n) -> Tuple[pd.Series, pd.Series]:\n    \"\"\"Score universes and convert to a probability distribution.\"\"\"\n\n    if metrics_df is None or metrics_df.empty:\n        return pd.Series(dtype=float), pd.Series(dtype=float)\n\n    df = _ensure_columns(metrics_df.copy(), _METRIC_COLUMNS)\n\n    def _w(key: str) -> float:\n        return float(weights.get(key, 0.0)) if isinstance(weights, Mapping) else 0.0\n\n    scores = (\n        _w(\"alpha\") * df[\"alpha\"].fillna(0.0)\n        + _w(\"sortino\") * df[\"sortino\"].fillna(0.0)\n        - _w(\"mdd\") * df[\"mdd\"].fillna(0.0)\n        + _w(\"coverage\") * df[\"coverage\"].fillna(0.0)\n        - _w(\"turnover\") * df[\"turnover_cost\"].fillna(0.0)\n    )\n\n    temp = float(temperature or 0.0)\n    if temp <= 0:\n        temp = 1.0\n\n    values = scores.to_numpy(dtype=float)\n    values = np.where(np.isfinite(values), values, 0.0)\n    if values.size == 0:\n        probabilities = np.array([], dtype=float)\n    else:\n        shifted = values / temp\n        shifted -= np.max(shifted) if shifted.size else 0.0\n        exps = np.exp(shifted)\n        total = float(exps.sum())\n        if total <= 0:\n            probabilities = np.full_like(exps, 1.0 / len(exps))\n        else:\n            probabilities = exps / total\n\n    probs_series = pd.Series(probabilities, index=scores.index, dtype=float)\n    return scores.astype(float), probs_series\n\n\ndef _load_history(metrics_history_path: Path) -> pd.DataFrame:\n    if not metrics_history_path.exists():\n        return pd.DataFrame()\n    try:\n        data = json.loads(metrics_history_path.read_text())\n    except json.JSONDecodeError:\n        return pd.DataFrame()\n    if not isinstance(data, list):\n        return pd.DataFrame()\n    if not data:\n        return pd.DataFrame()\n    df = pd.DataFrame(data)\n    if \"date\" in df.columns:\n        df[\"date\"] = pd.to_datetime(df[\"date\"], errors=\"coerce\")\n        df = df.dropna(subset=[\"date\"])\n    return df\n\n\ndef _append_decision_log(path: Path, record: Dict[str, object]) -> None:\n    path.parent.mkdir(parents=True, exist_ok=True)\n    try:\n        if path.exists():\n            existing = json.loads(path.read_text())\n            if isinstance(existing, list):\n                log: List[Dict[str, object]] = existing\n            else:\n                log = []\n        else:\n            log = []\n    except json.JSONDecodeError:\n        log = []\n\n    log.append(record)\n    path.write_text(json.dumps(log, indent=2, sort_keys=True))\n\n\ndef _resolve_min_constituents(\n    name: str,\n    constraints: MutableMapping[str, object] | Mapping[str, object] | None,\n) -> int:\n    min_map = {}\n    if constraints and isinstance(constraints, Mapping):\n        raw = constraints.get(\"min_constituents\")\n        if isinstance(raw, Mapping):\n            min_map = {k: int(v) for k, v in raw.items() if v is not None}\n\n    for key in (name, \"default\"):\n        if key in min_map and int(min_map[key]) > 0:\n            return max(1, int(min_map[key]))\n\n    return max(1, int(_expected_min_constituents(name)))\n\n\ndef _compute_coverage(\n    candidates: Iterable[str],\n    constraints: Mapping[str, object] | None,\n    registry_fn: Callable[[str], pd.DataFrame],\n) -> Dict[str, float]:\n    coverage: Dict[str, float] = {}\n    for name in candidates:\n        min_required = float(_resolve_min_constituents(name, constraints))\n        try:\n            frame = registry_fn(name)\n            current = float(len(frame)) if frame is not None else 0.0\n        except Exception:  # pragma: no cover - defensive, exercised in tests via mocks\n            current = 0.0\n        ratio = 0.0 if min_required <= 0 else current / min_required\n        coverage[name] = float(min(1.0, max(0.0, ratio)))\n    return coverage\n\n\ndef choose_universe(\n    candidates: List[str],\n    constraints: Dict[str, object],\n    registry_fn: Callable[[str], pd.DataFrame],\n    metrics_history_path: Path,\n    spec: Dict[str, object],\n    as_of,\n) -> Dict[str, object]:\n    selection_cfg = (spec or {}).get(\"universe_selection\", {}) or {}\n    lookback_weeks = int(selection_cfg.get(\"lookback_weeks\", selection_cfg.get(\"lookback\", 8)))\n    min_weeks = int(\n        selection_cfg.get(\"min_weeks\", selection_cfg.get(\"min_weeks_required\", 4))\n    )\n    weights = selection_cfg.get(\"weights\", {}) or {}\n    temperature = float(selection_cfg.get(\"temperature\", selection_cfg.get(\"softmax_temperature\", 1.0)))\n\n    history_df = _load_history(metrics_history_path)\n    if not history_df.empty:\n        history_df = history_df[history_df.get(\"universe\").isin(candidates)]\n\n    metrics_df = compute_universe_metrics(history_df, lookback_weeks, min_weeks)\n    metrics_df = metrics_df.reindex(candidates)\n    metrics_df.index.name = \"universe\"\n    metrics_df = _ensure_columns(metrics_df, _METRIC_COLUMNS + [\"n_weeks\"])\n    metrics_df[\"n_weeks\"] = metrics_df[\"n_weeks\"].fillna(0).astype(float)\n\n    coverage_now = _compute_coverage(candidates, constraints, registry_fn)\n    for name in metrics_df.index:\n        if pd.isna(metrics_df.at[name, \"coverage\"]):\n            metrics_df.at[name, \"coverage\"] = coverage_now.get(name, 0.0)\n    metrics_df[\"turnover_cost\"] = metrics_df[\"turnover_cost\"].fillna(0.0)\n\n    scores, probabilities = score_universes(metrics_df, weights, temperature)\n    if scores.empty:\n        scores = pd.Series(0.0, index=pd.Index(candidates, name=\"universe\"))\n        probabilities = pd.Series(\n            [1.0 / len(candidates)] * len(candidates),\n            index=pd.Index(candidates, name=\"universe\"),\n        )\n\n    winner = str(probabilities.idxmax()) if not probabilities.empty else candidates[0]\n\n    row = metrics_df.loc[winner]\n    contributions = {\n        \"alpha\": float(weights.get(\"alpha\", 0.0)) * float(row.get(\"alpha\", 0.0)),\n        \"sortino\": float(weights.get(\"sortino\", 0.0)) * float(row.get(\"sortino\", 0.0)),\n        \"mdd\": -float(weights.get(\"mdd\", 0.0)) * float(row.get(\"mdd\", 0.0)),\n        \"coverage\": float(weights.get(\"coverage\", 0.0)) * float(row.get(\"coverage\", 0.0)),\n        \"turnover\": -float(weights.get(\"turnover\", 0.0))\n        * float(row.get(\"turnover_cost\", 0.0)),\n    }\n\n    driver_labels = {\n        \"alpha\": \"alpha\",\n        \"sortino\": \"risk-adjusted returns\",\n        \"mdd\": \"drawdown control\",\n        \"coverage\": \"coverage\",\n        \"turnover\": \"turnover discipline\",\n    }\n    driver_bits: List[str] = []\n    for key, value in sorted(contributions.items(), key=lambda kv: abs(kv[1]), reverse=True)[\n        :2\n    ]:\n        if value == 0:\n            continue\n        direction = \"positive\" if value > 0 else \"negative\"\n        driver_bits.append(f\"{driver_labels.get(key, key)} ({direction} impact {value:.3f})\")\n\n    rationale_parts = [f\"Selected {winner}\"]\n    if driver_bits:\n        rationale_parts.append(\"; \".join(driver_bits))\n    if float(row.get(\"n_weeks\", 0.0)) < float(min_weeks):\n        rationale_parts.append(\n            f\"history limited to {int(row.get('n_weeks', 0))} of required {min_weeks} weeks\"\n        )\n    rationale = \" \u2014 \".join(rationale_parts)\n\n    decision = UniverseDecision(\n        winner=winner,\n        scores=scores.fillna(0.0).round(6).to_dict(),\n        probabilities=probabilities.fillna(0.0).round(6).to_dict(),\n        rationale=rationale,\n        parameters={\n            \"temperature\": temperature,\n            **{f\"w_{k}\": float(v) for k, v in weights.items()},\n        },\n        metrics=metrics_df,\n        as_of=str(as_of),\n        spec_version=str(spec.get(\"version\", \"\")),\n        lookback_weeks=lookback_weeks,\n        min_weeks=min_weeks,\n        candidates=list(candidates),\n        coverage_now=coverage_now,\n    )\n\n    logging_cfg = selection_cfg.get(\"logging\", {}) if isinstance(selection_cfg, Mapping) else {}\n    fields = None\n    if isinstance(logging_cfg, Mapping):\n        requested = logging_cfg.get(\"fields\")\n        if isinstance(requested, list):\n            fields = requested\n\n    decisions_path = metrics_history_path.parent / \"runs\" / \"universe_decisions.json\"\n    _append_decision_log(decisions_path, decision.to_log_record(fields))\n\n    return {\n        \"winner\": decision.winner,\n        \"scores\": decision.scores,\n        \"probabilities\": decision.probabilities,\n        \"rationale\": decision.rationale,\n        \"metrics\": decision.metrics,\n        \"candidates\": decision.candidates,\n        \"coverage_now\": decision.coverage_now,\n        \"log_path\": str(decisions_path),\n        \"parameters\": decision.parameters,\n        \"lookback_weeks\": decision.lookback_weeks,\n        \"min_weeks\": decision.min_weeks,\n    }\n",
  "tests/test_universe.py": "from pathlib import Path\n\nimport pandas as pd\nimport pytest\n\nfrom src import universe, universe_registry\n\n\n@pytest.fixture\ndef sample_base(monkeypatch):\n    base_df = pd.DataFrame({\"symbol\": [\"AAA\", \"BBB\", \"CCC\"]})\n\n    def _mock_base(mode: str) -> pd.DataFrame:\n        return base_df.copy()\n\n    monkeypatch.setattr(universe, \"_load_base\", _mock_base)\n    monkeypatch.setattr(universe, \"_ensure_mini_cache\", lambda df: None)\n    monkeypatch.setattr(universe, \"_log_universe\", lambda mode, symbols: None)\n\n    def _expected(name: str, spec_path: Path = Path(\"spec/current_spec.json\")) -> int:\n        return len(base_df)\n\n    monkeypatch.setattr(universe_registry, \"expected_min_constituents\", _expected)\n    return base_df\n\n\ndef test_filters_skipped_when_snapshot_missing(sample_base, monkeypatch, tmp_path):\n    missing_snapshot = tmp_path / \"missing.csv\"\n    monkeypatch.setattr(universe, \"OHLCV_FILE\", missing_snapshot)\n\n    df, meta = universe.load_universe_with_meta(\"TEST\", apply_filters=True)\n\n    assert df.equals(sample_base)\n    assert meta[\"filters_applied\"] is False\n    assert meta[\"raw_count\"] == len(sample_base)\n    assert meta[\"filtered_count\"] == len(sample_base)\n    assert \"missing\" in meta[\"reason\"].lower()\n\n\ndef test_filters_applied_with_valid_snapshot(sample_base, monkeypatch, tmp_path):\n    snapshot = pd.DataFrame(\n        {\n            \"symbol\": [\"AAA\", \"BBB\", \"CCC\"],\n            \"close\": [10.0, 2.0, 20.0],\n            \"adv_usd\": [10_000_000.0, 10_000_000.0, 1_000_000.0],\n        }\n    )\n    snapshot_path = tmp_path / \"ohlcv.csv\"\n    snapshot.to_csv(snapshot_path, index=False)\n    monkeypatch.setattr(universe, \"OHLCV_FILE\", snapshot_path)\n\n    df, meta = universe.load_universe_with_meta(\"TEST\", apply_filters=True)\n\n    assert meta[\"filters_applied\"] is True\n    assert meta[\"reason\"] == \"\"\n    assert meta[\"raw_count\"] == len(sample_base)\n    assert meta[\"filtered_count\"] < len(sample_base)\n    assert \"adv_usd\" in df.columns\n    assert \"last_price\" in df.columns\n    assert set(df[\"symbol\"]) == {\"AAA\"}\n\n\ndef test_app_bypass_checkbox(sample_base, monkeypatch):\n    df, meta = universe.load_universe_with_meta(\"TEST\", apply_filters=False)\n\n    assert df.equals(sample_base)\n    assert meta[\"filters_applied\"] is False\n    assert meta[\"filtered_count\"] == meta[\"raw_count\"]\n    assert \"bypass\" in meta[\"reason\"].lower()\n",
  "tests/test_universe_registry.py": "import json\nfrom pathlib import Path\n\nimport pandas as pd\nfrom pandas.api import types as ptypes\n\nfrom src import universe_registry\n\nSNAPSHOT_DIR = Path(\"data/reference/snapshots\")\n\n\ndef test_registry_known_universes():\n    expected = {\"SP500_MINI\", \"SP500_FULL\", \"R1000\", \"NASDAQ_100\", \"FTSE_350\"}\n    assert set(universe_registry.registry_list()) == expected\n\n\ndef test_parse_snapshots_offline():\n    snapshots = {\n        \"SP500_FULL\": universe_registry.fetch_sp500_full(\n            SNAPSHOT_DIR / \"sp500_wikipedia.html\"\n        ),\n        \"NASDAQ_100\": universe_registry.fetch_nasdaq_100(\n            SNAPSHOT_DIR / \"nasdaq100_wikipedia.html\"\n        ),\n        \"FTSE_350\": universe_registry.fetch_ftse_350(\n            html_path_100=SNAPSHOT_DIR / \"ftse100_wikipedia.html\",\n            html_path_250=SNAPSHOT_DIR / \"ftse250_wikipedia.html\",\n        ),\n        \"R1000\": universe_registry.fetch_r1000(\n            SNAPSHOT_DIR / \"r1000_wikipedia.html\"\n        ),\n    }\n\n    for name, frame in snapshots.items():\n        assert isinstance(frame, pd.DataFrame)\n        assert list(frame.columns) == [\"symbol\", \"name\", \"sector\"]\n        assert not frame.empty\n        assert len(frame) >= 3\n\n    assert snapshots[\"FTSE_350\"][\"symbol\"].str.endswith(\".L\").all()\n\n\ndef test_ftse350_composite_offline():\n    df = universe_registry.fetch_ftse_350(\n        html_path_100=SNAPSHOT_DIR / \"ftse100_wikipedia.html\",\n        html_path_250=SNAPSHOT_DIR / \"ftse250_wikipedia.html\",\n    )\n\n    assert list(df.columns) == [\"symbol\", \"name\", \"sector\"]\n    assert not df.empty\n    assert len(df) >= 4\n    assert df[\"symbol\"].str.endswith(\".L\").all()\n    assert df[\"symbol\"].is_unique\n\n\ndef test_ftse_suffixed_and_string_types():\n    df = universe_registry.fetch_ftse_350(\n        html_path_100=SNAPSHOT_DIR / \"ftse100_wikipedia.html\",\n        html_path_250=SNAPSHOT_DIR / \"ftse250_wikipedia.html\",\n    )\n\n    assert ptypes.is_string_dtype(df[\"symbol\"])\n    assert df[\"symbol\"].equals(df[\"symbol\"].str.upper())\n    assert df[\"symbol\"].str.endswith(\".L\").all()\n\n\ndef test_expected_min_constituents_defaults(tmp_path):\n    empty_spec = tmp_path / \"spec_empty.json\"\n    empty_spec.write_text(json.dumps({}))\n\n    assert universe_registry.expected_min_constituents(\n        \"SP500_FULL\", spec_path=empty_spec\n    ) == 450\n    assert universe_registry.expected_min_constituents(\n        \"SP500_MINI\", spec_path=empty_spec\n    ) == 5\n\n    override_spec = tmp_path / \"spec_override.json\"\n    override_spec.write_text(\n        json.dumps(\n            {\n                \"universe_selection\": {\n                    \"constraints\": {\n                        \"min_constituents\": {\n                            \"SP500_MINI\": 12,\n                            \"default\": 42,\n                        }\n                    }\n                }\n            }\n        )\n    )\n\n    assert universe_registry.expected_min_constituents(\n        \"SP500_MINI\", spec_path=override_spec\n    ) == 12\n    assert universe_registry.expected_min_constituents(\n        \"UNKNOWN\", spec_path=override_spec\n    ) == 42\n\n\ndef test_normalize_handles_integer_columns():\n    raw = pd.DataFrame(\n        {\n            \"Ticker\": [101, 202, 303, None],\n            \"Security\": [\"Alpha\", \" beta\", None, \"\"],\n            \"Sector\": [1, 2, None, 3],\n        }\n    )\n\n    normalized = universe_registry._normalize_universe_df(raw, \"R1000\")\n\n    assert len(normalized) == 3  # row with missing ticker dropped\n    assert ptypes.is_string_dtype(normalized[\"symbol\"])\n    assert normalized[\"symbol\"].equals(normalized[\"symbol\"].str.upper())\n\n\ndef test_provider_handles_integer_headers(tmp_path):\n    html = \"\"\"\n    <table>\n        <thead>\n            <tr><th>0</th><th>1</th><th>2</th></tr>\n            <tr><th>Symbol</th><th>Company</th><th>Sector</th></tr>\n        </thead>\n        <tbody>\n            <tr><td>aapl</td><td>Apple Inc</td><td>Technology</td></tr>\n            <tr><td>msft</td><td>Microsoft</td><td>Technology</td></tr>\n        </tbody>\n    </table>\n    \"\"\"\n    html_path = tmp_path / \"nasdaq_inline.html\"\n    html_path.write_text(html, encoding=\"utf-8\")\n\n    df = universe_registry.fetch_nasdaq_100(html_path)\n\n    assert list(df.columns) == [\"symbol\", \"name\", \"sector\"]\n    assert df.loc[0, \"symbol\"] == \"AAPL\"\n    assert df.loc[1, \"symbol\"] == \"MSFT\"\n\n\ndef test_refresh_writes_csvs(tmp_path, monkeypatch):\n    monkeypatch.setattr(universe_registry, \"REF_DIR\", tmp_path)\n\n    providers = {\n        \"SP500_FULL\": (universe_registry.fetch_sp500_full, \"sp500_wikipedia.html\"),\n        \"R1000\": (universe_registry.fetch_r1000, \"r1000_wikipedia.html\"),\n        \"NASDAQ_100\": (universe_registry.fetch_nasdaq_100, \"nasdaq100_wikipedia.html\"),\n        \"FTSE_350\": (\n            universe_registry.fetch_ftse_350,\n            (\"ftse100_wikipedia.html\", \"ftse250_wikipedia.html\"),\n        ),\n    }\n\n    for name, (fetcher, filename) in providers.items():\n        if name == \"FTSE_350\":\n            file_100, file_250 = filename\n\n            def _provider(\n                _html_path=None,\n                *,\n                func=fetcher,\n                path_100=SNAPSHOT_DIR / file_100,\n                path_250=SNAPSHOT_DIR / file_250,\n            ):\n                return func(html_path_100=path_100, html_path_250=path_250)\n\n        else:\n            snapshot_file = SNAPSHOT_DIR / filename\n\n            def _provider(_html_path=None, *, path=snapshot_file, func=fetcher):\n                return func(path)\n\n        monkeypatch.setattr(\n            universe_registry._UNIVERSES[name],  # type: ignore[attr-defined]\n            \"provider\",\n            _provider,\n            raising=False,\n        )\n        df, source = universe_registry.refresh_universe(name, force=True)\n        assert source == \"live\"\n        assert not df.empty\n        expected_path = tmp_path / universe_registry._UNIVERSES[name].csv_filename  # type: ignore[attr-defined]\n        assert expected_path.exists()\n",
  "tests/test_universe_selector.py": "import json\n\nimport pandas as pd\nimport pytest\n\nfrom src.universe_selector import (\n    choose_universe,\n    compute_universe_metrics,\n    score_universes,\n)\n\n\ndef _build_history() -> pd.DataFrame:\n    dates = pd.date_range(\"2024-01-05\", periods=6, freq=\"W-FRI\")\n    records = []\n    for idx, dt in enumerate(dates):\n        records.append(\n            {\n                \"date\": dt,\n                \"universe\": \"U1\",\n                \"alpha\": 0.10 + idx * 0.01,\n                \"sortino\": 1.5 + 0.1 * idx,\n                \"mdd\": 0.15 + 0.01 * idx,\n                \"hit_rate\": 0.55,\n                \"val_alpha\": 0.08,\n                \"val_sortino\": 1.1,\n                \"coverage\": 0.95,\n                \"turnover_cost\": 0.01,\n            }\n        )\n    for idx, dt in enumerate(dates[:2]):\n        records.append(\n            {\n                \"date\": dt,\n                \"universe\": \"U2\",\n                \"alpha\": 0.05 + idx * 0.02,\n                \"sortino\": 1.1 + 0.05 * idx,\n                \"mdd\": 0.10 + 0.02 * idx,\n                \"hit_rate\": 0.52,\n                \"val_alpha\": 0.04,\n                \"val_sortino\": 0.9,\n                \"turnover_cost\": 0.015,\n            }\n        )\n    for idx, dt in enumerate(dates[:4]):\n        records.append(\n            {\n                \"date\": dt,\n                \"universe\": \"U3\",\n                \"alpha\": 0.03 + idx * 0.015,\n                \"sortino\": 0.9 + 0.05 * idx,\n                \"mdd\": 0.18 + 0.015 * idx,\n                \"hit_rate\": 0.5,\n                \"val_alpha\": 0.02,\n                \"val_sortino\": 0.8,\n                \"coverage\": None,\n                \"turnover_cost\": 0.02,\n            }\n        )\n    return pd.DataFrame.from_records(records)\n\n\ndef test_compute_universe_metrics_respects_lookback():\n    history = _build_history()\n    metrics = compute_universe_metrics(history, lookback_weeks=4, min_weeks=3)\n\n    assert set(metrics.columns) == {\n        \"alpha\",\n        \"sortino\",\n        \"mdd\",\n        \"hit_rate\",\n        \"val_alpha\",\n        \"val_sortino\",\n        \"coverage\",\n        \"turnover_cost\",\n        \"n_weeks\",\n    }\n    assert metrics.loc[\"U1\", \"n_weeks\"] == 4\n    # Last four alpha values for U1 start at 0.12\n    expected_alpha = (0.12 + 0.13 + 0.14 + 0.15) / 4\n    assert metrics.loc[\"U1\", \"alpha\"] == pytest.approx(expected_alpha)\n    # Sparse universe still returns an entry\n    assert metrics.loc[\"U2\", \"n_weeks\"] == 2\n\n\ndef test_score_universes_softmax_and_ranking():\n    metrics_df = pd.DataFrame(\n        {\n            \"alpha\": [0.12, 0.05, 0.03],\n            \"sortino\": [1.4, 1.1, 0.9],\n            \"mdd\": [0.12, 0.08, 0.2],\n            \"coverage\": [0.9, 0.6, 0.3],\n            \"turnover_cost\": [0.01, 0.02, 0.03],\n        },\n        index=[\"U1\", \"U2\", \"U3\"],\n    )\n    weights = {\"alpha\": 0.4, \"sortino\": 0.25, \"mdd\": 0.2, \"coverage\": 0.1, \"turnover\": 0.05}\n\n    scores, probs = score_universes(metrics_df, weights, temperature=0.7)\n\n    assert pytest.approx(probs.sum(), rel=1e-6) == 1.0\n    ordered = scores.sort_values(ascending=False).index.tolist()\n    assert ordered[0] == \"U1\"\n    assert scores[\"U1\"] > scores[\"U2\"] > scores[\"U3\"]\n\n\ndef test_choose_universe_writes_log_and_returns_decision(tmp_path):\n    history = _build_history()\n    metrics_history_path = tmp_path / \"metrics_history.json\"\n    metrics_history_path.write_text(history.to_json(orient=\"records\"))\n\n    registry_frames = {\n        \"U1\": pd.DataFrame({\"symbol\": list(range(12))}),\n        \"U2\": pd.DataFrame({\"symbol\": list(range(6))}),\n        \"U3\": pd.DataFrame({\"symbol\": list(range(20))}),\n    }\n\n    def _registry(name: str) -> pd.DataFrame:\n        return registry_frames[name]\n\n    spec = {\n        \"version\": \"0.4\",\n        \"universe_selection\": {\n            \"lookback_weeks\": 4,\n            \"min_weeks\": 3,\n            \"weights\": {\n                \"alpha\": 0.4,\n                \"sortino\": 0.25,\n                \"mdd\": 0.2,\n                \"coverage\": 0.1,\n                \"turnover\": 0.05,\n            },\n            \"temperature\": 0.7,\n            \"constraints\": {\"min_constituents\": {\"U1\": 10, \"default\": 8}},\n            \"logging\": {\"fields\": [\"as_of\", \"winner\", \"coverage_now\"]},\n        },\n    }\n\n    candidates = [\"U1\", \"U2\", \"U3\"]\n    decision = choose_universe(\n        candidates,\n        spec[\"universe_selection\"].get(\"constraints\", {}),\n        _registry,\n        metrics_history_path,\n        spec,\n        \"2025-01-10\",\n    )\n\n    assert decision[\"winner\"] in candidates\n    assert decision[\"rationale\"].startswith(\"Selected\")\n    assert set(decision[\"scores\"].keys()) == set(candidates)\n    assert set(decision[\"probabilities\"].keys()) == set(candidates)\n    assert decision[\"coverage_now\"][\"U2\"] == pytest.approx(6 / 8)\n\n    log_path = tmp_path / \"runs\" / \"universe_decisions.json\"\n    assert log_path.exists()\n    history_log = json.loads(log_path.read_text())\n    assert history_log[-1][\"winner\"] == decision[\"winner\"]\n    assert set(history_log[-1].keys()) == {\"as_of\", \"winner\", \"coverage_now\"}\n"
}