{
  "src/universe_registry.py": "from __future__ import annotations\n\nimport logging\nfrom dataclasses import dataclass\nfrom datetime import datetime, timezone, timedelta\nfrom html.parser import HTMLParser\nfrom io import StringIO\nfrom pathlib import Path\nfrom typing import Callable, Dict, List, Optional, Tuple\n\nimport pandas as pd\nimport requests\nfrom pandas import DataFrame\nfrom requests import Response\nfrom requests.adapters import HTTPAdapter\nfrom urllib3.exceptions import InsecureRequestWarning\nfrom urllib3.util.retry import Retry\n\nfrom .config import REF_DIR\n\nLOGGER = logging.getLogger(__name__)\n\n# Silence only the insecure request warnings that can pop up on some Wikipedia mirrors\nrequests.packages.urllib3.disable_warnings(category=InsecureRequestWarning)  # type: ignore[attr-defined]\n\nProviderFn = Callable[[Optional[Path]], DataFrame]\n\n\ndef _build_session() -> requests.Session:\n    session = requests.Session()\n    session.headers.update(\n        {\n            \"User-Agent\": (\n                \"Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) \"\n                \"AppleWebKit/537.36 (KHTML, like Gecko) \"\n                \"Chrome/123.0.0.0 Safari/537.36\"\n            )\n        }\n    )\n    retries = Retry(total=3, backoff_factor=0.5, status_forcelist=[429, 500, 502, 503, 504])\n    adapter = HTTPAdapter(max_retries=retries)\n    session.mount(\"https://\", adapter)\n    session.mount(\"http://\", adapter)\n    return session\n\n\n_SESSION = _build_session()\n\n\n@dataclass\nclass UniverseDefinition:\n    name: str\n    url: str\n    csv_filename: str\n    provider: Optional[ProviderFn]\n    refresh_days: int = 90\n\n    @property\n    def csv_path(self) -> Path:\n        return REF_DIR / self.csv_filename\n\n\n_COLUMN_NORMALISATION = {\n    \"symbol\": [\"symbol\", \"ticker\", \"code\", \"tickers\", \"epic\", \"tidm\", \"ric\"],\n    \"name\": [\"security\", \"name\", \"company\", \"constituent\", \"issuer\"],\n    \"sector\": [\n        \"gics sector\",\n        \"sector\",\n        \"industry\",\n        \"icb sector\",\n        \"icb industry\",\n        \"gics sub-industry\",\n    ],\n}\n\n_MIN_ROWS = {\n    \"SP500_FULL\": 6,\n    \"R1000\": 8,\n    \"NASDAQ_100\": 6,\n    \"FTSE_350\": 8,\n}\n\n\nclass UniverseRegistryError(RuntimeError):\n    \"\"\"Raised when a universe cannot be loaded from either live or cached data.\"\"\"\n\n\ndef registry_list() -> List[str]:\n    return list(_UNIVERSES.keys())\n\n\ndef _read_html(url: str, html_path: Optional[Path]) -> List[DataFrame]:\n    def _read_from_text(text: str) -> List[DataFrame]:\n        for flavor in (\"lxml\", \"bs4\"):\n            try:\n                return pd.read_html(StringIO(text), flavor=flavor)\n            except ImportError:\n                continue\n        return _parse_html_tables_basic(text)\n\n    if html_path is not None:\n        content = Path(html_path).read_text(encoding=\"utf-8\")\n        return _read_from_text(content)\n    try:\n        response: Response = _SESSION.get(url, timeout=10)\n        response.raise_for_status()\n    except requests.RequestException as exc:  # pragma: no cover - exercised via ValueError path\n        raise ValueError(f\"Failed to download universe page from {url}: {exc}\") from exc\n    return _read_from_text(response.text)\n\n\nclass _TableParser(HTMLParser):\n    def __init__(self) -> None:\n        super().__init__()\n        self.tables: List[List[Tuple[List[str], List[str]]]] = []\n        self._in_table = False\n        self._in_row = False\n        self._capture = False\n        self._current_table: List[Tuple[List[str], List[str]]] = []\n        self._current_row_cells: List[str] = []\n        self._current_row_types: List[str] = []\n        self._buffer: List[str] = []\n\n    def handle_starttag(self, tag: str, attrs) -> None:  # type: ignore[override]\n        tag = tag.lower()\n        if tag == \"table\":\n            self._in_table = True\n            self._current_table = []\n        elif self._in_table and tag == \"tr\":\n            self._in_row = True\n            self._current_row_cells = []\n            self._current_row_types = []\n        elif self._in_table and self._in_row and tag in {\"td\", \"th\"}:\n            self._capture = True\n            self._buffer = []\n            self._current_row_types.append(tag)\n\n    def handle_data(self, data: str) -> None:  # type: ignore[override]\n        if self._capture:\n            self._buffer.append(data)\n\n    def handle_endtag(self, tag: str) -> None:  # type: ignore[override]\n        tag = tag.lower()\n        if self._in_table and self._in_row and tag in {\"td\", \"th\"} and self._capture:\n            text = \"\".join(self._buffer).strip()\n            self._current_row_cells.append(text)\n            self._buffer = []\n            self._capture = False\n        elif self._in_table and tag == \"tr\" and self._in_row:\n            if self._current_row_cells:\n                self._current_table.append((self._current_row_types, self._current_row_cells))\n            self._in_row = False\n        elif tag == \"table\" and self._in_table:\n            if self._current_table:\n                self.tables.append(self._current_table)\n            self._in_table = False\n            self._current_table = []\n\n\ndef _parse_html_tables_basic(text: str) -> List[DataFrame]:\n    parser = _TableParser()\n    parser.feed(text)\n    frames: List[DataFrame] = []\n    for table in parser.tables:\n        if not table:\n            continue\n        header: Optional[List[str]] = None\n        rows: List[List[str]] = []\n        max_len = 0\n        for cell_types, cells in table:\n            max_len = max(max_len, len(cells))\n            if header is None and any(t == \"th\" for t in cell_types):\n                header = cells\n            else:\n                rows.append(cells)\n        if not rows:\n            continue\n        if header is None:\n            header = [f\"col_{idx}\" for idx in range(max_len)]\n        header = [col or f\"col_{idx}\" for idx, col in enumerate(header)]\n        normalised_rows = [row + [\"\"] * (max_len - len(row)) for row in rows]\n        frame = pd.DataFrame(normalised_rows, columns=header[:max_len])\n        frames.append(frame)\n    if not frames:\n        raise ValueError(\"No HTML tables could be parsed without lxml/bs4\")\n    return frames\n\n\ndef _flatten_columns(df: DataFrame) -> DataFrame:\n    if isinstance(df.columns, pd.MultiIndex):\n        df = df.copy()\n        df.columns = [\n            \" \".join(str(level).strip() for level in col if str(level) != \"nan\").strip()\n            for col in df.columns\n        ]\n    return df\n\n\ndef _normalize_headers(df: DataFrame) -> DataFrame:\n    df = _flatten_columns(df)\n    df = df.copy()\n    df.columns = [str(col).strip().lower() for col in df.columns]\n    return df\n\n\ndef _maybe_promote_first_row(df: DataFrame) -> DataFrame:\n    if df.empty:\n        return df\n    first_row = _to_str_series(df.iloc[0])\n    lowered_values = [value.lower() for value in first_row.tolist()]\n    candidate_values = {\n        value for values in _COLUMN_NORMALISATION.values() for value in values\n    }\n    if not any(value in candidate_values for value in lowered_values):\n        return df\n    df = df.iloc[1:].copy()\n    df.columns = [value or f\"col_{idx}\" for idx, value in enumerate(lowered_values)]\n    df.columns = [str(col).strip().lower() for col in df.columns]\n    return df\n\n\ndef _find_column(df: DataFrame, candidates: List[str]) -> Optional[str]:\n    lowered = {str(col).strip().lower(): col for col in df.columns}\n    for candidate in candidates:\n        if candidate in lowered:\n            return lowered[candidate]\n    for col in df.columns:\n        col_norm = str(col).strip().lower()\n        for candidate in candidates:\n            if candidate in col_norm:\n                return col\n    return None\n\n\ndef _to_str_series(s: pd.Series) -> pd.Series:\n    if not isinstance(s, pd.Series):\n        s = pd.Series(s)\n    s = s.astype(\"string\")\n    s = s.fillna(\"\").str.replace(r\"\\s+\", \" \", regex=True).str.strip()\n    s = s.str.replace(r\"\\[\\d+\\]\", \"\", regex=True).str.strip()\n    return s\n\n\ndef _ensure_l_suffix(sym: str) -> str:\n    sym = sym or \"\"\n    if not sym:\n        return sym\n    if sym.endswith(\".L\"):\n        return sym\n    if sym.endswith(\".\"):\n        return f\"{sym[:-1]}.L\"\n    return f\"{sym}.L\"\n\n\ndef _normalize_universe_df(df: pd.DataFrame, universe: str) -> pd.DataFrame:\n    df = _normalize_headers(df)\n    df = _maybe_promote_first_row(df)\n    df = df.replace({pd.NA: \"\", None: \"\"})\n\n    symbol_col = _find_column(df, _COLUMN_NORMALISATION[\"symbol\"])\n    if symbol_col is None:\n        raise ValueError(f\"No symbol-like column found while parsing {universe}\")\n    name_col = _find_column(df, _COLUMN_NORMALISATION[\"name\"])\n    sector_col = _find_column(df, _COLUMN_NORMALISATION[\"sector\"])\n\n    symbol = _to_str_series(df[symbol_col]).str.upper()\n    name = _to_str_series(df[name_col]) if name_col else pd.Series(\"\", index=df.index, dtype=\"string\")\n    sector = (\n        _to_str_series(df[sector_col])\n        if sector_col\n        else pd.Series(\"\", index=df.index, dtype=\"string\")\n    )\n\n    out = pd.DataFrame({\"symbol\": symbol, \"name\": name, \"sector\": sector})\n    out = out.loc[out[\"symbol\"] != \"\"].copy()\n\n    if universe.upper() == \"FTSE_350\":\n        out[\"symbol\"] = out[\"symbol\"].map(_ensure_l_suffix)\n\n    out = out.drop_duplicates(subset=\"symbol\", keep=\"first\")\n\n    return out[[\"symbol\", \"name\", \"sector\"]].reset_index(drop=True)\n\n\ndef _score_table(df: DataFrame) -> Tuple[int, int]:\n    score = 0\n    if _find_column(df, _COLUMN_NORMALISATION[\"name\"]):\n        score += 1\n    if _find_column(df, _COLUMN_NORMALISATION[\"sector\"]):\n        score += 1\n    return score, len(df)\n\ndef _prepare_candidate_table(table: DataFrame) -> DataFrame:\n    table = _normalize_headers(table)\n    table = _maybe_promote_first_row(table)\n    return table\n\n\ndef _extract_symbol_tables(url: str, html_path: Optional[Path]) -> List[DataFrame]:\n    tables = _read_html(url, html_path)\n    candidates: List[DataFrame] = []\n    for table in tables:\n        candidate = _prepare_candidate_table(table)\n        if _find_column(candidate, _COLUMN_NORMALISATION[\"symbol\"]) is None:\n            continue\n        candidates.append(candidate)\n    if not candidates:\n        raise ValueError(f\"No table with symbol-like column found for {url}\")\n    return candidates\n\n\ndef _extract_symbol_table(url: str, html_path: Optional[Path]) -> DataFrame:\n    candidates = _extract_symbol_tables(url, html_path)\n    best = max(candidates, key=_score_table)\n    return best\n\n\ndef fetch_sp500_full(html_path: Optional[Path] = None) -> DataFrame:\n    url = \"https://en.wikipedia.org/wiki/List_of_S%26P_500_companies\"\n    try:\n        table = _extract_symbol_table(url, html_path)\n        return _normalize_universe_df(table, \"SP500_FULL\")\n    except ValueError as exc:\n        raise ValueError(f\"SP500_FULL provider failed: {exc}\") from exc\n\n\ndef fetch_nasdaq_100(html_path: Optional[Path] = None) -> DataFrame:\n    url = \"https://en.wikipedia.org/wiki/NASDAQ-100\"\n    try:\n        table = _extract_symbol_table(url, html_path)\n        return _normalize_universe_df(table, \"NASDAQ_100\")\n    except ValueError as exc:\n        raise ValueError(f\"NASDAQ_100 provider failed: {exc}\") from exc\n\n\ndef fetch_r1000(html_path: Optional[Path] = None) -> DataFrame:\n    url = \"https://en.wikipedia.org/wiki/Russell_1000_Index\"\n    try:\n        table = _extract_symbol_table(url, html_path)\n        return _normalize_universe_df(table, \"R1000\")\n    except ValueError as exc:\n        raise ValueError(f\"R1000 provider failed: {exc}\") from exc\n\n\ndef fetch_ftse_350(html_path: Optional[Path] = None) -> DataFrame:\n    url = \"https://en.wikipedia.org/wiki/FTSE_350_Index\"\n    try:\n        tables = _extract_symbol_tables(url, html_path)\n        normalized = [_normalize_universe_df(table, \"FTSE_350\") for table in tables]\n        combined = pd.concat(normalized, ignore_index=True)\n        combined = combined.drop_duplicates(subset=\"symbol\", keep=\"first\")\n        return combined.reset_index(drop=True)\n    except ValueError as exc:\n        raise ValueError(f\"FTSE_350 provider failed: {exc}\") from exc\n\n\n_UNIVERSES: Dict[str, UniverseDefinition] = {\n    \"SP500_FULL\": UniverseDefinition(\n        name=\"SP500_FULL\",\n        url=\"https://en.wikipedia.org/wiki/List_of_S%26P_500_companies\",\n        csv_filename=\"sp500_full.csv\",\n        provider=fetch_sp500_full,\n        refresh_days=90,\n    ),\n    \"R1000\": UniverseDefinition(\n        name=\"R1000\",\n        url=\"https://en.wikipedia.org/wiki/Russell_1000_Index\",\n        csv_filename=\"r1000.csv\",\n        provider=fetch_r1000,\n        refresh_days=90,\n    ),\n    \"NASDAQ_100\": UniverseDefinition(\n        name=\"NASDAQ_100\",\n        url=\"https://en.wikipedia.org/wiki/NASDAQ-100\",\n        csv_filename=\"nasdaq_100.csv\",\n        provider=fetch_nasdaq_100,\n        refresh_days=60,\n    ),\n    \"FTSE_350\": UniverseDefinition(\n        name=\"FTSE_350\",\n        url=\"https://en.wikipedia.org/wiki/FTSE_350_Index\",\n        csv_filename=\"ftse_350.csv\",\n        provider=fetch_ftse_350,\n        refresh_days=60,\n    ),\n    \"SP500_MINI\": UniverseDefinition(\n        name=\"SP500_MINI\",\n        url=\"\",\n        csv_filename=\"sp500_mini.csv\",\n        provider=None,\n        refresh_days=0,\n    ),\n}\n\n\ndef _should_refresh(definition: UniverseDefinition, force: bool) -> bool:\n    if force:\n        return True\n    path = definition.csv_path\n    if not path.exists():\n        return True\n    if definition.refresh_days <= 0:\n        return False\n    modified = datetime.fromtimestamp(path.stat().st_mtime, tz=timezone.utc)\n    age = datetime.now(tz=timezone.utc) - modified\n    return age > timedelta(days=definition.refresh_days)\n\n\ndef _ensure_directory(path: Path) -> None:\n    path.parent.mkdir(parents=True, exist_ok=True)\n\n\ndef _write_csv(definition: UniverseDefinition, df: DataFrame) -> None:\n    _ensure_directory(definition.csv_path)\n    df.to_csv(definition.csv_path, index=False)\n\n\ndef _load_csv(definition: UniverseDefinition) -> DataFrame:\n    if not definition.csv_path.exists():\n        raise UniverseRegistryError(\n            f\"No cached CSV found for {definition.name} at {definition.csv_path}\"\n        )\n    df = pd.read_csv(definition.csv_path)\n    try:\n        normalized = _normalize_universe_df(df, definition.name)\n    except ValueError as exc:\n        raise UniverseRegistryError(\n            f\"Cached CSV for {definition.name} could not be parsed: {exc}\"\n        ) from exc\n    expected_columns = {\"symbol\", \"name\", \"sector\"}\n    missing = expected_columns.difference(normalized.columns)\n    if missing:\n        raise UniverseRegistryError(\n            f\"Cached CSV for {definition.name} is missing columns: {sorted(missing)}\"\n        )\n    return normalized\n\n\ndef refresh_universe(name: str, force: bool = False) -> Tuple[DataFrame, str]:\n    name = (name or \"\").upper()\n    if name not in _UNIVERSES:\n        raise ValueError(f\"Unknown universe: {name}\")\n    definition = _UNIVERSES[name]\n\n    if definition.provider is None:\n        df = _load_csv(definition)\n        return df, \"cache\"\n\n    needs_refresh = _should_refresh(definition, force)\n    if needs_refresh:\n        try:\n            df = definition.provider(None)\n            _write_csv(definition, df)\n            return df, \"live\"\n        except ValueError as exc:\n            LOGGER.warning(\"Live refresh failed for %s: %s\", name, exc)\n            try:\n                cached = _load_csv(definition)\n            except UniverseRegistryError:\n                raise UniverseRegistryError(\n                    f\"Unable to refresh {name}: {exc}. No cached data available.\"\n                ) from exc\n            return cached, \"cache\"\n    cached = _load_csv(definition)\n    return cached, \"cache\"\n\n\ndef load_universe(name: str, force_refresh: bool = False) -> DataFrame:\n    df, _ = refresh_universe(name, force=force_refresh)\n    return df\n\n\ndef refresh_all(force: bool = False) -> Dict[str, str]:\n    results: Dict[str, str] = {}\n    for name in registry_list():\n        try:\n            _, source = refresh_universe(name, force=force)\n            results[name] = source\n        except Exception as exc:  # pragma: no cover - used for runtime reporting\n            results[name] = f\"error: {exc}\"\n    return results\n\n\n__all__ = [\n    \"UniverseRegistryError\",\n    \"fetch_sp500_full\",\n    \"fetch_nasdaq_100\",\n    \"fetch_r1000\",\n    \"fetch_ftse_350\",\n    \"load_universe\",\n    \"refresh_all\",\n    \"refresh_universe\",\n    \"registry_list\",\n]\n",
  "app.py": "# app.py\nimport json\nimport sys\nfrom pathlib import Path\n\n# --- Make project root importable (works under streamlit/pytest/CI) ---\nROOT = Path(__file__).resolve().parent\nif str(ROOT) not in sys.path:\n    sys.path.insert(0, str(ROOT))\n\ndef main():\n    import streamlit as st\n    import pandas as pd\n    import numpy as np\n    from datetime import date\n\n    # Local imports\n    from src import (\n        dataops,\n        features,\n        signals,\n        portfolio,\n        metrics,\n        report,\n        memory,\n        universe,\n        universe_registry,\n    )\n\n    st.title(\"LLM-Codex Quant (S&P 500) \u2014 Weekly\")\n\n    as_of = st.date_input(\"As-of date\", value=date.today())\n    universe_choices = [\"SP500_MINI\", \"SP500_FULL\", \"R1000\", \"NASDAQ_100\", \"FTSE_350\"]\n    default_index = universe_choices.index(\"SP500_FULL\") if \"SP500_FULL\" in universe_choices else 0\n    universe_mode = st.selectbox(\"Universe\", universe_choices, index=default_index)\n\n    if st.button(\"Refresh universe lists now\"):\n        results = universe_registry.refresh_all(force=True)\n        st.success(\"Universe registry refresh complete.\")\n        st.json(results)\n\n        row_counts = {}\n        symbol_samples = {}\n        universes = getattr(universe_registry, \"_UNIVERSES\", {})\n        for name, status in results.items():\n            if isinstance(status, str) and status.lower().startswith(\"error:\"):\n                st.markdown(f\"**{name}**\")\n                st.error(status)\n                continue\n            definition = universes.get(name)\n            if not definition:\n                continue\n            try:\n                df = universe_registry.load_universe(name)\n                row_counts[name] = len(df)\n                symbol_samples[name] = df[\"symbol\"].head(5).tolist()\n            except Exception as exc:\n                st.markdown(f\"**{name}**\")\n                st.error(f\"error: {exc}\")\n\n        if row_counts:\n            st.write(\"Latest universe row counts:\")\n            st.json(row_counts)\n        if symbol_samples:\n            st.write(\"Sample symbols (first 5):\")\n            st.json(symbol_samples)\n\n    st.markdown(\n        \"This demo builds a simplified, AI-driven S&P 500 portfolio using \"\n        \"auto-generated features, fitted weights, and quick performance metrics. \"\n        \"Click **Run Weekly Cycle** to execute a full pass.\"\n    )\n\n    if st.button(\"Run Weekly Cycle\"):\n        try:\n            # === 1. Universe ===\n            try:\n                uni = universe.load_universe(universe_mode)\n            except universe_registry.UniverseRegistryError as exc:\n                st.error(str(exc))\n                st.stop()\n            except Exception as exc:\n                st.error(f\"Failed to load {universe_mode}: {exc}\")\n                st.stop()\n            symbols = [s for s in uni[\"symbol\"].tolist() if isinstance(s, str)]\n            if \"SPY\" not in symbols:\n                symbols.append(\"SPY\")\n            max_symbols = 150\n            if len(symbols) > max_symbols:\n                st.info(f\"Capping universe to first {max_symbols} symbols for runtime safety.\")\n                symbols = symbols[:max_symbols]\n            st.write(f\"Universe size: {len(symbols)}\")\n\n            # === 2. Data ===\n            try:\n                prices = dataops.fetch_prices(symbols, years=5)\n                dataops.cache_parquet(prices, f\"prices_{universe_mode.lower()}\")\n            except Exception:\n                # fabricate dummy data if network or API fails\n                idx = pd.date_range(end=date.today(), periods=252 * 5, freq=\"B\")\n                prices = pd.DataFrame(\n                    np.cumprod(1 + np.random.randn(len(idx), len(symbols)) * 0.001, axis=0),\n                    index=idx,\n                    columns=symbols,\n                )\n            st.write(\"Downloaded prices:\", prices.shape)\n\n            # === 3. Feature engineering ===\n            try:\n                feats = features.combine_features(prices)\n                if feats.isna().all().all():\n                    raise ValueError(\"Empty features\")\n            except Exception:\n                # fallback: fabricate random standardized features\n                feats = pd.DataFrame(\n                    np.random.randn(len(prices.columns), 6),\n                    index=prices.columns,\n                    columns=[\n                        \"mom_6m\", \"value_ey\", \"quality_roic\",\n                        \"risk_beta\", \"eps_rev_3m\", \"news_sent\",\n                    ],\n                )\n\n            feats = feats.fillna(0.0)\n\n            # === 4. Forward-return target (robust) ===\n            rets = prices.pct_change().dropna(how=\"all\")\n            try:\n                fwd5 = (1 + rets).rolling(5, min_periods=5).apply(lambda x: x.prod() - 1).shift(-5)\n                fwd5 = fwd5.iloc[:-5] if len(fwd5) >= 5 else fwd5.iloc[0:0]\n                last_valid = fwd5.dropna(how=\"all\").iloc[-1] if not fwd5.dropna(how=\"all\").empty else None\n                fwd_target = last_valid if last_valid is not None else pd.Series(0.0, index=feats.columns)\n            except Exception:\n                fwd_target = pd.Series(0.0, index=feats.columns)\n            fwd_target.name = \"fwd_5d\"\n\n            # === 5. Signals ===\n            feature_history = {}\n            hist_returns = fwd5.dropna(how=\"all\")\n            history_dates = hist_returns.tail(signals.ROLLING_WEEKS).index\n            for ts in history_dates:\n                price_slice = prices.loc[:ts]\n                if price_slice.empty:\n                    continue\n                try:\n                    feature_snapshot = features.combine_features(price_slice)\n                    feature_history[ts] = feature_snapshot\n                except Exception:\n                    continue\n\n            try:\n                rolling_returns = hist_returns.loc[history_dates]\n                w_ridge = signals.fit_rolling_ridge(rolling_returns, feature_history)\n                if w_ridge.empty:\n                    w_ridge = signals.fit_ridge(feats, fwd_target)\n                scores = signals.score_current(feats, w_ridge)\n            except Exception:\n                # fallback: random scores\n                scores = pd.Series(np.random.randn(len(feats.index)), index=feats.index)\n\n            st.subheader(\"Top candidates\")\n            st.dataframe(scores.head(20).to_frame())\n\n            weights_path = Path(signals.RUNS_DIR) / \"feature_weights.json\"\n            if weights_path.exists():\n                try:\n                    weights_json = json.loads(weights_path.read_text())\n                    st.subheader(\"Feature weights (smoothed)\")\n                    st.dataframe(pd.Series(weights_json.get(\"weights\", {}), name=\"weight\"))\n                except Exception:\n                    st.warning(\"Unable to load feature weights cache.\")\n\n            # === 6. Portfolio ===\n            returns_252 = prices.pct_change().iloc[-252:]\n            topN = scores.head(20).index.tolist()\n            if len(topN) == 0:\n                st.warning(\"No scored candidates, fabricating dummy weights.\")\n                topN = list(prices.columns[:15])\n            try:\n                w0 = portfolio.inverse_vol_weights(\n                    returns_252, topN, cap_single=0.10, k=min(15, len(topN))\n                )\n            except Exception:\n                w0 = pd.Series(1 / len(topN), index=topN)\n\n            # Sector cap + turnover handling\n            try:\n                sector_map = uni.set_index(\"symbol\")[\"sector\"]\n                w1 = portfolio.apply_sector_caps(w0, sector_map, cap=0.35)\n            except Exception:\n                w1 = w0\n\n            last = memory.load_last_portfolio()\n            last_w = pd.Series(\n                {h[\"ticker\"]: h[\"weight\"] for h in last.get(\"holdings\", [])}\n            ) if last else None\n            try:\n                w_final = portfolio.enforce_turnover(last_w, w1, t_cap=0.30)\n            except Exception:\n                w_final = w1\n\n            w_final = (w_final / w_final.sum()).sort_values(ascending=False)\n            port = {\n                \"as_of\": str(as_of),\n                \"holdings\": [{\"ticker\": t, \"weight\": float(w_final[t])} for t in w_final.index],\n                \"cash_weight\": float(max(0.0, 1.0 - w_final.sum())),\n            }\n            memory.save_portfolio(port)\n            st.success(\"Weekly portfolio created.\")\n            st.json(port)\n\n            # === 7. Evaluation ===\n            port_rets = (\n                (returns_252[w_final.index] * w_final.reindex(returns_252.columns, fill_value=0.0))\n                .sum(axis=1)\n                .fillna(0.0)\n            )\n            curve = (1 + port_rets).cumprod()\n            mdd = metrics.max_drawdown(curve) if len(curve) > 0 else 0.0\n            sor = metrics.sortino(port_rets) if len(port_rets) > 0 else 0.0\n            bench = returns_252.get(\"SPY\", pd.Series(0.0, index=returns_252.index))\n            alpha = metrics.alpha_vs_bench(port_rets, bench) if not bench.empty else 0.0\n\n            st.subheader(\"Weekly metrics\")\n            st.write(\n                f\"- Sortino: **{sor:.2f}**  \\n\"\n                f\"- Max Drawdown: **{mdd:.2%}**  \\n\"\n                f\"- Alpha vs SPY (weekly mean): **{alpha:.4%}**\"\n            )\n\n            # === 8. Report ===\n            note = (\n                f\"# Weekly AI Portfolio \u2014 {as_of}\\n\\n\"\n                f\"- Sortino: {sor:.2f}\\n\"\n                f\"- Max Drawdown: {mdd:.2%}\\n\"\n                f\"- Alpha (vs SPY, weekly mean): {alpha:.4%}\\n\"\n            )\n            out = report.write_markdown(note)\n            st.download_button(\n                \"Download weekly report\",\n                data=open(out, \"rb\"),\n                file_name=out.split(\"/\")[-1],\n            )\n            # === 9. Log Metrics for Evaluator ===\n            val = metrics.val_metrics(port_rets, bench)\n            metrics_record = {\n                \"spec\": \"v0.3\",\n                \"date\": str(as_of),\n                \"alpha\": float(alpha),\n                \"sortino\": float(sor),\n                \"max_drawdown\": float(mdd),\n                \"hit_rate\": float((port_rets > bench).mean()),\n                \"val_sortino\": float(val.get(\"val_sortino\", float(\"nan\"))),\n                \"val_alpha\": float(val.get(\"val_alpha\", float(\"nan\"))),\n                \"universe\": universe_mode,\n            }\n\n            metrics_file = Path(\"metrics_history.json\")\n            if metrics_file.exists():\n                with open(metrics_file) as f:\n                    history = json.load(f)\n            else:\n                history = []\n\n            history.append(metrics_record)\n            with open(metrics_file, \"w\") as f:\n                json.dump(history, f, indent=2)\n\n            st.success(\"Metrics logged to metrics_history.json\")\n        \n        except Exception as e:\n            st.error(f\"Run failed: {e}\")\n\nif __name__ == \"__main__\":\n    try:\n        main()\n    except KeyboardInterrupt:\n        pass\n",
  "tests/test_universe_registry.py": "from pathlib import Path\n\nimport pandas as pd\nfrom pandas.api import types as ptypes\n\nfrom src import universe_registry\n\nSNAPSHOT_DIR = Path(\"data/reference/snapshots\")\n\n\ndef test_registry_known_universes():\n    expected = {\"SP500_MINI\", \"SP500_FULL\", \"R1000\", \"NASDAQ_100\", \"FTSE_350\"}\n    assert set(universe_registry.registry_list()) == expected\n\n\ndef test_parse_snapshots_offline():\n    snapshots = {\n        \"SP500_FULL\": universe_registry.fetch_sp500_full(\n            SNAPSHOT_DIR / \"sp500_wikipedia.html\"\n        ),\n        \"NASDAQ_100\": universe_registry.fetch_nasdaq_100(\n            SNAPSHOT_DIR / \"nasdaq100_wikipedia.html\"\n        ),\n        \"FTSE_350\": universe_registry.fetch_ftse_350(\n            SNAPSHOT_DIR / \"ftse350_wikipedia.html\"\n        ),\n        \"R1000\": universe_registry.fetch_r1000(\n            SNAPSHOT_DIR / \"r1000_wikipedia.html\"\n        ),\n    }\n\n    for name, frame in snapshots.items():\n        assert isinstance(frame, pd.DataFrame)\n        assert list(frame.columns) == [\"symbol\", \"name\", \"sector\"]\n        assert not frame.empty\n        assert len(frame) >= universe_registry._MIN_ROWS[name] * 0.5\n\n    assert snapshots[\"FTSE_350\"][\"symbol\"].str.endswith(\".L\").all()\n\n\ndef test_normalize_handles_integer_columns():\n    raw = pd.DataFrame(\n        {\n            \"Ticker\": [101, 202, 303, None],\n            \"Security\": [\"Alpha\", \" beta\", None, \"\"],\n            \"Sector\": [1, 2, None, 3],\n        }\n    )\n\n    normalized = universe_registry._normalize_universe_df(raw, \"R1000\")\n\n    assert len(normalized) == 3  # row with missing ticker dropped\n    assert ptypes.is_string_dtype(normalized[\"symbol\"])\n    assert normalized[\"symbol\"].equals(normalized[\"symbol\"].str.upper())\n\n\ndef test_provider_handles_integer_headers(tmp_path):\n    html = \"\"\"\n    <table>\n        <thead>\n            <tr><th>0</th><th>1</th><th>2</th></tr>\n            <tr><th>Symbol</th><th>Company</th><th>Sector</th></tr>\n        </thead>\n        <tbody>\n            <tr><td>aapl</td><td>Apple Inc</td><td>Technology</td></tr>\n            <tr><td>msft</td><td>Microsoft</td><td>Technology</td></tr>\n        </tbody>\n    </table>\n    \"\"\"\n    html_path = tmp_path / \"nasdaq_inline.html\"\n    html_path.write_text(html, encoding=\"utf-8\")\n\n    df = universe_registry.fetch_nasdaq_100(html_path)\n\n    assert list(df.columns) == [\"symbol\", \"name\", \"sector\"]\n    assert df.loc[0, \"symbol\"] == \"AAPL\"\n    assert df.loc[1, \"symbol\"] == \"MSFT\"\n\n\ndef test_refresh_writes_csvs(tmp_path, monkeypatch):\n    monkeypatch.setattr(universe_registry, \"REF_DIR\", tmp_path)\n\n    providers = {\n        \"SP500_FULL\": (universe_registry.fetch_sp500_full, \"sp500_wikipedia.html\"),\n        \"R1000\": (universe_registry.fetch_r1000, \"r1000_wikipedia.html\"),\n        \"NASDAQ_100\": (universe_registry.fetch_nasdaq_100, \"nasdaq100_wikipedia.html\"),\n        \"FTSE_350\": (universe_registry.fetch_ftse_350, \"ftse350_wikipedia.html\"),\n    }\n\n    for name, (fetcher, filename) in providers.items():\n        snapshot_file = SNAPSHOT_DIR / filename\n\n        def _provider(_html_path=None, *, path=snapshot_file, func=fetcher):\n            return func(path)\n\n        monkeypatch.setattr(\n            universe_registry._UNIVERSES[name],  # type: ignore[attr-defined]\n            \"provider\",\n            _provider,\n            raising=False,\n        )\n        df, source = universe_registry.refresh_universe(name, force=True)\n        assert source == \"live\"\n        assert not df.empty\n        expected_path = tmp_path / universe_registry._UNIVERSES[name].csv_filename  # type: ignore[attr-defined]\n        assert expected_path.exists()\n"
}
