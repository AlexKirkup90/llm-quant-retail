{
  "src/universe_registry.py": "from __future__ import annotations\n\nimport logging\nfrom dataclasses import dataclass\nfrom datetime import datetime, timezone, timedelta\nfrom html.parser import HTMLParser\nfrom io import StringIO\nfrom pathlib import Path\nfrom typing import Callable, Dict, List, Optional, Tuple\n\nimport pandas as pd\nimport requests\nfrom pandas import DataFrame\nfrom requests import Response\nfrom requests.adapters import HTTPAdapter\nfrom urllib3.exceptions import InsecureRequestWarning\nfrom urllib3.util.retry import Retry\n\nfrom .config import REF_DIR\n\nLOGGER = logging.getLogger(__name__)\n\n# Silence only the insecure request warnings that can pop up on some Wikipedia mirrors\nrequests.packages.urllib3.disable_warnings(category=InsecureRequestWarning)  # type: ignore[attr-defined]\n\nProviderFn = Callable[[Optional[Path]], DataFrame]\n\n\ndef _build_session() -> requests.Session:\n    session = requests.Session()\n    session.headers.update(\n        {\n            \"User-Agent\": (\n                \"Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) \"\n                \"AppleWebKit/537.36 (KHTML, like Gecko) \"\n                \"Chrome/123.0.0.0 Safari/537.36\"\n            )\n        }\n    )\n    retries = Retry(total=3, backoff_factor=0.5, status_forcelist=[429, 500, 502, 503, 504])\n    adapter = HTTPAdapter(max_retries=retries)\n    session.mount(\"https://\", adapter)\n    session.mount(\"http://\", adapter)\n    return session\n\n\n_SESSION = _build_session()\n\n\n@dataclass\nclass UniverseDefinition:\n    name: str\n    url: str\n    csv_filename: str\n    provider: Optional[ProviderFn]\n    refresh_days: int = 90\n\n    @property\n    def csv_path(self) -> Path:\n        return REF_DIR / self.csv_filename\n\n\n_COLUMN_NORMALISATION = {\n    \"symbol\": [\"symbol\", \"ticker\", \"tickers\", \"code\", \"epic\", \"tidm\", \"ric\"],\n    \"name\": [\"security\", \"company\", \"name\", \"constituent\", \"issuer\"],\n    \"sector\": [\"gics sector\", \"sector\", \"industry\", \"gics sub-industry\"],\n}\n\n\nclass UniverseRegistryError(RuntimeError):\n    \"\"\"Raised when a universe cannot be loaded from either live or cached data.\"\"\"\n\n\ndef registry_list() -> List[str]:\n    return list(_UNIVERSES.keys())\n\n\ndef _read_html(url: str, html_path: Optional[Path]) -> List[DataFrame]:\n    def _read_from_text(text: str) -> List[DataFrame]:\n        for flavor in (\"lxml\", \"bs4\"):\n            try:\n                return pd.read_html(StringIO(text), flavor=flavor)\n            except ImportError:\n                continue\n        return _parse_html_tables_basic(text)\n\n    if html_path is not None:\n        content = Path(html_path).read_text(encoding=\"utf-8\")\n        return _read_from_text(content)\n    try:\n        response: Response = _SESSION.get(url, timeout=10)\n        response.raise_for_status()\n    except requests.RequestException as exc:  # pragma: no cover - exercised via ValueError path\n        raise ValueError(f\"Failed to download universe page from {url}: {exc}\") from exc\n    return _read_from_text(response.text)\n\n\nclass _TableParser(HTMLParser):\n    def __init__(self) -> None:\n        super().__init__()\n        self.tables: List[List[Tuple[List[str], List[str]]]] = []\n        self._in_table = False\n        self._in_row = False\n        self._capture = False\n        self._current_table: List[Tuple[List[str], List[str]]] = []\n        self._current_row_cells: List[str] = []\n        self._current_row_types: List[str] = []\n        self._buffer: List[str] = []\n\n    def handle_starttag(self, tag: str, attrs) -> None:  # type: ignore[override]\n        tag = tag.lower()\n        if tag == \"table\":\n            self._in_table = True\n            self._current_table = []\n        elif self._in_table and tag == \"tr\":\n            self._in_row = True\n            self._current_row_cells = []\n            self._current_row_types = []\n        elif self._in_table and self._in_row and tag in {\"td\", \"th\"}:\n            self._capture = True\n            self._buffer = []\n            self._current_row_types.append(tag)\n\n    def handle_data(self, data: str) -> None:  # type: ignore[override]\n        if self._capture:\n            self._buffer.append(data)\n\n    def handle_endtag(self, tag: str) -> None:  # type: ignore[override]\n        tag = tag.lower()\n        if self._in_table and self._in_row and tag in {\"td\", \"th\"} and self._capture:\n            text = \"\".join(self._buffer).strip()\n            self._current_row_cells.append(text)\n            self._buffer = []\n            self._capture = False\n        elif self._in_table and tag == \"tr\" and self._in_row:\n            if self._current_row_cells:\n                self._current_table.append((self._current_row_types, self._current_row_cells))\n            self._in_row = False\n        elif tag == \"table\" and self._in_table:\n            if self._current_table:\n                self.tables.append(self._current_table)\n            self._in_table = False\n            self._current_table = []\n\n\ndef _parse_html_tables_basic(text: str) -> List[DataFrame]:\n    parser = _TableParser()\n    parser.feed(text)\n    frames: List[DataFrame] = []\n    for table in parser.tables:\n        if not table:\n            continue\n        header: Optional[List[str]] = None\n        rows: List[List[str]] = []\n        max_len = 0\n        for cell_types, cells in table:\n            max_len = max(max_len, len(cells))\n            if header is None and any(t == \"th\" for t in cell_types):\n                header = cells\n            else:\n                rows.append(cells)\n        if not rows:\n            continue\n        if header is None:\n            header = [f\"col_{idx}\" for idx in range(max_len)]\n        header = [col or f\"col_{idx}\" for idx, col in enumerate(header)]\n        normalised_rows = [row + [\"\"] * (max_len - len(row)) for row in rows]\n        frame = pd.DataFrame(normalised_rows, columns=header[:max_len])\n        frames.append(frame)\n    if not frames:\n        raise ValueError(\"No HTML tables could be parsed without lxml/bs4\")\n    return frames\n\n\ndef _flatten_columns(df: DataFrame) -> DataFrame:\n    if isinstance(df.columns, pd.MultiIndex):\n        df = df.copy()\n        df.columns = [\n            \" \".join(str(level).strip() for level in col if str(level) != \"nan\").strip()\n            for col in df.columns\n        ]\n    return df\n\n\ndef _find_column(df: DataFrame, candidates: List[str]) -> Optional[str]:\n    lowered = {col.strip().lower(): col for col in df.columns}\n    for candidate in candidates:\n        if candidate in lowered:\n            return lowered[candidate]\n    for col in df.columns:\n        col_norm = col.strip().lower()\n        for candidate in candidates:\n            if candidate in col_norm:\n                return col\n    return None\n\n\ndef _normalise_table(df: DataFrame, *, ftse_suffix: bool = False) -> DataFrame:\n    df = _flatten_columns(df)\n    df = df.replace({pd.NA: \"\", None: \"\"})\n    symbol_col = _find_column(df, _COLUMN_NORMALISATION[\"symbol\"])\n    if symbol_col is None:\n        raise ValueError(\"No symbol column found\")\n    name_col = _find_column(df, _COLUMN_NORMALISATION[\"name\"])\n    sector_col = _find_column(df, _COLUMN_NORMALISATION[\"sector\"])\n\n    out = pd.DataFrame()\n    out[\"symbol\"] = df[symbol_col].astype(str).str.strip()\n    if name_col:\n        out[\"name\"] = df[name_col].astype(str).str.strip()\n    else:\n        out[\"name\"] = \"\"\n    if sector_col:\n        out[\"sector\"] = df[sector_col].astype(str).str.strip()\n    else:\n        out[\"sector\"] = \"\"\n\n    out[\"symbol\"] = out[\"symbol\"].replace({\"nan\": \"\", \"\": \"\"})\n    out = out.loc[out[\"symbol\"].astype(bool)].copy()\n    out[\"symbol\"] = out[\"symbol\"].str.upper()\n\n    if ftse_suffix:\n        out[\"symbol\"] = out[\"symbol\"].apply(_append_ftse_suffix)\n\n    out[\"name\"] = out[\"name\"].astype(str).str.strip()\n    out[\"sector\"] = out[\"sector\"].astype(str).str.strip()\n    return out[[\"symbol\", \"name\", \"sector\"]]\n\n\ndef _append_ftse_suffix(symbol: str) -> str:\n    symbol = symbol.strip().upper()\n    if symbol.endswith(\".L\"):\n        return symbol\n    if symbol.endswith(\".\"):\n        symbol = symbol[:-1]\n    if not symbol:\n        return symbol\n    return f\"{symbol}.L\"\n\n\ndef _extract_tables(url: str, html_path: Optional[Path], *, combine: bool = False, ftse_suffix: bool = False) -> DataFrame:\n    tables = _read_html(url, html_path)\n    normalised_tables: List[DataFrame] = []\n    for table in tables:\n        try:\n            norm = _normalise_table(table, ftse_suffix=ftse_suffix)\n        except ValueError:\n            continue\n        if norm.empty:\n            continue\n        normalised_tables.append(norm)\n    if not normalised_tables:\n        raise ValueError(f\"No parseable tables found for {url}\")\n    if combine:\n        combined = pd.concat(normalised_tables, ignore_index=True)\n        combined = combined.drop_duplicates(subset=\"symbol\")\n        return combined.reset_index(drop=True)\n    largest = max(normalised_tables, key=len)\n    return largest.reset_index(drop=True)\n\n\ndef fetch_sp500_full(html_path: Optional[Path] = None) -> DataFrame:\n    url = \"https://en.wikipedia.org/wiki/List_of_S%26P_500_companies\"\n    df = _extract_tables(url, html_path)\n    return df\n\n\ndef fetch_nasdaq_100(html_path: Optional[Path] = None) -> DataFrame:\n    url = \"https://en.wikipedia.org/wiki/NASDAQ-100\"\n    df = _extract_tables(url, html_path)\n    return df\n\n\ndef fetch_r1000(html_path: Optional[Path] = None) -> DataFrame:\n    url = \"https://en.wikipedia.org/wiki/Russell_1000_Index\"\n    df = _extract_tables(url, html_path)\n    return df\n\n\ndef fetch_ftse_350(html_path: Optional[Path] = None) -> DataFrame:\n    url = \"https://en.wikipedia.org/wiki/FTSE_350_Index\"\n    df = _extract_tables(url, html_path, combine=True, ftse_suffix=True)\n    return df\n\n\n_UNIVERSES: Dict[str, UniverseDefinition] = {\n    \"SP500_FULL\": UniverseDefinition(\n        name=\"SP500_FULL\",\n        url=\"https://en.wikipedia.org/wiki/List_of_S%26P_500_companies\",\n        csv_filename=\"sp500_full.csv\",\n        provider=fetch_sp500_full,\n        refresh_days=90,\n    ),\n    \"R1000\": UniverseDefinition(\n        name=\"R1000\",\n        url=\"https://en.wikipedia.org/wiki/Russell_1000_Index\",\n        csv_filename=\"r1000.csv\",\n        provider=fetch_r1000,\n        refresh_days=90,\n    ),\n    \"NASDAQ_100\": UniverseDefinition(\n        name=\"NASDAQ_100\",\n        url=\"https://en.wikipedia.org/wiki/NASDAQ-100\",\n        csv_filename=\"nasdaq_100.csv\",\n        provider=fetch_nasdaq_100,\n        refresh_days=60,\n    ),\n    \"FTSE_350\": UniverseDefinition(\n        name=\"FTSE_350\",\n        url=\"https://en.wikipedia.org/wiki/FTSE_350_Index\",\n        csv_filename=\"ftse_350.csv\",\n        provider=fetch_ftse_350,\n        refresh_days=60,\n    ),\n    \"SP500_MINI\": UniverseDefinition(\n        name=\"SP500_MINI\",\n        url=\"\",\n        csv_filename=\"sp500_mini.csv\",\n        provider=None,\n        refresh_days=0,\n    ),\n}\n\n\ndef _should_refresh(definition: UniverseDefinition, force: bool) -> bool:\n    if force:\n        return True\n    path = definition.csv_path\n    if not path.exists():\n        return True\n    if definition.refresh_days <= 0:\n        return False\n    modified = datetime.fromtimestamp(path.stat().st_mtime, tz=timezone.utc)\n    age = datetime.now(tz=timezone.utc) - modified\n    return age > timedelta(days=definition.refresh_days)\n\n\ndef _ensure_directory(path: Path) -> None:\n    path.parent.mkdir(parents=True, exist_ok=True)\n\n\ndef _write_csv(definition: UniverseDefinition, df: DataFrame) -> None:\n    _ensure_directory(definition.csv_path)\n    df.to_csv(definition.csv_path, index=False)\n\n\ndef _load_csv(definition: UniverseDefinition) -> DataFrame:\n    if not definition.csv_path.exists():\n        raise UniverseRegistryError(\n            f\"No cached CSV found for {definition.name} at {definition.csv_path}\"\n        )\n    df = pd.read_csv(definition.csv_path)\n    expected_columns = {\"symbol\", \"name\", \"sector\"}\n    missing = expected_columns.difference(df.columns)\n    if missing:\n        raise UniverseRegistryError(\n            f\"Cached CSV for {definition.name} is missing columns: {sorted(missing)}\"\n        )\n    df[\"symbol\"] = df[\"symbol\"].astype(str).str.upper().str.strip()\n    df[\"name\"] = df[\"name\"].astype(str).str.strip()\n    df[\"sector\"] = df[\"sector\"].astype(str).str.strip()\n    df = df.loc[df[\"symbol\"].astype(bool)].reset_index(drop=True)\n    return df\n\n\ndef refresh_universe(name: str, force: bool = False) -> Tuple[DataFrame, str]:\n    name = (name or \"\").upper()\n    if name not in _UNIVERSES:\n        raise ValueError(f\"Unknown universe: {name}\")\n    definition = _UNIVERSES[name]\n\n    if definition.provider is None:\n        df = _load_csv(definition)\n        return df, \"cache\"\n\n    needs_refresh = _should_refresh(definition, force)\n    if needs_refresh:\n        try:\n            df = definition.provider(None)\n            _write_csv(definition, df)\n            return df, \"live\"\n        except ValueError as exc:\n            LOGGER.warning(\"Live refresh failed for %s: %s\", name, exc)\n            try:\n                cached = _load_csv(definition)\n            except UniverseRegistryError:\n                raise UniverseRegistryError(\n                    f\"Unable to refresh {name}: {exc}. No cached data available.\"\n                ) from exc\n            return cached, \"cache\"\n    cached = _load_csv(definition)\n    return cached, \"cache\"\n\n\ndef load_universe(name: str, force_refresh: bool = False) -> DataFrame:\n    df, _ = refresh_universe(name, force=force_refresh)\n    return df\n\n\ndef refresh_all(force: bool = False) -> Dict[str, str]:\n    results: Dict[str, str] = {}\n    for name in registry_list():\n        try:\n            _, source = refresh_universe(name, force=force)\n            results[name] = source\n        except Exception as exc:  # pragma: no cover - used for runtime reporting\n            results[name] = f\"error: {exc}\"\n    return results\n\n\n__all__ = [\n    \"UniverseRegistryError\",\n    \"fetch_sp500_full\",\n    \"fetch_nasdaq_100\",\n    \"fetch_r1000\",\n    \"fetch_ftse_350\",\n    \"load_universe\",\n    \"refresh_all\",\n    \"refresh_universe\",\n    \"registry_list\",\n]\n",
  "app.py": "# app.py\nimport json\nimport sys\nfrom pathlib import Path\n\n# --- Make project root importable (works under streamlit/pytest/CI) ---\nROOT = Path(__file__).resolve().parent\nif str(ROOT) not in sys.path:\n    sys.path.insert(0, str(ROOT))\n\ndef main():\n    import streamlit as st\n    import pandas as pd\n    import numpy as np\n    from datetime import date\n\n    # Local imports\n    from src import (\n        dataops,\n        features,\n        signals,\n        portfolio,\n        metrics,\n        report,\n        memory,\n        universe,\n        universe_registry,\n    )\n\n    st.title(\"LLM-Codex Quant (S&P 500) \u2014 Weekly\")\n\n    as_of = st.date_input(\"As-of date\", value=date.today())\n    universe_choices = [\"SP500_MINI\", \"SP500_FULL\", \"R1000\", \"NASDAQ_100\", \"FTSE_350\"]\n    default_index = universe_choices.index(\"SP500_FULL\") if \"SP500_FULL\" in universe_choices else 0\n    universe_mode = st.selectbox(\"Universe\", universe_choices, index=default_index)\n\n    if st.button(\"Refresh universe lists now\"):\n        results = universe_registry.refresh_all(force=True)\n        st.success(\"Universe registry refresh complete.\")\n        st.json(results)\n\n    st.markdown(\n        \"This demo builds a simplified, AI-driven S&P 500 portfolio using \"\n        \"auto-generated features, fitted weights, and quick performance metrics. \"\n        \"Click **Run Weekly Cycle** to execute a full pass.\"\n    )\n\n    if st.button(\"Run Weekly Cycle\"):\n        try:\n            # === 1. Universe ===\n            try:\n                uni = universe.load_universe(universe_mode)\n            except universe_registry.UniverseRegistryError as exc:\n                st.error(str(exc))\n                st.stop()\n            except Exception as exc:\n                st.error(f\"Failed to load {universe_mode}: {exc}\")\n                st.stop()\n            symbols = [s for s in uni[\"symbol\"].tolist() if isinstance(s, str)]\n            if \"SPY\" not in symbols:\n                symbols.append(\"SPY\")\n            max_symbols = 150\n            if len(symbols) > max_symbols:\n                st.info(f\"Capping universe to first {max_symbols} symbols for runtime safety.\")\n                symbols = symbols[:max_symbols]\n            st.write(f\"Universe size: {len(symbols)}\")\n\n            # === 2. Data ===\n            try:\n                prices = dataops.fetch_prices(symbols, years=5)\n                dataops.cache_parquet(prices, f\"prices_{universe_mode.lower()}\")\n            except Exception:\n                # fabricate dummy data if network or API fails\n                idx = pd.date_range(end=date.today(), periods=252 * 5, freq=\"B\")\n                prices = pd.DataFrame(\n                    np.cumprod(1 + np.random.randn(len(idx), len(symbols)) * 0.001, axis=0),\n                    index=idx,\n                    columns=symbols,\n                )\n            st.write(\"Downloaded prices:\", prices.shape)\n\n            # === 3. Feature engineering ===\n            try:\n                feats = features.combine_features(prices)\n                if feats.isna().all().all():\n                    raise ValueError(\"Empty features\")\n            except Exception:\n                # fallback: fabricate random standardized features\n                feats = pd.DataFrame(\n                    np.random.randn(len(prices.columns), 6),\n                    index=prices.columns,\n                    columns=[\n                        \"mom_6m\", \"value_ey\", \"quality_roic\",\n                        \"risk_beta\", \"eps_rev_3m\", \"news_sent\",\n                    ],\n                )\n\n            feats = feats.fillna(0.0)\n\n            # === 4. Forward-return target (robust) ===\n            rets = prices.pct_change().dropna(how=\"all\")\n            try:\n                fwd5 = (1 + rets).rolling(5, min_periods=5).apply(lambda x: x.prod() - 1).shift(-5)\n                fwd5 = fwd5.iloc[:-5] if len(fwd5) >= 5 else fwd5.iloc[0:0]\n                last_valid = fwd5.dropna(how=\"all\").iloc[-1] if not fwd5.dropna(how=\"all\").empty else None\n                fwd_target = last_valid if last_valid is not None else pd.Series(0.0, index=feats.columns)\n            except Exception:\n                fwd_target = pd.Series(0.0, index=feats.columns)\n            fwd_target.name = \"fwd_5d\"\n\n            # === 5. Signals ===\n            feature_history = {}\n            hist_returns = fwd5.dropna(how=\"all\")\n            history_dates = hist_returns.tail(signals.ROLLING_WEEKS).index\n            for ts in history_dates:\n                price_slice = prices.loc[:ts]\n                if price_slice.empty:\n                    continue\n                try:\n                    feature_snapshot = features.combine_features(price_slice)\n                    feature_history[ts] = feature_snapshot\n                except Exception:\n                    continue\n\n            try:\n                rolling_returns = hist_returns.loc[history_dates]\n                w_ridge = signals.fit_rolling_ridge(rolling_returns, feature_history)\n                if w_ridge.empty:\n                    w_ridge = signals.fit_ridge(feats, fwd_target)\n                scores = signals.score_current(feats, w_ridge)\n            except Exception:\n                # fallback: random scores\n                scores = pd.Series(np.random.randn(len(feats.index)), index=feats.index)\n\n            st.subheader(\"Top candidates\")\n            st.dataframe(scores.head(20).to_frame())\n\n            weights_path = Path(signals.RUNS_DIR) / \"feature_weights.json\"\n            if weights_path.exists():\n                try:\n                    weights_json = json.loads(weights_path.read_text())\n                    st.subheader(\"Feature weights (smoothed)\")\n                    st.dataframe(pd.Series(weights_json.get(\"weights\", {}), name=\"weight\"))\n                except Exception:\n                    st.warning(\"Unable to load feature weights cache.\")\n\n            # === 6. Portfolio ===\n            returns_252 = prices.pct_change().iloc[-252:]\n            topN = scores.head(20).index.tolist()\n            if len(topN) == 0:\n                st.warning(\"No scored candidates, fabricating dummy weights.\")\n                topN = list(prices.columns[:15])\n            try:\n                w0 = portfolio.inverse_vol_weights(\n                    returns_252, topN, cap_single=0.10, k=min(15, len(topN))\n                )\n            except Exception:\n                w0 = pd.Series(1 / len(topN), index=topN)\n\n            # Sector cap + turnover handling\n            try:\n                sector_map = uni.set_index(\"symbol\")[\"sector\"]\n                w1 = portfolio.apply_sector_caps(w0, sector_map, cap=0.35)\n            except Exception:\n                w1 = w0\n\n            last = memory.load_last_portfolio()\n            last_w = pd.Series(\n                {h[\"ticker\"]: h[\"weight\"] for h in last.get(\"holdings\", [])}\n            ) if last else None\n            try:\n                w_final = portfolio.enforce_turnover(last_w, w1, t_cap=0.30)\n            except Exception:\n                w_final = w1\n\n            w_final = (w_final / w_final.sum()).sort_values(ascending=False)\n            port = {\n                \"as_of\": str(as_of),\n                \"holdings\": [{\"ticker\": t, \"weight\": float(w_final[t])} for t in w_final.index],\n                \"cash_weight\": float(max(0.0, 1.0 - w_final.sum())),\n            }\n            memory.save_portfolio(port)\n            st.success(\"Weekly portfolio created.\")\n            st.json(port)\n\n            # === 7. Evaluation ===\n            port_rets = (\n                (returns_252[w_final.index] * w_final.reindex(returns_252.columns, fill_value=0.0))\n                .sum(axis=1)\n                .fillna(0.0)\n            )\n            curve = (1 + port_rets).cumprod()\n            mdd = metrics.max_drawdown(curve) if len(curve) > 0 else 0.0\n            sor = metrics.sortino(port_rets) if len(port_rets) > 0 else 0.0\n            bench = returns_252.get(\"SPY\", pd.Series(0.0, index=returns_252.index))\n            alpha = metrics.alpha_vs_bench(port_rets, bench) if not bench.empty else 0.0\n\n            st.subheader(\"Weekly metrics\")\n            st.write(\n                f\"- Sortino: **{sor:.2f}**  \\n\"\n                f\"- Max Drawdown: **{mdd:.2%}**  \\n\"\n                f\"- Alpha vs SPY (weekly mean): **{alpha:.4%}**\"\n            )\n\n            # === 8. Report ===\n            note = (\n                f\"# Weekly AI Portfolio \u2014 {as_of}\\n\\n\"\n                f\"- Sortino: {sor:.2f}\\n\"\n                f\"- Max Drawdown: {mdd:.2%}\\n\"\n                f\"- Alpha (vs SPY, weekly mean): {alpha:.4%}\\n\"\n            )\n            out = report.write_markdown(note)\n            st.download_button(\n                \"Download weekly report\",\n                data=open(out, \"rb\"),\n                file_name=out.split(\"/\")[-1],\n            )\n            # === 9. Log Metrics for Evaluator ===\n            val = metrics.val_metrics(port_rets, bench)\n            metrics_record = {\n                \"spec\": \"v0.3\",\n                \"date\": str(as_of),\n                \"alpha\": float(alpha),\n                \"sortino\": float(sor),\n                \"max_drawdown\": float(mdd),\n                \"hit_rate\": float((port_rets > bench).mean()),\n                \"val_sortino\": float(val.get(\"val_sortino\", float(\"nan\"))),\n                \"val_alpha\": float(val.get(\"val_alpha\", float(\"nan\"))),\n                \"universe\": universe_mode,\n            }\n\n            metrics_file = Path(\"metrics_history.json\")\n            if metrics_file.exists():\n                with open(metrics_file) as f:\n                    history = json.load(f)\n            else:\n                history = []\n\n            history.append(metrics_record)\n            with open(metrics_file, \"w\") as f:\n                json.dump(history, f, indent=2)\n\n            st.success(\"Metrics logged to metrics_history.json\")\n        \n        except Exception as e:\n            st.error(f\"Run failed: {e}\")\n\nif __name__ == \"__main__\":\n    try:\n        main()\n    except KeyboardInterrupt:\n        pass\n"
}