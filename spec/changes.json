{
  "src/universe_registry.py": "from __future__ import annotations\n\nimport json\nimport logging\nfrom dataclasses import dataclass\nfrom datetime import datetime, timezone, timedelta\nfrom html.parser import HTMLParser\nfrom io import StringIO\nfrom pathlib import Path\nfrom typing import Callable, Dict, List, Optional, Tuple\n\nimport pandas as pd\nimport requests\nfrom pandas import DataFrame\nfrom requests import Response\nfrom requests.adapters import HTTPAdapter\nfrom urllib3.exceptions import InsecureRequestWarning\nfrom urllib3.util.retry import Retry\n\nfrom .config import REF_DIR\n\nLOGGER = logging.getLogger(__name__)\n\n# Silence only the insecure request warnings that can pop up on some Wikipedia mirrors\nrequests.packages.urllib3.disable_warnings(category=InsecureRequestWarning)  # type: ignore[attr-defined]\n\nProviderFn = Callable[[Optional[Path]], DataFrame]\n\n\ndef _build_session() -> requests.Session:\n    session = requests.Session()\n    session.headers.update(\n        {\n            \"User-Agent\": (\n                \"Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) \"\n                \"AppleWebKit/537.36 (KHTML, like Gecko) \"\n                \"Chrome/123.0.0.0 Safari/537.36\"\n            )\n        }\n    )\n    retries = Retry(total=3, backoff_factor=0.5, status_forcelist=[429, 500, 502, 503, 504])\n    adapter = HTTPAdapter(max_retries=retries)\n    session.mount(\"https://\", adapter)\n    session.mount(\"http://\", adapter)\n    return session\n\n\n_SESSION = _build_session()\n\n\n@dataclass\nclass UniverseDefinition:\n    name: str\n    url: str\n    csv_filename: str\n    provider: Optional[ProviderFn]\n    refresh_days: int = 90\n\n    @property\n    def csv_path(self) -> Path:\n        return REF_DIR / self.csv_filename\n\n\n_COLUMN_NORMALISATION = {\n    \"symbol\": [\"symbol\", \"ticker\", \"code\", \"tickers\", \"epic\", \"tidm\", \"ric\"],\n    \"name\": [\"security\", \"name\", \"company\", \"constituent\", \"issuer\"],\n    \"sector\": [\n        \"gics sector\",\n        \"sector\",\n        \"industry\",\n        \"icb sector\",\n        \"icb industry\",\n        \"gics sub-industry\",\n    ],\n}\n\n_MIN_ROWS = {\n    \"SP500_FULL\": 6,\n    \"R1000\": 8,\n    \"NASDAQ_100\": 6,\n    \"FTSE_350\": 8,\n}\n\n_SPEC_MIN_CONSTRAINTS: Optional[Dict[str, int]] = None\n\n\ndef _load_spec_min_constraints() -> Dict[str, int]:\n    global _SPEC_MIN_CONSTRAINTS\n    if _SPEC_MIN_CONSTRAINTS is not None:\n        return _SPEC_MIN_CONSTRAINTS\n\n    spec_path = Path(__file__).resolve().parent.parent / \"spec\" / \"current_spec.json\"\n    try:\n        data = json.loads(spec_path.read_text())\n        selection = data.get(\"universe_selection\", {})\n        constraints = selection.get(\"constraints\", {})\n        min_map = constraints.get(\"min_constituents\", {})\n        if isinstance(min_map, dict):\n            _SPEC_MIN_CONSTRAINTS = {\n                key: int(value)\n                for key, value in min_map.items()\n                if value is not None\n            }\n        else:\n            _SPEC_MIN_CONSTRAINTS = {}\n    except (FileNotFoundError, json.JSONDecodeError, TypeError, ValueError):\n        _SPEC_MIN_CONSTRAINTS = {}\n    return _SPEC_MIN_CONSTRAINTS\n\n\ndef expected_min_constituents(name: str) -> int:\n    \"\"\"Return expected minimum count for a universe.\n\n    Preference order:\n        1. Spec-defined constraints (spec/current_spec.json).\n        2. Local fallback map based on historical expectations.\n        3. Default of 1 to avoid division by zero.\n    \"\"\"\n\n    min_constraints = _load_spec_min_constraints()\n    if name in min_constraints:\n        return max(1, int(min_constraints[name]))\n    if \"default\" in min_constraints:\n        return max(1, int(min_constraints[\"default\"]))\n    if name in _MIN_ROWS:\n        return max(1, int(_MIN_ROWS[name]))\n    return 1\n\n\nclass UniverseRegistryError(RuntimeError):\n    \"\"\"Raised when a universe cannot be loaded from either live or cached data.\"\"\"\n\n\ndef registry_list() -> List[str]:\n    return list(_UNIVERSES.keys())\n\n\ndef _read_html(url: str, html_path: Optional[Path]) -> List[DataFrame]:\n    def _read_from_text(text: str) -> List[DataFrame]:\n        for flavor in (\"lxml\", \"bs4\"):\n            try:\n                return pd.read_html(StringIO(text), flavor=flavor)\n            except ImportError:\n                continue\n        return _parse_html_tables_basic(text)\n\n    if html_path is not None:\n        content = Path(html_path).read_text(encoding=\"utf-8\")\n        return _read_from_text(content)\n    try:\n        response: Response = _SESSION.get(url, timeout=10)\n        response.raise_for_status()\n    except requests.RequestException as exc:  # pragma: no cover - exercised via ValueError path\n        raise ValueError(f\"Failed to download universe page from {url}: {exc}\") from exc\n    return _read_from_text(response.text)\n\n\nclass _TableParser(HTMLParser):\n    def __init__(self) -> None:\n        super().__init__()\n        self.tables: List[List[Tuple[List[str], List[str]]]] = []\n        self._in_table = False\n        self._in_row = False\n        self._capture = False\n        self._current_table: List[Tuple[List[str], List[str]]] = []\n        self._current_row_cells: List[str] = []\n        self._current_row_types: List[str] = []\n        self._buffer: List[str] = []\n\n    def handle_starttag(self, tag: str, attrs) -> None:  # type: ignore[override]\n        tag = tag.lower()\n        if tag == \"table\":\n            self._in_table = True\n            self._current_table = []\n        elif self._in_table and tag == \"tr\":\n            self._in_row = True\n            self._current_row_cells = []\n            self._current_row_types = []\n        elif self._in_table and self._in_row and tag in {\"td\", \"th\"}:\n            self._capture = True\n            self._buffer = []\n            self._current_row_types.append(tag)\n\n    def handle_data(self, data: str) -> None:  # type: ignore[override]\n        if self._capture:\n            self._buffer.append(data)\n\n    def handle_endtag(self, tag: str) -> None:  # type: ignore[override]\n        tag = tag.lower()\n        if self._in_table and self._in_row and tag in {\"td\", \"th\"} and self._capture:\n            text = \"\".join(self._buffer).strip()\n            self._current_row_cells.append(text)\n            self._buffer = []\n            self._capture = False\n        elif self._in_table and tag == \"tr\" and self._in_row:\n            if self._current_row_cells:\n                self._current_table.append((self._current_row_types, self._current_row_cells))\n            self._in_row = False\n        elif tag == \"table\" and self._in_table:\n            if self._current_table:\n                self.tables.append(self._current_table)\n            self._in_table = False\n            self._current_table = []\n\n\ndef _parse_html_tables_basic(text: str) -> List[DataFrame]:\n    parser = _TableParser()\n    parser.feed(text)\n    frames: List[DataFrame] = []\n    for table in parser.tables:\n        if not table:\n            continue\n        header: Optional[List[str]] = None\n        rows: List[List[str]] = []\n        max_len = 0\n        for cell_types, cells in table:\n            max_len = max(max_len, len(cells))\n            if header is None and any(t == \"th\" for t in cell_types):\n                header = cells\n            else:\n                rows.append(cells)\n        if not rows:\n            continue\n        if header is None:\n            header = [f\"col_{idx}\" for idx in range(max_len)]\n        header = [col or f\"col_{idx}\" for idx, col in enumerate(header)]\n        normalised_rows = [row + [\"\"] * (max_len - len(row)) for row in rows]\n        frame = pd.DataFrame(normalised_rows, columns=header[:max_len])\n        frames.append(frame)\n    if not frames:\n        raise ValueError(\"No HTML tables could be parsed without lxml/bs4\")\n    return frames\n\n\ndef _flatten_columns(df: DataFrame) -> DataFrame:\n    if isinstance(df.columns, pd.MultiIndex):\n        df = df.copy()\n        df.columns = [\n            \" \".join(str(level).strip() for level in col if str(level) != \"nan\").strip()\n            for col in df.columns\n        ]\n    return df\n\n\ndef _normalize_headers(df: DataFrame) -> DataFrame:\n    df = _flatten_columns(df)\n    df = df.copy()\n    df.columns = [str(col).strip().lower() for col in df.columns]\n    return df\n\n\ndef _maybe_promote_first_row(df: DataFrame) -> DataFrame:\n    if df.empty:\n        return df\n    first_row = _to_str_series(df.iloc[0])\n    lowered_values = [value.lower() for value in first_row.tolist()]\n    candidate_values = {\n        value for values in _COLUMN_NORMALISATION.values() for value in values\n    }\n    if not any(value in candidate_values for value in lowered_values):\n        return df\n    df = df.iloc[1:].copy()\n    df.columns = [value or f\"col_{idx}\" for idx, value in enumerate(lowered_values)]\n    df.columns = [str(col).strip().lower() for col in df.columns]\n    return df\n\n\ndef _find_column(df: DataFrame, candidates: List[str]) -> Optional[str]:\n    lowered = {str(col).strip().lower(): col for col in df.columns}\n    for candidate in candidates:\n        if candidate in lowered:\n            return lowered[candidate]\n    for col in df.columns:\n        col_norm = str(col).strip().lower()\n        for candidate in candidates:\n            if candidate in col_norm:\n                return col\n    return None\n\n\ndef _to_str_series(s: pd.Series) -> pd.Series:\n    if not isinstance(s, pd.Series):\n        s = pd.Series(s)\n    s = s.astype(\"string\")\n    s = s.fillna(\"\").str.replace(r\"\\s+\", \" \", regex=True).str.strip()\n    s = s.str.replace(r\"\\[\\d+\\]\", \"\", regex=True).str.strip()\n    return s\n\n\ndef _ensure_l_suffix(sym: str) -> str:\n    sym = sym or \"\"\n    if not sym:\n        return sym\n    if sym.endswith(\".L\"):\n        return sym\n    if sym.endswith(\".\"):\n        return f\"{sym[:-1]}.L\"\n    return f\"{sym}.L\"\n\n\ndef _normalize_universe_df(df: pd.DataFrame, universe: str) -> pd.DataFrame:\n    df = _normalize_headers(df)\n    df = _maybe_promote_first_row(df)\n    df = df.replace({pd.NA: \"\", None: \"\"})\n\n    symbol_col = _find_column(df, _COLUMN_NORMALISATION[\"symbol\"])\n    if symbol_col is None:\n        raise ValueError(f\"No symbol-like column found while parsing {universe}\")\n    name_col = _find_column(df, _COLUMN_NORMALISATION[\"name\"])\n    sector_col = _find_column(df, _COLUMN_NORMALISATION[\"sector\"])\n\n    symbol = _to_str_series(df[symbol_col]).str.upper()\n    name = _to_str_series(df[name_col]) if name_col else pd.Series(\"\", index=df.index, dtype=\"string\")\n    sector = (\n        _to_str_series(df[sector_col])\n        if sector_col\n        else pd.Series(\"\", index=df.index, dtype=\"string\")\n    )\n\n    out = pd.DataFrame({\"symbol\": symbol, \"name\": name, \"sector\": sector})\n    out = out.loc[out[\"symbol\"] != \"\"].copy()\n\n    if universe.upper() == \"FTSE_350\":\n        out[\"symbol\"] = out[\"symbol\"].map(_ensure_l_suffix)\n\n    out = out.drop_duplicates(subset=\"symbol\", keep=\"first\")\n\n    return out[[\"symbol\", \"name\", \"sector\"]].reset_index(drop=True)\n\n\ndef _score_table(df: DataFrame) -> Tuple[int, int]:\n    score = 0\n    if _find_column(df, _COLUMN_NORMALISATION[\"name\"]):\n        score += 1\n    if _find_column(df, _COLUMN_NORMALISATION[\"sector\"]):\n        score += 1\n    return score, len(df)\n\ndef _prepare_candidate_table(table: DataFrame) -> DataFrame:\n    table = _normalize_headers(table)\n    table = _maybe_promote_first_row(table)\n    return table\n\n\ndef _extract_symbol_tables(url: str, html_path: Optional[Path]) -> List[DataFrame]:\n    tables = _read_html(url, html_path)\n    candidates: List[DataFrame] = []\n    for table in tables:\n        candidate = _prepare_candidate_table(table)\n        if _find_column(candidate, _COLUMN_NORMALISATION[\"symbol\"]) is None:\n            continue\n        candidates.append(candidate)\n    if not candidates:\n        raise ValueError(f\"No table with symbol-like column found for {url}\")\n    return candidates\n\n\ndef _extract_symbol_table(url: str, html_path: Optional[Path]) -> DataFrame:\n    candidates = _extract_symbol_tables(url, html_path)\n    best = max(candidates, key=_score_table)\n    return best\n\n\ndef fetch_sp500_full(html_path: Optional[Path] = None) -> DataFrame:\n    url = \"https://en.wikipedia.org/wiki/List_of_S%26P_500_companies\"\n    try:\n        table = _extract_symbol_table(url, html_path)\n        return _normalize_universe_df(table, \"SP500_FULL\")\n    except ValueError as exc:\n        raise ValueError(f\"SP500_FULL provider failed: {exc}\") from exc\n\n\ndef fetch_nasdaq_100(html_path: Optional[Path] = None) -> DataFrame:\n    url = \"https://en.wikipedia.org/wiki/NASDAQ-100\"\n    try:\n        table = _extract_symbol_table(url, html_path)\n        return _normalize_universe_df(table, \"NASDAQ_100\")\n    except ValueError as exc:\n        raise ValueError(f\"NASDAQ_100 provider failed: {exc}\") from exc\n\n\ndef fetch_r1000(html_path: Optional[Path] = None) -> DataFrame:\n    url = \"https://en.wikipedia.org/wiki/Russell_1000_Index\"\n    try:\n        table = _extract_symbol_table(url, html_path)\n        return _normalize_universe_df(table, \"R1000\")\n    except ValueError as exc:\n        raise ValueError(f\"R1000 provider failed: {exc}\") from exc\n\n\ndef _extract_largest_symbol_table(url: str, html_path: Optional[Path]) -> DataFrame:\n    candidates = _extract_symbol_tables(url, html_path)\n    largest = max(candidates, key=lambda frame: len(frame))\n    return largest\n\n\ndef _fetch_ftse_component(\n    url: str,\n    html_path: Optional[Path],\n    label: str,\n) -> DataFrame:\n    try:\n        table = _extract_largest_symbol_table(url, html_path)\n    except ValueError as exc:\n        raise ValueError(f\"{label} lookup failed: {exc}\") from exc\n    return _normalize_universe_df(table, \"FTSE_350\")\n\n\ndef fetch_ftse_350(\n    html_path: Optional[Path] = None,\n    *,\n    html_path_100: Optional[Path] = None,\n    html_path_250: Optional[Path] = None,\n) -> DataFrame:\n    url_100 = \"https://en.wikipedia.org/wiki/FTSE_100_Index\"\n    url_250 = \"https://en.wikipedia.org/wiki/FTSE_250_Index\"\n\n    if html_path is not None and html_path_100 is None and html_path_250 is None:\n        # Backwards compatibility with combined snapshot tests.\n        try:\n            tables = _extract_symbol_tables(\n                \"https://en.wikipedia.org/wiki/FTSE_350_Index\", html_path\n            )\n            normalized = [_normalize_universe_df(table, \"FTSE_350\") for table in tables]\n            combined = pd.concat(normalized, ignore_index=True)\n            combined = combined.drop_duplicates(subset=\"symbol\", keep=\"first\")\n            return combined.reset_index(drop=True)\n        except ValueError as exc:\n            raise ValueError(f\"FTSE_350 provider failed: {exc}\") from exc\n\n    errors: List[str] = []\n    frames: List[DataFrame] = []\n    components = [\n        (\"FTSE 100\", url_100, html_path_100),\n        (\"FTSE 250\", url_250, html_path_250),\n    ]\n\n    for label, url, path in components:\n        try:\n            frame = _fetch_ftse_component(url, path, label)\n        except ValueError as exc:\n            errors.append(str(exc))\n        else:\n            frames.append(frame)\n\n    if errors:\n        joined = \"; \".join(errors)\n        raise ValueError(f\"FTSE_350 provider failed: {joined}\")\n\n    combined = pd.concat(frames, ignore_index=True)\n    combined = combined.drop_duplicates(subset=\"symbol\", keep=\"first\")\n    return combined.reset_index(drop=True)[[\"symbol\", \"name\", \"sector\"]]\n\n\n_UNIVERSES: Dict[str, UniverseDefinition] = {\n    \"SP500_FULL\": UniverseDefinition(\n        name=\"SP500_FULL\",\n        url=\"https://en.wikipedia.org/wiki/List_of_S%26P_500_companies\",\n        csv_filename=\"sp500_full.csv\",\n        provider=fetch_sp500_full,\n        refresh_days=90,\n    ),\n    \"R1000\": UniverseDefinition(\n        name=\"R1000\",\n        url=\"https://en.wikipedia.org/wiki/Russell_1000_Index\",\n        csv_filename=\"r1000.csv\",\n        provider=fetch_r1000,\n        refresh_days=90,\n    ),\n    \"NASDAQ_100\": UniverseDefinition(\n        name=\"NASDAQ_100\",\n        url=\"https://en.wikipedia.org/wiki/NASDAQ-100\",\n        csv_filename=\"nasdaq_100.csv\",\n        provider=fetch_nasdaq_100,\n        refresh_days=60,\n    ),\n    \"FTSE_350\": UniverseDefinition(\n        name=\"FTSE_350\",\n        url=\"https://en.wikipedia.org/wiki/FTSE_350_Index\",\n        csv_filename=\"ftse_350.csv\",\n        provider=fetch_ftse_350,\n        refresh_days=60,\n    ),\n    \"SP500_MINI\": UniverseDefinition(\n        name=\"SP500_MINI\",\n        url=\"\",\n        csv_filename=\"sp500_mini.csv\",\n        provider=None,\n        refresh_days=0,\n    ),\n}\n\n\ndef _should_refresh(definition: UniverseDefinition, force: bool) -> bool:\n    if force:\n        return True\n    path = definition.csv_path\n    if not path.exists():\n        return True\n    if definition.refresh_days <= 0:\n        return False\n    modified = datetime.fromtimestamp(path.stat().st_mtime, tz=timezone.utc)\n    age = datetime.now(tz=timezone.utc) - modified\n    return age > timedelta(days=definition.refresh_days)\n\n\ndef _ensure_directory(path: Path) -> None:\n    path.parent.mkdir(parents=True, exist_ok=True)\n\n\ndef _write_csv(definition: UniverseDefinition, df: DataFrame) -> None:\n    _ensure_directory(definition.csv_path)\n    df.to_csv(definition.csv_path, index=False)\n\n\ndef _load_csv(definition: UniverseDefinition) -> DataFrame:\n    if not definition.csv_path.exists():\n        raise UniverseRegistryError(\n            f\"No cached CSV found for {definition.name} at {definition.csv_path}\"\n        )\n    df = pd.read_csv(definition.csv_path)\n    try:\n        normalized = _normalize_universe_df(df, definition.name)\n    except ValueError as exc:\n        raise UniverseRegistryError(\n            f\"Cached CSV for {definition.name} could not be parsed: {exc}\"\n        ) from exc\n    expected_columns = {\"symbol\", \"name\", \"sector\"}\n    missing = expected_columns.difference(normalized.columns)\n    if missing:\n        raise UniverseRegistryError(\n            f\"Cached CSV for {definition.name} is missing columns: {sorted(missing)}\"\n        )\n    return normalized\n\n\ndef refresh_universe(name: str, force: bool = False) -> Tuple[DataFrame, str]:\n    name = (name or \"\").upper()\n    if name not in _UNIVERSES:\n        raise ValueError(f\"Unknown universe: {name}\")\n    definition = _UNIVERSES[name]\n\n    if definition.provider is None:\n        df = _load_csv(definition)\n        return df, \"cache\"\n\n    needs_refresh = _should_refresh(definition, force)\n    if needs_refresh:\n        try:\n            df = definition.provider(None)\n            _write_csv(definition, df)\n            return df, \"live\"\n        except ValueError as exc:\n            LOGGER.warning(\"Live refresh failed for %s: %s\", name, exc)\n            try:\n                cached = _load_csv(definition)\n            except UniverseRegistryError:\n                raise UniverseRegistryError(\n                    f\"Unable to refresh {name}: {exc}. No cached data available.\"\n                ) from exc\n            return cached, \"cache\"\n    cached = _load_csv(definition)\n    return cached, \"cache\"\n\n\ndef load_universe(name: str, force_refresh: bool = False) -> DataFrame:\n    df, _ = refresh_universe(name, force=force_refresh)\n    return df\n\n\ndef refresh_all(force: bool = False) -> Dict[str, str]:\n    results: Dict[str, str] = {}\n    for name in registry_list():\n        try:\n            _, source = refresh_universe(name, force=force)\n            results[name] = source\n        except Exception as exc:  # pragma: no cover - used for runtime reporting\n            results[name] = f\"error: {exc}\"\n    return results\n\n\n__all__ = [\n    \"UniverseRegistryError\",\n    \"fetch_sp500_full\",\n    \"fetch_nasdaq_100\",\n    \"fetch_r1000\",\n    \"fetch_ftse_350\",\n    \"load_universe\",\n    \"refresh_all\",\n    \"refresh_universe\",\n    \"registry_list\",\n]\n",
  "app.py": "# app.py\nimport json\nimport sys\nfrom pathlib import Path\n\n# --- Make project root importable (works under streamlit/pytest/CI) ---\nROOT = Path(__file__).resolve().parent\nif str(ROOT) not in sys.path:\n    sys.path.insert(0, str(ROOT))\n\ndef main():\n    import streamlit as st\n    import pandas as pd\n    import numpy as np\n    from datetime import date\n\n    # Local imports\n    from src import (\n        dataops,\n        features,\n        signals,\n        portfolio,\n        metrics,\n        report,\n        memory,\n        universe,\n        universe_registry,\n        universe_selector,\n    )\n\n    st.title(\"LLM-Codex Quant (S&P 500) \u2014 Weekly\")\n\n    as_of = st.date_input(\"As-of date\", value=date.today())\n    universe_choices = [\"SP500_MINI\", \"SP500_FULL\", \"R1000\", \"NASDAQ_100\", \"FTSE_350\"]\n    default_index = (\n        universe_choices.index(\"SP500_FULL\") if \"SP500_FULL\" in universe_choices else 0\n    )\n\n    universe_mode = st.selectbox(\"Universe Mode\", [\"auto\", \"manual\"], index=0)\n    manual_universe = None\n    if universe_mode == \"manual\":\n        manual_universe = st.selectbox(\"Universe\", universe_choices, index=default_index)\n\n    if st.button(\"Refresh universe lists now\"):\n        results = universe_registry.refresh_all(force=True)\n        st.success(\"Universe registry refresh complete.\")\n        st.json(results)\n\n        row_counts = {}\n        symbol_samples = {}\n        universes = getattr(universe_registry, \"_UNIVERSES\", {})\n        for name, status in results.items():\n            if isinstance(status, str) and status.lower().startswith(\"error:\"):\n                st.markdown(f\"**{name}**\")\n                st.error(status)\n                continue\n            definition = universes.get(name)\n            if not definition:\n                continue\n            try:\n                df = universe_registry.load_universe(name)\n                row_counts[name] = len(df)\n                symbol_samples[name] = df[\"symbol\"].head(5).tolist()\n            except Exception as exc:\n                st.markdown(f\"**{name}**\")\n                st.error(f\"error: {exc}\")\n\n        if row_counts:\n            st.write(\"Latest universe row counts:\")\n            st.json(row_counts)\n        if symbol_samples:\n            st.write(\"Sample symbols (first 5):\")\n            st.json(symbol_samples)\n\n    st.markdown(\n        \"This demo builds a simplified, AI-driven S&P 500 portfolio using \"\n        \"auto-generated features, fitted weights, and quick performance metrics. \"\n        \"Click **Run Weekly Cycle** to execute a full pass.\"\n    )\n\n    if st.button(\"Run Weekly Cycle\"):\n        try:\n            spec_path = Path(\"spec/current_spec.json\")\n            spec_data = json.loads(spec_path.read_text()) if spec_path.exists() else {}\n\n            decision_info = None\n            selection_cfg = spec_data.get(\"universe_selection\", {})\n            if universe_mode == \"auto\":\n                candidates = selection_cfg.get(\"candidates\") or spec_data.get(\"universe\", {}).get(\n                    \"modes\", []\n                )\n                if not candidates:\n                    candidates = universe_choices\n                constraints = selection_cfg.get(\"constraints\", {}) or {}\n                decision_info = universe_selector.choose_universe(\n                    candidates,\n                    constraints,\n                    universe_registry.load_universe,\n                    Path(\"metrics_history.json\"),\n                    spec_data,\n                    as_of,\n                )\n                selected_universe_name = decision_info.get(\"winner\") or candidates[0]\n            else:\n                selected_universe_name = manual_universe or universe_choices[default_index]\n\n            # === 1. Universe ===\n            try:\n                uni = universe.load_universe(selected_universe_name)\n            except universe_registry.UniverseRegistryError as exc:\n                st.error(str(exc))\n                st.stop()\n            except Exception as exc:\n                st.error(f\"Failed to load {selected_universe_name}: {exc}\")\n                st.stop()\n\n            if decision_info:\n                metrics_table = decision_info[\"metrics\"].copy()\n                display_cols = [\n                    \"alpha\",\n                    \"sortino\",\n                    \"mdd\",\n                    \"coverage\",\n                    \"turnover_cost\",\n                    \"n_weeks\",\n                ]\n                for col in display_cols:\n                    if col not in metrics_table.columns:\n                        metrics_table[col] = np.nan\n                summary_df = metrics_table[display_cols].copy()\n                summary_df[\"score\"] = pd.Series(decision_info[\"scores\"])\n                summary_df[\"probability\"] = pd.Series(decision_info[\"probabilities\"])\n                summary_df = summary_df.fillna(0.0)\n\n                def _highlight(row):\n                    if row.name == selected_universe_name:\n                        return [\"background-color: #0b5394; color: white\"] * len(row)\n                    return [\"\"] * len(row)\n\n                st.subheader(\"Universe decision (auto)\")\n                st.dataframe(\n                    summary_df.style.format(\n                        {\n                            \"alpha\": \"{:.4f}\",\n                            \"sortino\": \"{:.2f}\",\n                            \"mdd\": \"{:.2%}\",\n                            \"coverage\": \"{:.1%}\",\n                            \"turnover_cost\": \"{:.4%}\",\n                            \"probability\": \"{:.1%}\",\n                            \"score\": \"{:.4f}\",\n                        }\n                    ).apply(_highlight, axis=1)\n                )\n                st.caption(decision_info.get(\"rationale\", \"\"))\n\n            symbols = [s for s in uni[\"symbol\"].tolist() if isinstance(s, str)]\n            coverage_current = 0.0\n            if len(uni) > 0:\n                min_expected = universe_registry.expected_min_constituents(selected_universe_name)\n                coverage_current = (\n                    float(len(uni)) / float(min_expected) if min_expected else 0.0\n                )\n                coverage_current = float(min(1.0, coverage_current))\n            if \"SPY\" not in symbols:\n                symbols.append(\"SPY\")\n            max_symbols = 150\n            if len(symbols) > max_symbols:\n                st.info(f\"Capping universe to first {max_symbols} symbols for runtime safety.\")\n                symbols = symbols[:max_symbols]\n            st.write(f\"Universe size: {len(symbols)}\")\n\n            # === 2. Data ===\n            try:\n                prices = dataops.fetch_prices(symbols, years=5)\n                dataops.cache_parquet(prices, f\"prices_{selected_universe_name.lower()}\")\n            except Exception:\n                # fabricate dummy data if network or API fails\n                idx = pd.date_range(end=date.today(), periods=252 * 5, freq=\"B\")\n                prices = pd.DataFrame(\n                    np.cumprod(1 + np.random.randn(len(idx), len(symbols)) * 0.001, axis=0),\n                    index=idx,\n                    columns=symbols,\n                )\n            st.write(\"Downloaded prices:\", prices.shape)\n\n            # === 3. Feature engineering ===\n            try:\n                feats = features.combine_features(prices)\n                if feats.isna().all().all():\n                    raise ValueError(\"Empty features\")\n            except Exception:\n                # fallback: fabricate random standardized features\n                feats = pd.DataFrame(\n                    np.random.randn(len(prices.columns), 6),\n                    index=prices.columns,\n                    columns=[\n                        \"mom_6m\", \"value_ey\", \"quality_roic\",\n                        \"risk_beta\", \"eps_rev_3m\", \"news_sent\",\n                    ],\n                )\n\n            feats = feats.fillna(0.0)\n\n            # === 4. Forward-return target (robust) ===\n            rets = prices.pct_change().dropna(how=\"all\")\n            try:\n                fwd5 = (1 + rets).rolling(5, min_periods=5).apply(lambda x: x.prod() - 1).shift(-5)\n                fwd5 = fwd5.iloc[:-5] if len(fwd5) >= 5 else fwd5.iloc[0:0]\n                last_valid = fwd5.dropna(how=\"all\").iloc[-1] if not fwd5.dropna(how=\"all\").empty else None\n                fwd_target = last_valid if last_valid is not None else pd.Series(0.0, index=feats.columns)\n            except Exception:\n                fwd_target = pd.Series(0.0, index=feats.columns)\n            fwd_target.name = \"fwd_5d\"\n\n            # === 5. Signals ===\n            feature_history = {}\n            hist_returns = fwd5.dropna(how=\"all\")\n            history_dates = hist_returns.tail(signals.ROLLING_WEEKS).index\n            for ts in history_dates:\n                price_slice = prices.loc[:ts]\n                if price_slice.empty:\n                    continue\n                try:\n                    feature_snapshot = features.combine_features(price_slice)\n                    feature_history[ts] = feature_snapshot\n                except Exception:\n                    continue\n\n            try:\n                rolling_returns = hist_returns.loc[history_dates]\n                w_ridge = signals.fit_rolling_ridge(rolling_returns, feature_history)\n                if w_ridge.empty:\n                    w_ridge = signals.fit_ridge(feats, fwd_target)\n                scores = signals.score_current(feats, w_ridge)\n            except Exception:\n                # fallback: random scores\n                scores = pd.Series(np.random.randn(len(feats.index)), index=feats.index)\n\n            st.subheader(\"Top candidates\")\n            st.dataframe(scores.head(20).to_frame())\n\n            weights_path = Path(signals.RUNS_DIR) / \"feature_weights.json\"\n            if weights_path.exists():\n                try:\n                    weights_json = json.loads(weights_path.read_text())\n                    st.subheader(\"Feature weights (smoothed)\")\n                    st.dataframe(pd.Series(weights_json.get(\"weights\", {}), name=\"weight\"))\n                except Exception:\n                    st.warning(\"Unable to load feature weights cache.\")\n\n            # === 6. Portfolio ===\n            returns_252 = prices.pct_change().iloc[-252:]\n            topN = scores.head(20).index.tolist()\n            if len(topN) == 0:\n                st.warning(\"No scored candidates, fabricating dummy weights.\")\n                topN = list(prices.columns[:15])\n            try:\n                w0 = portfolio.inverse_vol_weights(\n                    returns_252, topN, cap_single=0.10, k=min(15, len(topN))\n                )\n            except Exception:\n                w0 = pd.Series(1 / len(topN), index=topN)\n\n            # Sector cap + turnover handling\n            try:\n                sector_map = uni.set_index(\"symbol\")[\"sector\"]\n                w1 = portfolio.apply_sector_caps(w0, sector_map, cap=0.35)\n            except Exception:\n                w1 = w0\n\n            last = memory.load_last_portfolio()\n            last_w = pd.Series(\n                {h[\"ticker\"]: h[\"weight\"] for h in last.get(\"holdings\", [])}\n            ) if last else None\n            try:\n                w_final = portfolio.enforce_turnover(last_w, w1, t_cap=0.30)\n            except Exception:\n                w_final = w1\n\n            w_final = (w_final / w_final.sum()).sort_values(ascending=False)\n            try:\n                turnover_fraction = float(portfolio.turnover(last_w, w_final))\n            except Exception:\n                turnover_fraction = float(w_final.abs().sum()) if w_final is not None else 0.0\n            port = {\n                \"as_of\": str(as_of),\n                \"holdings\": [{\"ticker\": t, \"weight\": float(w_final[t])} for t in w_final.index],\n                \"cash_weight\": float(max(0.0, 1.0 - w_final.sum())),\n            }\n            memory.save_portfolio(port)\n            st.success(\"Weekly portfolio created.\")\n            st.json(port)\n\n            # === 7. Evaluation ===\n            port_rets = (\n                (returns_252[w_final.index] * w_final.reindex(returns_252.columns, fill_value=0.0))\n                .sum(axis=1)\n                .fillna(0.0)\n            )\n            curve = (1 + port_rets).cumprod()\n            mdd = metrics.max_drawdown(curve) if len(curve) > 0 else 0.0\n            sor = metrics.sortino(port_rets) if len(port_rets) > 0 else 0.0\n            bench = returns_252.get(\"SPY\", pd.Series(0.0, index=returns_252.index))\n            alpha = metrics.alpha_vs_bench(port_rets, bench) if not bench.empty else 0.0\n\n            st.subheader(\"Weekly metrics\")\n            st.write(\n                f\"- Sortino: **{sor:.2f}**  \\n\"\n                f\"- Max Drawdown: **{mdd:.2%}**  \\n\"\n                f\"- Alpha vs SPY (weekly mean): **{alpha:.4%}**\"\n            )\n\n            # === 8. Report ===\n            note = (\n                f\"# Weekly AI Portfolio \u2014 {as_of}\\n\\n\"\n                f\"- Sortino: {sor:.2f}\\n\"\n                f\"- Max Drawdown: {mdd:.2%}\\n\"\n                f\"- Alpha (vs SPY, weekly mean): {alpha:.4%}\\n\"\n            )\n            out = report.write_markdown(note)\n            st.download_button(\n                \"Download weekly report\",\n                data=open(out, \"rb\"),\n                file_name=out.split(\"/\")[-1],\n            )\n            # === 9. Log Metrics for Evaluator ===\n            val = metrics.val_metrics(port_rets, bench)\n            metrics_record = {\n                \"spec\": str(spec_data.get(\"version\", \"v0.4\")),\n                \"date\": str(as_of),\n                \"alpha\": float(alpha),\n                \"sortino\": float(sor),\n                \"mdd\": float(mdd),\n                \"hit_rate\": float((port_rets > bench).mean()),\n                \"val_sortino\": float(val.get(\"val_sortino\", float(\"nan\"))),\n                \"val_alpha\": float(val.get(\"val_alpha\", float(\"nan\"))),\n                \"universe\": selected_universe_name,\n                \"coverage\": float(coverage_current),\n                \"turnover_cost\": float(0.0005 * turnover_fraction if turnover_fraction else 0.0),\n            }\n\n            metrics_file = Path(\"metrics_history.json\")\n            if metrics_file.exists():\n                try:\n                    history = json.loads(metrics_file.read_text())\n                    if not isinstance(history, list):\n                        history = []\n                except json.JSONDecodeError:\n                    history = []\n            else:\n                history = []\n\n            history.append(metrics_record)\n            with open(metrics_file, \"w\") as f:\n                json.dump(history, f, indent=2)\n\n            st.success(\"Metrics logged to metrics_history.json\")\n        \n        except Exception as e:\n            st.error(f\"Run failed: {e}\")\n\nif __name__ == \"__main__\":\n    try:\n        main()\n    except KeyboardInterrupt:\n        pass\n",
  "tests/test_universe_registry.py": "from pathlib import Path\n\nimport pandas as pd\nfrom pandas.api import types as ptypes\n\nfrom src import universe_registry\n\nSNAPSHOT_DIR = Path(\"data/reference/snapshots\")\n\n\ndef test_registry_known_universes():\n    expected = {\"SP500_MINI\", \"SP500_FULL\", \"R1000\", \"NASDAQ_100\", \"FTSE_350\"}\n    assert set(universe_registry.registry_list()) == expected\n\n\ndef test_parse_snapshots_offline():\n    snapshots = {\n        \"SP500_FULL\": universe_registry.fetch_sp500_full(\n            SNAPSHOT_DIR / \"sp500_wikipedia.html\"\n        ),\n        \"NASDAQ_100\": universe_registry.fetch_nasdaq_100(\n            SNAPSHOT_DIR / \"nasdaq100_wikipedia.html\"\n        ),\n        \"FTSE_350\": universe_registry.fetch_ftse_350(\n            html_path_100=SNAPSHOT_DIR / \"ftse100_wikipedia.html\",\n            html_path_250=SNAPSHOT_DIR / \"ftse250_wikipedia.html\",\n        ),\n        \"R1000\": universe_registry.fetch_r1000(\n            SNAPSHOT_DIR / \"r1000_wikipedia.html\"\n        ),\n    }\n\n    for name, frame in snapshots.items():\n        assert isinstance(frame, pd.DataFrame)\n        assert list(frame.columns) == [\"symbol\", \"name\", \"sector\"]\n        assert not frame.empty\n        assert len(frame) >= universe_registry._MIN_ROWS[name] * 0.5\n\n    assert snapshots[\"FTSE_350\"][\"symbol\"].str.endswith(\".L\").all()\n\n\ndef test_ftse350_composite_offline():\n    df = universe_registry.fetch_ftse_350(\n        html_path_100=SNAPSHOT_DIR / \"ftse100_wikipedia.html\",\n        html_path_250=SNAPSHOT_DIR / \"ftse250_wikipedia.html\",\n    )\n\n    assert list(df.columns) == [\"symbol\", \"name\", \"sector\"]\n    assert not df.empty\n    assert len(df) >= 4\n    assert df[\"symbol\"].str.endswith(\".L\").all()\n    assert df[\"symbol\"].is_unique\n\n\ndef test_ftse_suffixed_and_string_types():\n    df = universe_registry.fetch_ftse_350(\n        html_path_100=SNAPSHOT_DIR / \"ftse100_wikipedia.html\",\n        html_path_250=SNAPSHOT_DIR / \"ftse250_wikipedia.html\",\n    )\n\n    assert ptypes.is_string_dtype(df[\"symbol\"])\n    assert df[\"symbol\"].equals(df[\"symbol\"].str.upper())\n    assert df[\"symbol\"].str.endswith(\".L\").all()\n\n\ndef test_normalize_handles_integer_columns():\n    raw = pd.DataFrame(\n        {\n            \"Ticker\": [101, 202, 303, None],\n            \"Security\": [\"Alpha\", \" beta\", None, \"\"],\n            \"Sector\": [1, 2, None, 3],\n        }\n    )\n\n    normalized = universe_registry._normalize_universe_df(raw, \"R1000\")\n\n    assert len(normalized) == 3  # row with missing ticker dropped\n    assert ptypes.is_string_dtype(normalized[\"symbol\"])\n    assert normalized[\"symbol\"].equals(normalized[\"symbol\"].str.upper())\n\n\ndef test_provider_handles_integer_headers(tmp_path):\n    html = \"\"\"\n    <table>\n        <thead>\n            <tr><th>0</th><th>1</th><th>2</th></tr>\n            <tr><th>Symbol</th><th>Company</th><th>Sector</th></tr>\n        </thead>\n        <tbody>\n            <tr><td>aapl</td><td>Apple Inc</td><td>Technology</td></tr>\n            <tr><td>msft</td><td>Microsoft</td><td>Technology</td></tr>\n        </tbody>\n    </table>\n    \"\"\"\n    html_path = tmp_path / \"nasdaq_inline.html\"\n    html_path.write_text(html, encoding=\"utf-8\")\n\n    df = universe_registry.fetch_nasdaq_100(html_path)\n\n    assert list(df.columns) == [\"symbol\", \"name\", \"sector\"]\n    assert df.loc[0, \"symbol\"] == \"AAPL\"\n    assert df.loc[1, \"symbol\"] == \"MSFT\"\n\n\ndef test_refresh_writes_csvs(tmp_path, monkeypatch):\n    monkeypatch.setattr(universe_registry, \"REF_DIR\", tmp_path)\n\n    providers = {\n        \"SP500_FULL\": (universe_registry.fetch_sp500_full, \"sp500_wikipedia.html\"),\n        \"R1000\": (universe_registry.fetch_r1000, \"r1000_wikipedia.html\"),\n        \"NASDAQ_100\": (universe_registry.fetch_nasdaq_100, \"nasdaq100_wikipedia.html\"),\n        \"FTSE_350\": (\n            universe_registry.fetch_ftse_350,\n            (\"ftse100_wikipedia.html\", \"ftse250_wikipedia.html\"),\n        ),\n    }\n\n    for name, (fetcher, filename) in providers.items():\n        if name == \"FTSE_350\":\n            file_100, file_250 = filename\n\n            def _provider(\n                _html_path=None,\n                *,\n                func=fetcher,\n                path_100=SNAPSHOT_DIR / file_100,\n                path_250=SNAPSHOT_DIR / file_250,\n            ):\n                return func(html_path_100=path_100, html_path_250=path_250)\n\n        else:\n            snapshot_file = SNAPSHOT_DIR / filename\n\n            def _provider(_html_path=None, *, path=snapshot_file, func=fetcher):\n                return func(path)\n\n        monkeypatch.setattr(\n            universe_registry._UNIVERSES[name],  # type: ignore[attr-defined]\n            \"provider\",\n            _provider,\n            raising=False,\n        )\n        df, source = universe_registry.refresh_universe(name, force=True)\n        assert source == \"live\"\n        assert not df.empty\n        expected_path = tmp_path / universe_registry._UNIVERSES[name].csv_filename  # type: ignore[attr-defined]\n        assert expected_path.exists()\n",
  "data/reference/snapshots/ftse100_wikipedia.html": "<html>\n  <body>\n    <h2>FTSE 100 companies</h2>\n    <table class=\"wikitable\">\n      <thead>\n        <tr>\n          <th>EPIC</th>\n          <th>Company</th>\n          <th>Sector</th>\n        </tr>\n      </thead>\n      <tbody>\n        <tr><td>AZN</td><td>AstraZeneca plc</td><td>Health Care</td></tr>\n        <tr><td>BP.</td><td>BP plc</td><td>Energy</td></tr>\n      </tbody>\n    </table>\n  </body>\n</html>\n",
  "data/reference/snapshots/ftse250_wikipedia.html": "<html>\n  <body>\n    <h2>FTSE 250 companies</h2>\n    <table class=\"wikitable\">\n      <thead>\n        <tr>\n          <th>TIDM</th>\n          <th>Company</th>\n          <th>Sector</th>\n        </tr>\n      </thead>\n      <tbody>\n        <tr><td>SMIN</td><td>Smiths Group plc</td><td>Industrials</td></tr>\n        <tr><td>WDH</td><td>Warehouse REIT plc</td><td>Real Estate</td></tr>\n      </tbody>\n    </table>\n  </body>\n</html>\n",
  "src/codegen/file_contract.py": "SPEC_REQUIRED_KEYS = {\n  \"as_of\",\"universe\",\"data\",\"features\",\"weights\",\"risk\",\"evaluation\",\"change_proposal\"\n}\nALLOWED_DIRS = {\"src\", \"tests\", \"app.py\", \"prompts\", \"data/reference\"}\n",
  "src/universe_selector.py": "from __future__ import annotations\n\nimport json\nfrom dataclasses import dataclass\nfrom datetime import date\nfrom pathlib import Path\nfrom typing import Callable, Dict, Iterable, List, Tuple\n\nimport numpy as np\nimport pandas as pd\n\ntry:\n    from .universe_registry import expected_min_constituents as _expected_min_constituents\nexcept Exception:  # pragma: no cover - fallback if registry import fails\n\n    def _expected_min_constituents(name: str) -> int:\n        return 1\n\n\n_METRIC_COLUMNS = [\n    \"alpha\",\n    \"sortino\",\n    \"mdd\",\n    \"hit_rate\",\n    \"val_alpha\",\n    \"val_sortino\",\n    \"coverage\",\n    \"turnover_cost\",\n]\n\n\n@dataclass\nclass UniverseDecision:\n    winner: str\n    scores: Dict[str, float]\n    probabilities: Dict[str, float]\n    rationale: str\n    parameters: Dict[str, float]\n    metrics: pd.DataFrame\n    as_of: str\n    spec_version: str\n    lookback_weeks: int\n    min_weeks: int\n\n    def to_log_record(self) -> Dict[str, object]:\n        return {\n            \"as_of\": self.as_of,\n            \"spec\": self.spec_version,\n            \"winner\": self.winner,\n            \"scores\": self.scores,\n            \"probabilities\": self.probabilities,\n            \"rationale\": self.rationale,\n            \"lookback_weeks\": self.lookback_weeks,\n            \"min_weeks\": self.min_weeks,\n        }\n\n\ndef _ensure_columns(df: pd.DataFrame, columns: Iterable[str]) -> pd.DataFrame:\n    for col in columns:\n        if col not in df.columns:\n            df[col] = np.nan\n    return df\n\n\ndef compute_universe_metrics(\n    history_df: pd.DataFrame, lookback_weeks: int, min_weeks: int\n) -> pd.DataFrame:\n    if history_df is None or history_df.empty:\n        empty = pd.DataFrame(columns=_METRIC_COLUMNS + [\"n_weeks\"])\n        return empty.astype({c: float for c in _METRIC_COLUMNS}, errors=\"ignore\")\n\n    df = history_df.copy()\n    if \"date\" in df.columns:\n        df[\"date\"] = pd.to_datetime(df[\"date\"], errors=\"coerce\")\n        df = df.sort_values([\"universe\", \"date\"])\n    else:\n        df = df.sort_values(\"universe\")\n\n    grouped: Dict[str, pd.DataFrame] = {\n        universe: group.tail(lookback_weeks if lookback_weeks > 0 else len(group))\n        for universe, group in df.groupby(\"universe\")\n    }\n\n    records: List[pd.Series] = []\n    for universe, group in grouped.items():\n        stats = {}\n        for col in _METRIC_COLUMNS:\n            if col in group.columns:\n                stats[col] = group[col].dropna().mean()\n            else:\n                stats[col] = np.nan\n        stats[\"n_weeks\"] = float(len(group))\n        records.append(pd.Series(stats, name=universe))\n\n    metrics_df = pd.DataFrame(records)\n    if not metrics_df.empty:\n        metrics_df.index.name = \"universe\"\n        metrics_df = metrics_df.astype({\"n_weeks\": float})\n    return metrics_df\n\n\ndef score_universes(\n    metrics_df: pd.DataFrame, weights: Dict[str, float], temperature: float\n) -> Tuple[pd.Series, pd.Series]:\n    if metrics_df is None or metrics_df.empty:\n        return pd.Series(dtype=float), pd.Series(dtype=float)\n\n    weights = weights or {}\n    df = _ensure_columns(metrics_df.copy(), _METRIC_COLUMNS)\n\n    alpha = df[\"alpha\"].fillna(0.0)\n    sortino = df[\"sortino\"].fillna(0.0)\n    mdd = df[\"mdd\"].fillna(0.0)\n    coverage = df[\"coverage\"].fillna(0.0)\n    turnover_cost = df[\"turnover_cost\"].fillna(0.0)\n\n    scores = (\n        weights.get(\"alpha\", 0.0) * alpha\n        + weights.get(\"sortino\", 0.0) * sortino\n        - weights.get(\"mdd\", 0.0) * mdd\n        + weights.get(\"coverage\", 0.0) * coverage\n        - weights.get(\"turnover\", 0.0) * turnover_cost\n    )\n\n    if temperature is None or temperature <= 0:\n        temperature = 1.0\n\n    arr = scores.to_numpy(dtype=float)\n    if not np.isfinite(arr).any():\n        probs = np.full_like(arr, 1.0 / len(arr), dtype=float)\n    else:\n        arr = np.where(np.isfinite(arr), arr, 0.0)\n        scaled = arr / temperature\n        max_val = np.max(scaled) if scaled.size else 0.0\n        exps = np.exp(scaled - max_val)\n        denom = exps.sum()\n        if denom <= 0:\n            probs = np.full_like(exps, 1.0 / len(exps))\n        else:\n            probs = exps / denom\n    probabilities = pd.Series(probs, index=scores.index, dtype=float)\n    return scores.astype(float), probabilities\n\n\ndef _load_history(metrics_history_path: Path) -> pd.DataFrame:\n    if not metrics_history_path.exists():\n        return pd.DataFrame()\n    try:\n        data = json.loads(metrics_history_path.read_text())\n    except json.JSONDecodeError:\n        return pd.DataFrame()\n    if not isinstance(data, list):\n        return pd.DataFrame()\n    if not data:\n        return pd.DataFrame()\n    return pd.DataFrame(data)\n\n\ndef _append_decision_log(path: Path, record: Dict[str, object]) -> None:\n    path.parent.mkdir(parents=True, exist_ok=True)\n    if path.exists():\n        try:\n            log = json.loads(path.read_text())\n            if not isinstance(log, list):\n                log = []\n        except json.JSONDecodeError:\n            log = []\n    else:\n        log = []\n    log.append(record)\n    path.write_text(json.dumps(log, indent=2, sort_keys=True))\n\n\ndef choose_universe(\n    candidates: List[str],\n    constraints: Dict[str, Dict[str, float]],\n    registry_fn: Callable[[str], pd.DataFrame],\n    metrics_history_path: Path,\n    spec: Dict[str, object],\n    as_of: date,\n) -> Dict[str, object]:\n    selection_cfg = (spec or {}).get(\"universe_selection\", {})\n    lookback_weeks = int(selection_cfg.get(\"lookback_weeks\", 8))\n    min_weeks = int(selection_cfg.get(\"min_weeks\", 4))\n    weights = selection_cfg.get(\"weights\", {}) or {}\n    temperature = float(selection_cfg.get(\"temperature\", 1.0))\n\n    history_df = _load_history(metrics_history_path)\n    if not history_df.empty:\n        history_df = history_df[history_df.get(\"universe\").isin(candidates)]\n\n    metrics_df = compute_universe_metrics(history_df, lookback_weeks, min_weeks)\n    metrics_df = metrics_df.reindex(candidates)\n    metrics_df = _ensure_columns(metrics_df, _METRIC_COLUMNS + [\"n_weeks\"])\n    metrics_df[\"n_weeks\"] = metrics_df[\"n_weeks\"].fillna(0).astype(float)\n\n    min_cons_map = (constraints or {}).get(\"min_constituents\", {})\n    coverage_now: Dict[str, float] = {}\n    for name in candidates:\n        min_cons_val = min_cons_map.get(name)\n        if not min_cons_val:\n            min_cons_val = min_cons_map.get(\"default\")\n        if not min_cons_val:\n            try:\n                min_cons_val = _expected_min_constituents(name)\n            except Exception:\n                min_cons_val = 1\n        min_cons = float(min_cons_val or 1)\n        try:\n            df = registry_fn(name)\n            current_count = len(df) if df is not None else 0\n        except Exception:\n            current_count = 0\n        coverage_now[name] = float(min(1.0, current_count / min_cons)) if min_cons else 0.0\n\n    metrics_df[\"coverage\"] = metrics_df.apply(\n        lambda row: coverage_now.get(row.name, 0.0)\n        if pd.isna(row.get(\"coverage\"))\n        else row.get(\"coverage\"),\n        axis=1,\n    )\n    metrics_df[\"turnover_cost\"] = metrics_df[\"turnover_cost\"].fillna(0.0)\n\n    scores, probabilities = score_universes(metrics_df, weights, temperature)\n    if scores.empty:\n        scores = pd.Series(0.0, index=pd.Index(candidates, name=\"universe\"))\n        probabilities = pd.Series(\n            [1.0 / len(candidates)] * len(candidates),\n            index=pd.Index(candidates, name=\"universe\"),\n        )\n\n    winner = str(probabilities.idxmax()) if not probabilities.empty else candidates[0]\n\n    contributions = {}\n    row = metrics_df.loc[winner]\n    contributions[\"alpha\"] = weights.get(\"alpha\", 0.0) * float(row.get(\"alpha\", 0.0))\n    contributions[\"sortino\"] = weights.get(\"sortino\", 0.0) * float(\n        row.get(\"sortino\", 0.0)\n    )\n    contributions[\"mdd\"] = -weights.get(\"mdd\", 0.0) * float(row.get(\"mdd\", 0.0))\n    contributions[\"coverage\"] = weights.get(\"coverage\", 0.0) * float(\n        row.get(\"coverage\", 0.0)\n    )\n    contributions[\"turnover\"] = -weights.get(\"turnover\", 0.0) * float(\n        row.get(\"turnover_cost\", 0.0)\n    )\n\n    driver_labels = {\n        \"alpha\": \"alpha\",\n        \"sortino\": \"risk-adjusted returns\",\n        \"mdd\": \"drawdown control\",\n        \"coverage\": \"coverage\",\n        \"turnover\": \"turnover discipline\",\n    }\n    sorted_drivers = sorted(\n        contributions.items(), key=lambda kv: abs(kv[1]), reverse=True\n    )\n    driver_bits: List[str] = []\n    for key, value in sorted_drivers[:2]:\n        if value == 0:\n            continue\n        direction = \"positive\" if value > 0 else \"negative\"\n        driver_bits.append(f\"{driver_labels.get(key, key)} ({direction} impact {value:.3f})\")\n\n    rationale_parts = [f\"Selected {winner}\"]\n    if driver_bits:\n        rationale_parts.append(\"; \".join(driver_bits))\n    n_weeks = row.get(\"n_weeks\", 0.0)\n    if n_weeks < min_weeks:\n        rationale_parts.append(\n            f\"history limited to {int(n_weeks)} of required {min_weeks} weeks\"\n        )\n    rationale = \" \u2014 \".join(rationale_parts)\n\n    decision = UniverseDecision(\n        winner=winner,\n        scores=scores.fillna(0.0).round(6).to_dict(),\n        probabilities=probabilities.fillna(0.0).round(6).to_dict(),\n        rationale=rationale,\n        parameters={\n            \"temperature\": temperature,\n            **{f\"w_{k}\": float(v) for k, v in weights.items()},\n        },\n        metrics=metrics_df,\n        as_of=str(as_of),\n        spec_version=str(spec.get(\"version\", \"\")),\n        lookback_weeks=lookback_weeks,\n        min_weeks=min_weeks,\n    )\n\n    decisions_path = metrics_history_path.parent / \"runs\" / \"universe_decisions.json\"\n    _append_decision_log(decisions_path, decision.to_log_record())\n\n    return {\n        \"winner\": decision.winner,\n        \"scores\": decision.scores,\n        \"probabilities\": decision.probabilities,\n        \"rationale\": decision.rationale,\n        \"metrics\": decision.metrics,\n        \"log_path\": str(decisions_path),\n        \"parameters\": decision.parameters,\n        \"lookback_weeks\": decision.lookback_weeks,\n        \"min_weeks\": decision.min_weeks,\n    }\n",
  "tests/test_universe_selector.py": "import json\nfrom datetime import date, timedelta\nimport pandas as pd\n\nfrom src.universe_selector import (\n    choose_universe,\n    compute_universe_metrics,\n    score_universes,\n)\n\n\ndef _make_history(universes, weeks):\n    rows = []\n    start = date(2024, 1, 1)\n    for uni in universes:\n        for idx in range(weeks):\n            rows.append(\n                {\n                    \"date\": (start + timedelta(weeks=idx)).isoformat(),\n                    \"universe\": uni,\n                    \"alpha\": 0.01 * (idx + 1),\n                    \"sortino\": 1.0 + 0.1 * idx,\n                    \"mdd\": 0.05 + 0.01 * idx,\n                    \"hit_rate\": 0.5 + 0.02 * idx,\n                    \"val_alpha\": 0.005 * (idx + 1),\n                    \"val_sortino\": 0.8 + 0.05 * idx,\n                    \"coverage\": 0.7,\n                    \"turnover_cost\": 0.001 * idx,\n                }\n            )\n    return pd.DataFrame(rows)\n\n\ndef test_compute_universe_metrics_window():\n    history = _make_history([\"U1\", \"U2\", \"U3\"], 10)\n    metrics = compute_universe_metrics(history, lookback_weeks=4, min_weeks=6)\n    assert set(metrics.index) == {\"U1\", \"U2\", \"U3\"}\n    assert set([\"alpha\", \"sortino\", \"mdd\", \"coverage\", \"turnover_cost\", \"n_weeks\"]).issubset(\n        metrics.columns\n    )\n    assert metrics.loc[\"U1\", \"n_weeks\"] == 4.0\n\n\ndef test_score_universes_softmax_sum_one():\n    metrics = pd.DataFrame(\n        {\n            \"alpha\": [0.02, 0.01],\n            \"sortino\": [1.5, 0.9],\n            \"mdd\": [0.1, 0.25],\n            \"coverage\": [0.9, 0.6],\n            \"turnover_cost\": [0.001, 0.002],\n        },\n        index=[\"U1\", \"U2\"],\n    )\n    weights = {\"alpha\": 1.0, \"sortino\": 0.5, \"mdd\": 0.2, \"coverage\": 0.3, \"turnover\": 0.1}\n    scores, probs = score_universes(metrics, weights, temperature=0.7)\n    assert list(scores.index) == [\"U1\", \"U2\"]\n    assert abs(probs.sum() - 1.0) < 1e-8\n    assert (probs >= 0).all()\n\n\ndef test_choose_universe_logs_and_rationale(tmp_path):\n    metrics_path = tmp_path / \"metrics_history.json\"\n    history = [\n        {\n            \"date\": \"2024-01-05\",\n            \"universe\": \"U1\",\n            \"alpha\": 0.02,\n            \"sortino\": 1.2,\n            \"mdd\": 0.1,\n            \"hit_rate\": 0.55,\n            \"val_alpha\": 0.01,\n            \"val_sortino\": 0.9,\n            \"turnover_cost\": 0.001,\n        },\n        {\n            \"date\": \"2024-01-12\",\n            \"universe\": \"U2\",\n            \"alpha\": 0.015,\n            \"sortino\": 1.0,\n            \"mdd\": 0.12,\n            \"hit_rate\": 0.52,\n            \"val_alpha\": 0.008,\n            \"val_sortino\": 0.85,\n        },\n        {\n            \"date\": \"2024-01-19\",\n            \"universe\": \"U3\",\n            \"alpha\": 0.01,\n            \"sortino\": 0.95,\n            \"mdd\": 0.09,\n            \"hit_rate\": 0.51,\n            \"val_alpha\": 0.007,\n            \"val_sortino\": 0.83,\n            \"coverage\": 0.65,\n        },\n    ]\n    metrics_path.write_text(json.dumps(history))\n\n    universe_sizes = {\n        \"U1\": 120,\n        \"U2\": 80,\n        \"U3\": 95,\n    }\n\n    def registry_fn(name: str):\n        count = universe_sizes.get(name, 0)\n        return pd.DataFrame({\"symbol\": list(range(count))})\n\n    spec = {\n        \"version\": \"v0.4\",\n        \"universe_selection\": {\n            \"lookback_weeks\": 6,\n            \"min_weeks\": 2,\n            \"weights\": {\n                \"alpha\": 0.6,\n                \"sortino\": 0.2,\n                \"mdd\": 0.3,\n                \"coverage\": 0.2,\n                \"turnover\": 0.1,\n            },\n            \"temperature\": 0.8,\n            \"constraints\": {\"min_constituents\": {\"U1\": 100, \"U2\": 100, \"U3\": 90}},\n        },\n    }\n\n    decision = choose_universe(\n        [\"U1\", \"U2\", \"U3\"],\n        spec[\"universe_selection\"].get(\"constraints\", {}),\n        registry_fn,\n        metrics_path,\n        spec,\n        date(2024, 3, 1),\n    )\n\n    assert decision[\"winner\"] in {\"U1\", \"U2\", \"U3\"}\n    assert isinstance(decision[\"rationale\"], str) and decision[\"rationale\"]\n    assert abs(sum(decision[\"probabilities\"].values()) - 1.0) < 1e-8\n\n    log_file = tmp_path / \"runs\" / \"universe_decisions.json\"\n    assert log_file.exists()\n    log_entries = json.loads(log_file.read_text())\n    assert log_entries[-1][\"winner\"] == decision[\"winner\"]\n    assert log_entries[-1][\"spec\"] == \"v0.4\"\n"
}
