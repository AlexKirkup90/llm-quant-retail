# app.py
import json
import sys
from pathlib import Path

# --- Make project root importable (works under streamlit/pytest/CI) ---
ROOT = Path(__file__).resolve().parent
if str(ROOT) not in sys.path:
    sys.path.insert(0, str(ROOT))

def main():
    import streamlit as st
    import pandas as pd
    import numpy as np
    from datetime import date

    # Local imports
    from src import (
        dataops,
        features,
        signals,
        portfolio,
        metrics,
        report,
        memory,
        universe,
        universe_registry,
        universe_selector,
    )

    st.title("LLM-Codex Quant (S&P 500) â€” Weekly")

    as_of = st.date_input("As-of date", value=date.today())
    universe_choices = ["SP500_MINI", "SP500_FULL", "R1000", "NASDAQ_100", "FTSE_350"]
    default_index = (
        universe_choices.index("SP500_FULL") if "SP500_FULL" in universe_choices else 0
    )

    universe_mode = st.selectbox("Universe Mode", ["auto", "manual"], index=0)
    manual_universe = None
    if universe_mode == "manual":
        manual_universe = st.selectbox("Universe", universe_choices, index=default_index)

    apply_filters = st.checkbox("Apply liquidity/price filters", value=True)

    if st.button("Refresh universe lists now"):
        results = universe_registry.refresh_all(force=True)
        st.success("Universe registry refresh complete.")
        st.json(results)

        row_counts = {}
        symbol_samples = {}
        universes = getattr(universe_registry, "_UNIVERSES", {})
        for name, status in results.items():
            if isinstance(status, str) and status.lower().startswith("error:"):
                st.markdown(f"**{name}**")
                st.error(status)
                continue
            definition = universes.get(name)
            if not definition:
                continue
            try:
                df = universe_registry.load_universe(name)
                row_counts[name] = len(df)
                symbol_samples[name] = df["symbol"].head(5).tolist()
            except Exception as exc:
                st.markdown(f"**{name}**")
                st.error(f"error: {exc}")

        if row_counts:
            st.write("Latest universe row counts:")
            st.json(row_counts)
        if symbol_samples:
            st.write("Sample symbols (first 5):")
            st.json(symbol_samples)

    st.markdown(
        "This demo builds a simplified, AI-driven S&P 500 portfolio using "
        "auto-generated features, fitted weights, and quick performance metrics. "
        "Click **Run Weekly Cycle** to execute a full pass."
    )

    if st.button("Run Weekly Cycle"):
        try:
            spec_path = Path("spec/current_spec.json")
            spec_data = json.loads(spec_path.read_text()) if spec_path.exists() else {}

            decision_info = None
            selection_cfg = spec_data.get("universe_selection", {})
            if universe_mode == "auto":
                candidates = selection_cfg.get("candidates") or spec_data.get("universe", {}).get(
                    "modes", []
                )
                if not candidates:
                    candidates = universe_choices
                constraints = selection_cfg.get("constraints", {}) or {}
                decision_info = universe_selector.choose_universe(
                    candidates,
                    constraints,
                    universe_registry.load_universe,
                    Path("metrics_history.json"),
                    spec_data,
                    as_of,
                )
                selected_universe_name = decision_info.get("winner") or candidates[0]
            else:
                selected_universe_name = manual_universe or universe_choices[default_index]

            # === 1. Universe ===
            try:
                uni = universe.load_universe(
                    selected_universe_name, apply_filters=apply_filters
                )
            except universe_registry.UniverseRegistryError as exc:
                st.error(str(exc))
                st.stop()
            except Exception as exc:
                st.error(f"Failed to load {selected_universe_name}: {exc}")
                st.stop()

            filter_meta = uni.attrs.get("universe_filter_meta", {})
            raw_count = int(filter_meta.get("raw_count", len(uni)))
            filtered_count = int(filter_meta.get("filtered_count", len(uni)))
            filters_applied = bool(filter_meta.get("filters_applied", False))
            reason = str(filter_meta.get("reason", "")) if filter_meta else ""

            st.write(f"Universe size (raw): {raw_count}")
            st.write("Universe size (post-filter): {}".format(filtered_count))
            if not filters_applied and reason:
                st.info(reason)

            if decision_info:
                metrics_table = decision_info["metrics"].copy()
                display_cols = [
                    "alpha",
                    "sortino",
                    "mdd",
                    "coverage",
                    "turnover_cost",
                    "n_weeks",
                ]
                for col in display_cols:
                    if col not in metrics_table.columns:
                        metrics_table[col] = np.nan
                summary_df = metrics_table[display_cols].copy()
                summary_df["score"] = pd.Series(decision_info["scores"])
                summary_df["probability"] = pd.Series(decision_info["probabilities"])
                summary_df = summary_df.fillna(0.0)

                def _highlight(row):
                    if row.name == selected_universe_name:
                        return ["background-color: #0b5394; color: white"] * len(row)
                    return [""] * len(row)

                st.subheader("Universe decision (auto)")
                st.dataframe(
                    summary_df.style.format(
                        {
                            "alpha": "{:.4f}",
                            "sortino": "{:.2f}",
                            "mdd": "{:.2%}",
                            "coverage": "{:.1%}",
                            "turnover_cost": "{:.4%}",
                            "probability": "{:.1%}",
                            "score": "{:.4f}",
                        }
                    ).apply(_highlight, axis=1)
                )
                st.caption(decision_info.get("rationale", ""))

            symbols = [s for s in uni["symbol"].tolist() if isinstance(s, str)]
            coverage_current = 0.0
            if len(uni) > 0:
                min_expected = universe_registry.expected_min_constituents(selected_universe_name)
                coverage_current = (
                    float(len(uni)) / float(min_expected) if min_expected else 0.0
                )
                coverage_current = float(min(1.0, coverage_current))
            if "SPY" not in symbols:
                symbols.append("SPY")
            max_symbols = 150
            if len(symbols) > max_symbols:
                st.info(f"Capping universe to first {max_symbols} symbols for runtime safety.")
                symbols = symbols[:max_symbols]
            st.write(f"Universe size: {len(symbols)}")

            # === 2. Data ===
            try:
                prices = dataops.fetch_prices(symbols, years=5)
                dataops.cache_parquet(prices, f"prices_{selected_universe_name.lower()}")
            except Exception:
                # fabricate dummy data if network or API fails
                idx = pd.date_range(end=date.today(), periods=252 * 5, freq="B")
                prices = pd.DataFrame(
                    np.cumprod(1 + np.random.randn(len(idx), len(symbols)) * 0.001, axis=0),
                    index=idx,
                    columns=symbols,
                )
            st.write("Downloaded prices:", prices.shape)

            # === 3. Feature engineering ===
            try:
                feats = features.combine_features(prices)
                if feats.isna().all().all():
                    raise ValueError("Empty features")
            except Exception:
                # fallback: fabricate random standardized features
                feats = pd.DataFrame(
                    np.random.randn(len(prices.columns), 6),
                    index=prices.columns,
                    columns=[
                        "mom_6m", "value_ey", "quality_roic",
                        "risk_beta", "eps_rev_3m", "news_sent",
                    ],
                )

            feats = feats.fillna(0.0)

            # === 4. Forward-return target (robust) ===
            rets = prices.pct_change().dropna(how="all")
            try:
                fwd5 = (1 + rets).rolling(5, min_periods=5).apply(lambda x: x.prod() - 1).shift(-5)
                fwd5 = fwd5.iloc[:-5] if len(fwd5) >= 5 else fwd5.iloc[0:0]
                last_valid = fwd5.dropna(how="all").iloc[-1] if not fwd5.dropna(how="all").empty else None
                fwd_target = last_valid if last_valid is not None else pd.Series(0.0, index=feats.columns)
            except Exception:
                fwd_target = pd.Series(0.0, index=feats.columns)
            fwd_target.name = "fwd_5d"

            # === 5. Signals ===
            feature_history = {}
            hist_returns = fwd5.dropna(how="all")
            history_dates = hist_returns.tail(signals.ROLLING_WEEKS).index
            for ts in history_dates:
                price_slice = prices.loc[:ts]
                if price_slice.empty:
                    continue
                try:
                    feature_snapshot = features.combine_features(price_slice)
                    feature_history[ts] = feature_snapshot
                except Exception:
                    continue

            try:
                rolling_returns = hist_returns.loc[history_dates]
                w_ridge = signals.fit_rolling_ridge(rolling_returns, feature_history)
                if w_ridge.empty:
                    w_ridge = signals.fit_ridge(feats, fwd_target)
                scores = signals.score_current(feats, w_ridge)
            except Exception:
                # fallback: random scores
                scores = pd.Series(np.random.randn(len(feats.index)), index=feats.index)

            st.subheader("Top candidates")
            st.dataframe(scores.head(20).to_frame())

            weights_path = Path(signals.RUNS_DIR) / "feature_weights.json"
            if weights_path.exists():
                try:
                    weights_json = json.loads(weights_path.read_text())
                    st.subheader("Feature weights (smoothed)")
                    st.dataframe(pd.Series(weights_json.get("weights", {}), name="weight"))
                except Exception:
                    st.warning("Unable to load feature weights cache.")

            # === 6. Portfolio ===
            returns_252 = prices.pct_change().iloc[-252:]
            topN = scores.head(20).index.tolist()
            if len(topN) == 0:
                st.warning("No scored candidates, fabricating dummy weights.")
                topN = list(prices.columns[:15])
            try:
                w0 = portfolio.inverse_vol_weights(
                    returns_252, topN, cap_single=0.10, k=min(15, len(topN))
                )
            except Exception:
                w0 = pd.Series(1 / len(topN), index=topN)

            # Sector cap + turnover handling
            try:
                sector_map = uni.set_index("symbol")["sector"]
                w1 = portfolio.apply_sector_caps(w0, sector_map, cap=0.35)
            except Exception:
                w1 = w0

            last = memory.load_last_portfolio()
            last_w = pd.Series(
                {h["ticker"]: h["weight"] for h in last.get("holdings", [])}
            ) if last else None
            try:
                w_final = portfolio.enforce_turnover(last_w, w1, t_cap=0.30)
            except Exception:
                w_final = w1

            w_final = (w_final / w_final.sum()).sort_values(ascending=False)
            try:
                turnover_fraction = float(portfolio.turnover(last_w, w_final))
            except Exception:
                turnover_fraction = float(w_final.abs().sum()) if w_final is not None else 0.0
            port = {
                "as_of": str(as_of),
                "holdings": [{"ticker": t, "weight": float(w_final[t])} for t in w_final.index],
                "cash_weight": float(max(0.0, 1.0 - w_final.sum())),
            }
            memory.save_portfolio(port)
            st.success("Weekly portfolio created.")
            st.json(port)

            # === 7. Evaluation ===
            port_rets = (
                (returns_252[w_final.index] * w_final.reindex(returns_252.columns, fill_value=0.0))
                .sum(axis=1)
                .fillna(0.0)
            )
            curve = (1 + port_rets).cumprod()
            mdd = metrics.max_drawdown(curve) if len(curve) > 0 else 0.0
            sor = metrics.sortino(port_rets) if len(port_rets) > 0 else 0.0
            bench = returns_252.get("SPY", pd.Series(0.0, index=returns_252.index))
            alpha = metrics.alpha_vs_bench(port_rets, bench) if not bench.empty else 0.0

            st.subheader("Weekly metrics")
            st.write(
                f"- Sortino: **{sor:.2f}**  \n"
                f"- Max Drawdown: **{mdd:.2%}**  \n"
                f"- Alpha vs SPY (weekly mean): **{alpha:.4%}**"
            )

            # === 8. Report ===
            note = (
                f"# Weekly AI Portfolio â€” {as_of}\n\n"
                f"- Sortino: {sor:.2f}\n"
                f"- Max Drawdown: {mdd:.2%}\n"
                f"- Alpha (vs SPY, weekly mean): {alpha:.4%}\n"
            )
            out = report.write_markdown(note)
            st.download_button(
                "Download weekly report",
                data=open(out, "rb"),
                file_name=out.split("/")[-1],
            )
            # === 9. Log Metrics for Evaluator ===
            val = metrics.val_metrics(port_rets, bench)
            metrics_record = {
                "spec": str(spec_data.get("version", "v0.4")),
                "date": str(as_of),
                "alpha": float(alpha),
                "sortino": float(sor),
                "mdd": float(mdd),
                "hit_rate": float((port_rets > bench).mean()),
                "val_sortino": float(val.get("val_sortino", float("nan"))),
                "val_alpha": float(val.get("val_alpha", float("nan"))),
                "universe": selected_universe_name,
                "coverage": float(coverage_current),
                "turnover_cost": float(0.0005 * turnover_fraction if turnover_fraction else 0.0),
            }

            metrics_file = Path("metrics_history.json")
            if metrics_file.exists():
                try:
                    history = json.loads(metrics_file.read_text())
                    if not isinstance(history, list):
                        history = []
                except json.JSONDecodeError:
                    history = []
            else:
                history = []

            history.append(metrics_record)
            with open(metrics_file, "w") as f:
                json.dump(history, f, indent=2)

            st.success("Metrics logged to metrics_history.json")
        
        except Exception as e:
            st.error(f"Run failed: {e}")

if __name__ == "__main__":
    try:
        main()
    except KeyboardInterrupt:
        pass
